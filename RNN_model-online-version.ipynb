{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'w266_common.utils' from '/home/ubuntu/project/w266_common/utils.py'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "import gzip\n",
    "from collections import namedtuple\n",
    "import tflearn\n",
    "# Helper libraries\n",
    "from w266_common import vocabulary, tf_embed_viz, glove_helper\n",
    "from w266_common import utils; reload(utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "#Using pretrained GLove embeddings\n",
    "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400003, 100)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read the amazon review data files\n",
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "#df = getDF('reviews_Toys_and_Games.json.gz') #old def function corresponding to the step bt step vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 238.9102017879486\n",
      "end getDF\n",
      "time taken to load data =  238.9109025001526\n"
     ]
    }
   ],
   "source": [
    "df_hnk = getDF('reviews_Home_and_Kitchen.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vid = getDF('reviews_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_aut = getDF('reviews_Automotive.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_toys = getDF('reviews_Toys_and_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home and Kitchen reviews train, dev and test set dataframe shape: (2552355, 9) (850785, 9) (850786, 9)\n"
     ]
    }
   ],
   "source": [
    "#Create train,dev,test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_toys,devtest = train_test_split(df_toys, test_size=0.4, random_state=42)\n",
    "# dev_toys,test_toys = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "# print('Toy reviews train, dev and test set dataframe shape:',train_toys.shape,dev_toys.shape,test_toys.shape)\n",
    "\n",
    "#For Video games reviews\n",
    "# train_vid,devtest = train_test_split(df_vid, test_size=0.4)\n",
    "# dev_vid,test_vid = train_test_split(devtest,test_size = 0.5)\n",
    "# print('Video games reviews train, dev and test set dataframe shape:',train_vid.shape,dev_vid.shape,test_vid.shape)\n",
    "\n",
    "#For Auto reviews\n",
    "# train_aut,devtest = train_test_split(df_aut, test_size=0.4)\n",
    "# dev_aut,test_aut = train_test_split(devtest,test_size = 0.5)\n",
    "# print('Auto reviews train, dev and test set dataframe shape:',train_aut.shape,dev_aut.shape,test_aut.shape)\n",
    "\n",
    "#For Home and Kitchen reviews\n",
    "train_hnk,devtest = train_test_split(df_hnk, test_size=0.4, random_state=42)\n",
    "dev_hnk,test_hnk = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Home and Kitchen reviews train, dev and test set dataframe shape:',train_hnk.shape,dev_hnk.shape,test_hnk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         overall                                         reviewText  \\\n",
      "2315529      3.0  It's great as a panini grill, but it takes for...   \n",
      "3381949      4.0  I originally intended to buy another brand mad...   \n",
      "2015374      5.0  This item was well made, and the instructions ...   \n",
      "2378385      5.0  I was hoping that this would wrap around the m...   \n",
      "3962814      5.0  These barstools are amazing. After scouring st...   \n",
      "\n",
      "                                      summary        asin   reviewTime  \\\n",
      "2315529  Not as good as I thought it would be  B002YD99Y4   03 2, 2013   \n",
      "3381949               Good for the price paid  B006G57132  10 12, 2013   \n",
      "2015374                           Cork Trivet  B00283ZQL0  01 10, 2014   \n",
      "2378385                        Very satisfied  B0035E63WM  06 20, 2014   \n",
      "3962814  Amazing quality for an amazing price  B00BLSEIB2  04 30, 2014   \n",
      "\n",
      "        helpful      reviewerID      reviewerName  unixReviewTime  \n",
      "2315529  [0, 0]  A1KPJ12YFSE293            CathyB      1362182400  \n",
      "3381949  [1, 1]  A1LGNL6N4HFBOR         Gadgeteer      1381536000  \n",
      "2015374  [0, 0]  A1JPI56R3CTR4J  Ralph C. Schultz      1389312000  \n",
      "2378385  [0, 0]  A2XLE75PI4GM2L             Lloyd      1403222400  \n",
      "3962814  [1, 1]  A1SVJT9AG21PRH     Leslie Mathis      1398816000  \n"
     ]
    }
   ],
   "source": [
    "#checking that we have different productids\n",
    "print(train_hnk.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a smaller sized train and dev data set. Enables testing accuracy for different sizes.\n",
    "#Also binarizes the labels. Ratings of 1,2 and to 0; Ratings of 4,5 to 1.\n",
    "\n",
    "def set_df_size(size,data_train,data_dev):\n",
    "    size_train = size\n",
    "    len_max_train = data_train[data_train.overall!=3].shape[0] #max possible length of train data set taking out the 3 ratings.\n",
    "    #print(\"Number of reviews with ratings != 3 in train set\",len_max_train)\n",
    "    temp_size_train = min(len_max_train,size_train)\n",
    "\n",
    "    len_max_dev = data_dev[data_dev.overall!=3].shape[0]\n",
    "    #print(\"Number of reviews with ratings != 3 in dev set\",len_max_dev)\n",
    "    temp_size_dev = min(len_max_dev,int(0.3*temp_size_train)) #making the dev set about 0.3 times the train set.\n",
    "\n",
    "    temp_train_data = data_train[data_train.overall != 3][:temp_size_train]\n",
    "    #print('Size of train data',temp_train_data.shape)\n",
    "    #print(temp_train_data.groupby('overall').count())\n",
    "    #print(temp_train_toys[:5])\n",
    "\n",
    "    temp_dev_data = data_dev[data_dev.overall!=3][:temp_size_dev]\n",
    "    #print('Size of dev data',temp_dev_data.shape)\n",
    "    #print(temp_dev_data.groupby('overall').count())\n",
    "    #print(temp_dev_data[:2])\n",
    "    \n",
    "    #Binarize ratings\n",
    "    temp_train_y = np.zeros(temp_size_train)\n",
    "    temp_train_y[temp_train_data.overall > 3] = 1\n",
    "    temp_dev_y = np.zeros(temp_size_dev)\n",
    "    temp_dev_y[temp_dev_data.overall>3] = 1\n",
    "    #print('binarized y shape',temp_train_y.shape,temp_dev_y.shape)\n",
    "    #print(temp_dev_y[:20],data_dev.overall[:20])\n",
    "    return temp_train_data,temp_dev_data,temp_train_y,temp_dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         overall                                         reviewText  \\\n",
      "1984789      1.0  My poster was horribly damaged when it was shi...   \n",
      "4251355      5.0  Excellent Product! Quality shipping and packag...   \n",
      "523170       5.0  This product is fantastic ! Before I purchased...   \n",
      "2489673      2.0  lid doesnt secure well. just buy one at a loca...   \n",
      "953589       2.0  My kids were very excited to make Popsicles us...   \n",
      "\n",
      "                                          summary        asin   reviewTime  \\\n",
      "1984789                          Damaged shipping  B0025NY19Q   07 8, 2013   \n",
      "4251355  Awesome product, and very fast shipping.  B00KK5ITO4   06 1, 2014   \n",
      "523170                               meat grinder  B0002I5QHW   07 8, 2013   \n",
      "2489673                                     cheap  B003FZBI7G  12 18, 2013   \n",
      "953589                      Broke after first use  B000G34F1Q  06 28, 2013   \n",
      "\n",
      "        helpful      reviewerID   reviewerName  unixReviewTime  \n",
      "1984789  [0, 1]  A1693NIWIIZ7H0         Hannah      1373241600  \n",
      "4251355  [0, 0]  A2P0VBNZ5131UO        M. Frey      1401580800  \n",
      "523170   [0, 0]   A52D5WTTJMG10           Raya      1373241600  \n",
      "2489673  [0, 0]  A2CVX6D9MCLI2V  Wicked Boston      1387324800  \n",
      "953589   [0, 0]   AJG57T4S7KEKX    Jocey Starr      1372377600  \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(dev_hnk.head(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] #list of keys that refer to each dataframe. Adding a new dataframe would require updating this list\n",
    "dict_train_df = {} #Dict to store train input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_dev_df = {} #Dict to store dev input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_train_y = {} #Dict to store binarized train data label for each domain\n",
    "dict_dev_y = {} #Dict to store binarized dev data label for each domain\n",
    "#print(len(dict_train_df))\n",
    "\n",
    "def create_sized_data(size = 10000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "#     key = list_df[0]\n",
    "    #print('Toys reviews\\n')\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "#     #print('\\n Video games reviews\\n')\n",
    "#     key = list_df[1]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_vid,dev_vid)\n",
    "#     #print('\\n Auto reviews\\n')\n",
    "#     key = list_df[2]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_aut,dev_aut)\n",
    "    #print('\\n Home and Kitchen reviews\\n')\n",
    "    key = list_df[3]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "    \n",
    "create_sized_data()\n",
    "#print(len(dict_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] #list of keys that refer to each dataframe. Adding a new dataframe would require updating this list\n",
    "dict_train_df = {} #Dict to store train input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_dev_df = {} #Dict to store dev input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_train_y = {} #Dict to store binarized train data label for each domain\n",
    "dict_dev_y = {} #Dict to store binarized dev data label for each domain\n",
    "#print(len(dict_train_df))\n",
    "\n",
    "def create_sized_data(size = 100000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "    key = list_df[3]\n",
    "    #print('Toys reviews\\n')\n",
    "\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "#     #print('\\n Video games reviews\\n')\n",
    "#     key = list_df[1]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_vid,dev_vid)\n",
    "#     #print('\\n Auto reviews\\n')\n",
    "#     key = list_df[2]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_aut,dev_aut)\n",
    "#     #print('\\n Home and Kitchen reviews\\n')\n",
    "    key = list_df[3]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "    \n",
    "create_sized_data()\n",
    "#print(len(dict_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(dict_train_df['hnk'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "x_train_tokens_list length 100000\n",
      "Vocabulary size: 103,554\n",
      "Total words  7875508\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing steps\n",
    "\n",
    "#Changing to nltk punkt tokenizer as the periods are not getting removed\n",
    "print(dict_train_df['hnk'].shape[0])\n",
    "\n",
    "train_cnt = collections.Counter()\n",
    "x_train_tokens_list = []\n",
    "start = time.time()\n",
    "for i in range(dict_train_df['hnk'].shape[0]):\n",
    "    #print(dict_train_df['hnk'].iloc[i][2])\n",
    "    x_train_tokens = word_tokenize(dict_train_df['hnk'].iloc[i][1])\n",
    "    \n",
    "    \n",
    "\n",
    "    #2. changing to lowercase and replacing numbers(are we losing any context by \n",
    "    #replacing all numbers in the review test? Are we losing any context here)\n",
    "    x_tokens_canonical = utils.canonicalize_words(x_train_tokens)\n",
    "    \n",
    "    x_train_tokens_list.append(x_tokens_canonical)\n",
    "    \n",
    "    if i%10000 == 0:\n",
    "        print(i) \n",
    "    #3. Build vocabulary\n",
    "    for items in x_tokens_canonical:\n",
    "            train_cnt[items] += 1\n",
    "            \n",
    "vocab = vocabulary.Vocabulary(train_cnt, size=None)  # size=None means unlimited\n",
    "total_words = sum(train_cnt.values())\n",
    "print(\"x_train_tokens_list length\", len(x_train_tokens_list))\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "#print(\"Vocabulary dict: \", vocab.word_to_id)\n",
    "print(\"Total words \",total_words )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_tokens_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_tokens_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'item', 'was', 'well', 'made', ',', 'and', 'the', 'instructions', 'for', 'personalizing', 'it', 'by', 'the', 'persons', 'for', 'whom', 'i', 'bought', 'it', 'were', 'clear', 'and', 'uncomplicated', '.', 'so', 'i', 'hope', 'they', 'will', 'enjoy', 'putting', 'it', 'together', 'with', 'their', 'many', 'wine', 'corks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tokens_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'originally', 'intended', 'to', 'buy', 'another', 'brand', 'made', 'in', 'italy', 'which', 'has', 'very', 'similar', 'design', 'also', 'has', 'the', 'mirror', 'front', 'which', 'would', 'cost', 'at', 'least', 'double', 'but', 'would', 'arrive', 'assembled', '.', 'i', 'ended', 'up', 'getting', 'this', 'model', 'instead', 'based', 'on', 'the', 'cost', '.', 'the', 'assembly', 'process', 'was', 'tedious', 'and', 'time-consuming', ',', 'but', 'the', 'parts', 'are', 'connected', 'well', 'and', 'quality', 'are', 'ok', '.', 'the', 'package', 'was', 'delivered', 'by', 'ups', 'and', 'one', 'of', 'the', 'mirror', 'board', 'arrived', 'with', 'cracks', '.', 'i', 'emailed', 'the', 'customer', 'service', 'according', 'to', 'the', 'information', 'on', 'an', 'insert', 'and', 'received', 'a', 'quick', 'reply', 'letting', 'me', 'know', 'that', 'the', 'replacement', 'part', 'will', 'be', 'shipped', 'to', 'me', 'in', 'about', 'a', 'week', '.', 'each', 'compartment', 'have', 'no', 'problem', 'accommodating', 'DG', 'pairs', 'of', 'size', 'DGDG', 'shoes', '.']\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tokens_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting all reviews to ids \n",
    "train_id_list = []\n",
    "for item in x_train_tokens_list:\n",
    "    train_id_list.append(vocab.words_to_ids(item))\n",
    "    \n",
    "test_id_list = []\n",
    "for item in x_test_tokens_list:\n",
    "    test_id_list.append(vocab.words_to_ids(item))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'item', 'was', 'well', 'made', ',', 'and', 'the', 'instructions', 'for', 'personalizing', 'it', 'by', 'the', 'persons', 'for', 'whom', 'i', 'bought', 'it', 'were', 'clear', 'and', 'uncomplicated', '.', 'so', 'i', 'hope', 'they', 'will', 'enjoy', 'putting', 'it', 'together', 'with', 'their', 'many', 'wine', 'corks', '.']\n"
     ]
    }
   ],
   "source": [
    "print((x_train_tokens_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4301, 76, 4302, 305, 4149, 570, 166, 76, 54, 41, 138, 4, 145, 223, 80, 19, 235, 95, 44, 704, 168, 76, 41, 4303, 26, 147, 6, 12, 4304, 3428, 970, 10, 915, 706, 1326, 706, 50, 17, 412, 706, 983, 343, 19, 235, 12, 43, 1457, 76, 12, 34, 23, 489, 2088, 43, 245, 36, 979, 13, 1612, 2496, 498, 289, 12, 232, 87, 2659, 10]\n"
     ]
    }
   ],
   "source": [
    "print(max((train_id_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest review: 0\n",
      "Longest review: 5990\n"
     ]
    }
   ],
   "source": [
    "review_lengths = [len(review) for review in x_train_tokens_list]\n",
    "print(\"Shortest review:\", min(review_lengths))\n",
    "print(\"Longest review:\",max(review_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neCJwbi6l/8AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f231bb8ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.axes()\n",
    "sns.distplot(review_lengths)\n",
    "ax.set_title(\"Distribution of the review lengths\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>78.755080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>94.835379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>91.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5990.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "count  100000.000000\n",
       "mean       78.755080\n",
       "std        94.835379\n",
       "min         0.000000\n",
       "25%        30.000000\n",
       "50%        49.000000\n",
       "75%        91.000000\n",
       "max      5990.000000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(review_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    162.0\n",
       "Name: 0.9, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(review_lengths).quantile(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Based on the above, we are setting a max_length of 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n",
      "(100000, 150)\n",
      "y train shape (100000,)\n",
      "Total words: 90595\n",
      "Time taken to vectorize 100000 size dataframe 16.173495054244995\n",
      "sample review These barstools are amazing. After scouring stores and the internet, I decided that these would be my best bet. I needed sturdy stools to deal with heavy use from kids, family, ect. They are 1000% solid. When they arrive you have to screw the seat onto the base swivel mechanism, however all the important parts that make it solid are already in place. You also have to attach the solid wooden seat onto the frame. The seat isn't pre-drilled so that is a bit annoying. Not hard, but annoying. These stools are going to last a very long time, without a doubt. I wouldn't hesitate to buy more. They feel stronger, more solid, and of higher quality than stools I saw for $300 each. I was a little nervous they would look clunky, but they are beautiful with very nice lines. I wish I would have ordered them two months ago when I first saw them.I forgot to mention the box was delivered to my door within two days. Fastest shipping ever. \n",
      "\n",
      "corresponding ids\n",
      " [129 130  44 131 132 133 134  41  17 135   1 136  73 137  20  77 138 139\n",
      " 140   1 141 142 143   4 144  57 145 146 147 148 149 150 151  44 152 153\n",
      " 154 106  26 155  83   4 156  17 157 158  17 159 160 161 162 163  17 164\n",
      "  43  73 165  97 153  44 166   9 167 168  16  83   4 169  17 153 170 157\n",
      " 158  17 171  36 157 172 173 174  73 175  67 176 177 178 179  25 177 129\n",
      " 143  44 180   4 181  67  13 182 183 184  67 185   1 186 187   4   5 188\n",
      " 151 189 190 188 153  41  54 191  47 192 143   1 193  95 194 195   1  39\n",
      "  67 196 197 106  20 198 199  25 106  44 200  57  13 201 202   1 203   1\n",
      "  20  83 204 205 206 207]\n",
      "sample review lid doesnt secure well. just buy one at a local store for the extra couple bucks. it is not terrible tho \n",
      "\n",
      "corresponding ids\n",
      " [1504  716 1256   46  304    5   53   22   67 1302  714   95   17 1485  822\n",
      "  753   97  175  288 2600  965    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "max_length =150\n",
    "vocab_processor = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=0)\n",
    "#Note : This function seems to be deprecated. Another function I ran into \n",
    "# tflearn.data_utils.VocabularyProcessor (max_document_length, min_frequency=3, vocabulary=None, tokenizer_fn=None)\n",
    "\n",
    "def process_inputs(key, vocab_processor):\n",
    "    \n",
    "    # For simplicity, we call our features x and our outputs y\n",
    "    start_vectorize = time.time()\n",
    "    x_train = dict_train_df[key].reviewText\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_df[key].reviewText\n",
    "    y_dev = dict_dev_y[key]\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    # Train the vocab_processor from the training set\n",
    "    x_train = vocab_processor.fit_transform(x_train)\n",
    "    # Transform our test set with the vocabulary processor\n",
    "    x_dev = vocab_processor.transform(x_dev)\n",
    "\n",
    "    # We need these to be np.arrays instead of generators\n",
    "    x_train = np.array(list(x_train))\n",
    "    print(x_train.shape)\n",
    "    x_dev = np.array(list(x_dev))\n",
    "    y_train = np.array(y_train).astype(int)\n",
    "    y_dev = np.array(y_dev).astype(int)\n",
    "    \n",
    "#     y_train = tf.expand_dims(y_train,1)\n",
    "#     y_dev = tf.expand_dims(y_dev,1)\n",
    "    print('y train shape',y_train.shape)\n",
    "\n",
    "    V = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % V)\n",
    "    end_vectorize = time.time()\n",
    "    print('Time taken to vectorize %d size dataframe'%x_train.shape[0],end_vectorize-start_vectorize)\n",
    "\n",
    "    # Return the transformed data and the number of words\n",
    "    return x_train, y_train, x_dev, y_dev, V\n",
    "\n",
    "x_train, y_train, x_dev, y_dev, V = process_inputs('hnk',vocab_processor)\n",
    "\n",
    "#Print a few examples for viewing\n",
    "print('sample review',dict_train_df['hnk']['reviewText'].iloc[3],'\\n')\n",
    "print('corresponding ids\\n',x_train[3])\n",
    "print('sample review',dict_dev_df['hnk']['reviewText'].iloc[3],'\\n')\n",
    "print('corresponding ids\\n',x_dev[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = x_train\n",
    "dev_ids = x_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 150)\n"
     ]
    }
   ],
   "source": [
    "print(train_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 12 10 16 17 18 19 20  7 21 22 10 23\n",
      " 24 25 26 27 28  6 29 10 30 31 12 32 33 13 12 34 35 36 36 37 38 19 39 32 26\n",
      "  9 40 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(train_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000,)\n",
      "(100000, 150)\n",
      "(30000, 150)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(train_ids.shape)\n",
    "print(dev_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "#batch_size = 50\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(ids, labels, batch_size=100):\n",
    "    \n",
    "    n_batches = len(ids)//batch_size\n",
    "    ids, labels = ids[:n_batches*batch_size], labels[:n_batches*batch_size]\n",
    "    \n",
    "    for ii in range(0, len(ids), batch_size):\n",
    "        yield ids[ii:ii+batch_size], labels[ii:ii+batch_size]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn(gl_embed=hands.W,\n",
    "              embed_size=embed_size,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              lstm_size=lstm_size,\n",
    "              lstm_layers=lstm_layers):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #n_words = len(vocabulary_to_int)\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs_ = tf.placeholder(tf.int32,[None, None],name='inputs_')\n",
    "    with tf.name_scope('labels'):\n",
    "        labels_ = tf.placeholder(tf.int32,[None, None],name='labels_')\n",
    "    with tf.name_scope('keep_prob'):    \n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "        \n",
    "    with tf.name_scope('embedding'):\n",
    "#         embedding = tf.Variable(tf.random_normal((n_words,embed_size),-1,1),name='embedding_')\n",
    "#         embed = tf.nn.embedding_lookup(embedding,inputs_)\n",
    "        embedding=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n",
    "                                       initializer=tf.constant_initializer(gl_embed),trainable=False)\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "        \n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "\n",
    "        # Stack up multiple LSTM layers, for deep learning\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers)\n",
    "        \n",
    "        with tf.name_scope(\"RNN_init_state\"):\n",
    "            # Getting an initial state of all zeros\n",
    "            initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, \n",
    "                                                        activation_fn=tf.sigmoid,\n",
    "                                                        weights_initializer=\n",
    "                                                        tf.truncated_normal_initializer(stddev=0.1))   \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs_', 'labels_','initial_state', 'final_state',\n",
    "                    'keep_prob', 'cell', 'cost', 'predictions', 'optimizer',\n",
    "                    'accuracy','merged']\n",
    "    \n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    \n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = build_rnn(gl_embed=hands.W,\n",
    "              embed_size=embed_size,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              lstm_size=lstm_size,\n",
    "              lstm_layers=lstm_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('output/logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, epoch,train_writer,test_writer):\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        iteration = 1\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(model.initial_state)\n",
    "\n",
    "            for ii, (x, y) in enumerate(batch_iterator(train_ids, y_train, batch_size), 1):\n",
    "                \n",
    "                feed = {model.inputs_: x,\n",
    "                        model.labels_: y[:, None],\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: state}\n",
    "                summary,loss, state, _ = sess.run([model.merged,model.cost, \n",
    "                                                   model.final_state, \n",
    "                                                   model.optimizer], feed_dict=feed)\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "                if iteration%100==0:\n",
    "                    print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Train loss: {:.3f}\".format(loss))\n",
    "                    train_writer.add_summary(summary, iteration)\n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(model.cell.zero_state(batch_size, tf.float32))\n",
    "                    \n",
    "                    for x, y in batch_iterator(dev_ids, y_dev, batch_size):\n",
    "                        feed = {model.inputs_: x,\n",
    "                                model.labels_: y[:, None],\n",
    "                                model.keep_prob: 1,\n",
    "                                model.initial_state: val_state}\n",
    "                        summary, dev_loss,batch_acc, val_state = sess.run([model.merged, model.cost,model.accuracy, \n",
    "                                                         model.final_state], feed_dict=feed)\n",
    "                        #print('batch_acc', batch_acc)\n",
    "                        val_acc.append(batch_acc)\n",
    "\n",
    "                    test_writer.add_summary(summary,iteration)\n",
    "                    print(\"Dev loss: {:.3f}\".format(dev_loss))\n",
    "                    print(\"Dev acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "                iteration +=1\n",
    "        saver.save(sess, \"output/checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  41  195    4 ...,    0    0    0]\n",
      " [  41 2656    6 ...,    0    0    0]\n",
      " [  41  101   44 ...,    0    0    0]\n",
      " ..., \n",
      " [   7  970   15 ...,    0    0    0]\n",
      " [  41   52    4 ...,    0    0    0]\n",
      " [ 142 1744  288 ...,    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size_options = [256]\n",
    "lstm_layers_options = [1]\n",
    "learning_rate_options = [0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm size: 256 nb layers : 1 learn rate : 0.001\n",
      "Epoch: 0/200 Iteration: 100 Train loss: 0.140\n",
      "Dev loss: 0.173\n",
      "Dev acc: 0.830\n",
      "Epoch: 0/200 Iteration: 200 Train loss: 0.176\n",
      "Dev loss: 0.170\n",
      "Dev acc: 0.830\n",
      "Epoch: 0/200 Iteration: 300 Train loss: 0.114\n",
      "Dev loss: 0.172\n",
      "Dev acc: 0.830\n",
      "Epoch: 0/200 Iteration: 400 Train loss: 0.140\n",
      "Dev loss: 0.168\n",
      "Dev acc: 0.830\n",
      "Epoch: 0/200 Iteration: 500 Train loss: 0.107\n",
      "Dev loss: 0.169\n",
      "Dev acc: 0.830\n",
      "Epoch: 0/200 Iteration: 600 Train loss: 0.111\n",
      "Dev loss: 0.169\n",
      "Dev acc: 0.830\n",
      "Epoch: 0/200 Iteration: 700 Train loss: 0.138\n",
      "Dev loss: 0.169\n",
      "Dev acc: 0.830\n",
      "Epoch: 1/200 Iteration: 800 Train loss: 0.165\n",
      "Dev loss: 0.168\n",
      "Dev acc: 0.830\n",
      "Epoch: 1/200 Iteration: 900 Train loss: 0.131\n",
      "Dev loss: 0.166\n",
      "Dev acc: 0.830\n",
      "Epoch: 1/200 Iteration: 1000 Train loss: 0.128\n",
      "Dev loss: 0.172\n",
      "Dev acc: 0.830\n",
      "Epoch: 1/200 Iteration: 1100 Train loss: 0.108\n",
      "Dev loss: 0.168\n",
      "Dev acc: 0.830\n",
      "Epoch: 1/200 Iteration: 1200 Train loss: 0.132\n",
      "Dev loss: 0.170\n",
      "Dev acc: 0.830\n",
      "Epoch: 1/200 Iteration: 1300 Train loss: 0.172\n",
      "Dev loss: 0.168\n",
      "Dev acc: 0.830\n",
      "Epoch: 1/200 Iteration: 1400 Train loss: 0.118\n",
      "Dev loss: 0.166\n",
      "Dev acc: 0.830\n",
      "Epoch: 1/200 Iteration: 1500 Train loss: 0.191\n",
      "Dev loss: 0.169\n",
      "Dev acc: 0.830\n",
      "Epoch: 2/200 Iteration: 1600 Train loss: 0.152\n",
      "Dev loss: 0.167\n",
      "Dev acc: 0.830\n",
      "Epoch: 2/200 Iteration: 1700 Train loss: 0.165\n",
      "Dev loss: 0.168\n",
      "Dev acc: 0.830\n",
      "Epoch: 2/200 Iteration: 1800 Train loss: 0.118\n",
      "Dev loss: 0.168\n",
      "Dev acc: 0.830\n",
      "Epoch: 2/200 Iteration: 1900 Train loss: 0.125\n",
      "Dev loss: 0.169\n",
      "Dev acc: 0.830\n",
      "Epoch: 2/200 Iteration: 2000 Train loss: 0.127\n",
      "Dev loss: 0.167\n",
      "Dev acc: 0.830\n",
      "Epoch: 2/200 Iteration: 2100 Train loss: 0.101\n",
      "Dev loss: 0.169\n",
      "Dev acc: 0.830\n",
      "Epoch: 2/200 Iteration: 2200 Train loss: 0.139\n",
      "Dev loss: 0.163\n",
      "Dev acc: 0.830\n",
      "Epoch: 2/200 Iteration: 2300 Train loss: 0.129\n",
      "Dev loss: 0.165\n",
      "Dev acc: 0.830\n",
      "Epoch: 3/200 Iteration: 2400 Train loss: 0.124\n",
      "Dev loss: 0.163\n",
      "Dev acc: 0.830\n",
      "Epoch: 3/200 Iteration: 2500 Train loss: 0.100\n",
      "Dev loss: 0.165\n",
      "Dev acc: 0.830\n",
      "Epoch: 3/200 Iteration: 2600 Train loss: 0.087\n",
      "Dev loss: 0.166\n",
      "Dev acc: 0.830\n",
      "Epoch: 3/200 Iteration: 2700 Train loss: 0.163\n",
      "Dev loss: 0.164\n",
      "Dev acc: 0.830\n",
      "Epoch: 3/200 Iteration: 2800 Train loss: 0.094\n",
      "Dev loss: 0.166\n",
      "Dev acc: 0.830\n",
      "Epoch: 3/200 Iteration: 2900 Train loss: 0.126\n",
      "Dev loss: 0.164\n",
      "Dev acc: 0.830\n",
      "Epoch: 3/200 Iteration: 3000 Train loss: 0.118\n",
      "Dev loss: 0.168\n",
      "Dev acc: 0.830\n",
      "Epoch: 3/200 Iteration: 3100 Train loss: 0.115\n",
      "Dev loss: 0.156\n",
      "Dev acc: 0.830\n",
      "Epoch: 4/200 Iteration: 3200 Train loss: 0.135\n",
      "Dev loss: 0.149\n",
      "Dev acc: 0.830\n",
      "Epoch: 4/200 Iteration: 3300 Train loss: 0.121\n",
      "Dev loss: 0.147\n",
      "Dev acc: 0.830\n",
      "Epoch: 4/200 Iteration: 3400 Train loss: 0.127\n",
      "Dev loss: 0.141\n",
      "Dev acc: 0.834\n",
      "Epoch: 4/200 Iteration: 3500 Train loss: 0.113\n",
      "Dev loss: 0.138\n",
      "Dev acc: 0.835\n",
      "Epoch: 4/200 Iteration: 3600 Train loss: 0.126\n",
      "Dev loss: 0.127\n",
      "Dev acc: 0.846\n",
      "Epoch: 4/200 Iteration: 3700 Train loss: 0.141\n",
      "Dev loss: 0.136\n",
      "Dev acc: 0.851\n",
      "Epoch: 4/200 Iteration: 3800 Train loss: 0.130\n",
      "Dev loss: 0.130\n",
      "Dev acc: 0.858\n",
      "Epoch: 4/200 Iteration: 3900 Train loss: 0.140\n",
      "Dev loss: 0.124\n",
      "Dev acc: 0.864\n",
      "Epoch: 5/200 Iteration: 4000 Train loss: 0.075\n",
      "Dev loss: 0.129\n",
      "Dev acc: 0.862\n",
      "Epoch: 5/200 Iteration: 4100 Train loss: 0.095\n",
      "Dev loss: 0.114\n",
      "Dev acc: 0.874\n",
      "Epoch: 5/200 Iteration: 4200 Train loss: 0.086\n",
      "Dev loss: 0.120\n",
      "Dev acc: 0.873\n",
      "Epoch: 5/200 Iteration: 4300 Train loss: 0.112\n",
      "Dev loss: 0.128\n",
      "Dev acc: 0.860\n",
      "Epoch: 5/200 Iteration: 4400 Train loss: 0.091\n",
      "Dev loss: 0.100\n",
      "Dev acc: 0.872\n",
      "Epoch: 5/200 Iteration: 4500 Train loss: 0.105\n",
      "Dev loss: 0.091\n",
      "Dev acc: 0.877\n",
      "Epoch: 5/200 Iteration: 4600 Train loss: 0.070\n",
      "Dev loss: 0.114\n",
      "Dev acc: 0.880\n",
      "Epoch: 6/200 Iteration: 4700 Train loss: 0.041\n",
      "Dev loss: 0.090\n",
      "Dev acc: 0.891\n",
      "Epoch: 6/200 Iteration: 4800 Train loss: 0.054\n",
      "Dev loss: 0.082\n",
      "Dev acc: 0.892\n",
      "Epoch: 6/200 Iteration: 4900 Train loss: 0.068\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.895\n",
      "Epoch: 6/200 Iteration: 5000 Train loss: 0.057\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.897\n",
      "Epoch: 6/200 Iteration: 5100 Train loss: 0.073\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.898\n",
      "Epoch: 6/200 Iteration: 5200 Train loss: 0.058\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.898\n",
      "Epoch: 6/200 Iteration: 5300 Train loss: 0.073\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.900\n",
      "Epoch: 6/200 Iteration: 5400 Train loss: 0.082\n",
      "Dev loss: 0.100\n",
      "Dev acc: 0.893\n",
      "Epoch: 7/200 Iteration: 5500 Train loss: 0.073\n",
      "Dev loss: 0.087\n",
      "Dev acc: 0.903\n",
      "Epoch: 7/200 Iteration: 5600 Train loss: 0.072\n",
      "Dev loss: 0.066\n",
      "Dev acc: 0.901\n",
      "Epoch: 7/200 Iteration: 5700 Train loss: 0.066\n",
      "Dev loss: 0.074\n",
      "Dev acc: 0.905\n",
      "Epoch: 7/200 Iteration: 5800 Train loss: 0.057\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.905\n",
      "Epoch: 7/200 Iteration: 5900 Train loss: 0.073\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.907\n",
      "Epoch: 7/200 Iteration: 6000 Train loss: 0.057\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.906\n",
      "Epoch: 7/200 Iteration: 6100 Train loss: 0.052\n",
      "Dev loss: 0.092\n",
      "Dev acc: 0.893\n",
      "Epoch: 7/200 Iteration: 6200 Train loss: 0.028\n",
      "Dev loss: 0.073\n",
      "Dev acc: 0.908\n",
      "Epoch: 8/200 Iteration: 6300 Train loss: 0.043\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.909\n",
      "Epoch: 8/200 Iteration: 6400 Train loss: 0.048\n",
      "Dev loss: 0.067\n",
      "Dev acc: 0.909\n",
      "Epoch: 8/200 Iteration: 6500 Train loss: 0.058\n",
      "Dev loss: 0.068\n",
      "Dev acc: 0.911\n",
      "Epoch: 8/200 Iteration: 6600 Train loss: 0.048\n",
      "Dev loss: 0.067\n",
      "Dev acc: 0.906\n",
      "Epoch: 8/200 Iteration: 6700 Train loss: 0.081\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.908\n",
      "Epoch: 8/200 Iteration: 6800 Train loss: 0.036\n",
      "Dev loss: 0.069\n",
      "Dev acc: 0.898\n",
      "Epoch: 8/200 Iteration: 6900 Train loss: 0.044\n",
      "Dev loss: 0.084\n",
      "Dev acc: 0.901\n",
      "Epoch: 8/200 Iteration: 7000 Train loss: 0.048\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.912\n",
      "Epoch: 9/200 Iteration: 7100 Train loss: 0.060\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.891\n",
      "Epoch: 9/200 Iteration: 7200 Train loss: 0.059\n",
      "Dev loss: 0.068\n",
      "Dev acc: 0.911\n",
      "Epoch: 9/200 Iteration: 7300 Train loss: 0.062\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.907\n",
      "Epoch: 9/200 Iteration: 7400 Train loss: 0.057\n",
      "Dev loss: 0.067\n",
      "Dev acc: 0.912\n",
      "Epoch: 9/200 Iteration: 7500 Train loss: 0.045\n",
      "Dev loss: 0.073\n",
      "Dev acc: 0.911\n",
      "Epoch: 9/200 Iteration: 7600 Train loss: 0.058\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.909\n",
      "Epoch: 9/200 Iteration: 7700 Train loss: 0.056\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.909\n",
      "Epoch: 9/200 Iteration: 7800 Train loss: 0.058\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.909\n",
      "Epoch: 10/200 Iteration: 7900 Train loss: 0.070\n",
      "Dev loss: 0.069\n",
      "Dev acc: 0.894\n",
      "Epoch: 10/200 Iteration: 8000 Train loss: 0.030\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.902\n",
      "Epoch: 10/200 Iteration: 8100 Train loss: 0.048\n",
      "Dev loss: 0.084\n",
      "Dev acc: 0.903\n",
      "Epoch: 10/200 Iteration: 8200 Train loss: 0.056\n",
      "Dev loss: 0.070\n",
      "Dev acc: 0.912\n",
      "Epoch: 10/200 Iteration: 8300 Train loss: 0.042\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.907\n",
      "Epoch: 10/200 Iteration: 8400 Train loss: 0.011\n",
      "Dev loss: 0.062\n",
      "Dev acc: 0.912\n",
      "Epoch: 10/200 Iteration: 8500 Train loss: 0.062\n",
      "Dev loss: 0.068\n",
      "Dev acc: 0.912\n",
      "Epoch: 11/200 Iteration: 8600 Train loss: 0.070\n",
      "Dev loss: 0.074\n",
      "Dev acc: 0.911\n",
      "Epoch: 11/200 Iteration: 8700 Train loss: 0.013\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.893\n",
      "Epoch: 11/200 Iteration: 8800 Train loss: 0.054\n",
      "Dev loss: 0.097\n",
      "Dev acc: 0.887\n",
      "Epoch: 11/200 Iteration: 8900 Train loss: 0.034\n",
      "Dev loss: 0.088\n",
      "Dev acc: 0.906\n",
      "Epoch: 11/200 Iteration: 9000 Train loss: 0.016\n",
      "Dev loss: 0.090\n",
      "Dev acc: 0.911\n",
      "Epoch: 11/200 Iteration: 9100 Train loss: 0.033\n",
      "Dev loss: 0.095\n",
      "Dev acc: 0.909\n",
      "Epoch: 11/200 Iteration: 9200 Train loss: 0.026\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.894\n",
      "Epoch: 11/200 Iteration: 9300 Train loss: 0.040\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.912\n",
      "Epoch: 12/200 Iteration: 9400 Train loss: 0.035\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.915\n",
      "Epoch: 12/200 Iteration: 9500 Train loss: 0.015\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.913\n",
      "Epoch: 12/200 Iteration: 9600 Train loss: 0.025\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.913\n",
      "Epoch: 12/200 Iteration: 9700 Train loss: 0.027\n",
      "Dev loss: 0.092\n",
      "Dev acc: 0.906\n",
      "Epoch: 12/200 Iteration: 9800 Train loss: 0.030\n",
      "Dev loss: 0.087\n",
      "Dev acc: 0.908\n",
      "Epoch: 12/200 Iteration: 9900 Train loss: 0.018\n",
      "Dev loss: 0.092\n",
      "Dev acc: 0.908\n",
      "Epoch: 12/200 Iteration: 10000 Train loss: 0.060\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.909\n",
      "Epoch: 12/200 Iteration: 10100 Train loss: 0.042\n",
      "Dev loss: 0.074\n",
      "Dev acc: 0.912\n",
      "Epoch: 13/200 Iteration: 10200 Train loss: 0.064\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.912\n",
      "Epoch: 13/200 Iteration: 10300 Train loss: 0.043\n",
      "Dev loss: 0.074\n",
      "Dev acc: 0.912\n",
      "Epoch: 13/200 Iteration: 10400 Train loss: 0.018\n",
      "Dev loss: 0.064\n",
      "Dev acc: 0.913\n",
      "Epoch: 13/200 Iteration: 10500 Train loss: 0.055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 0.077\n",
      "Dev acc: 0.909\n",
      "Epoch: 13/200 Iteration: 10600 Train loss: 0.013\n",
      "Dev loss: 0.082\n",
      "Dev acc: 0.910\n",
      "Epoch: 13/200 Iteration: 10700 Train loss: 0.038\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.903\n",
      "Epoch: 13/200 Iteration: 10800 Train loss: 0.035\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.912\n",
      "Epoch: 13/200 Iteration: 10900 Train loss: 0.037\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.909\n",
      "Epoch: 14/200 Iteration: 11000 Train loss: 0.020\n",
      "Dev loss: 0.099\n",
      "Dev acc: 0.883\n",
      "Epoch: 14/200 Iteration: 11100 Train loss: 0.040\n",
      "Dev loss: 0.070\n",
      "Dev acc: 0.906\n",
      "Epoch: 14/200 Iteration: 11200 Train loss: 0.024\n",
      "Dev loss: 0.067\n",
      "Dev acc: 0.915\n",
      "Epoch: 14/200 Iteration: 11300 Train loss: 0.029\n",
      "Dev loss: 0.062\n",
      "Dev acc: 0.914\n",
      "Epoch: 14/200 Iteration: 11400 Train loss: 0.025\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.913\n",
      "Epoch: 14/200 Iteration: 11500 Train loss: 0.048\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.910\n",
      "Epoch: 14/200 Iteration: 11600 Train loss: 0.033\n",
      "Dev loss: 0.069\n",
      "Dev acc: 0.911\n",
      "Epoch: 14/200 Iteration: 11700 Train loss: 0.033\n",
      "Dev loss: 0.084\n",
      "Dev acc: 0.909\n",
      "Epoch: 15/200 Iteration: 11800 Train loss: 0.005\n",
      "Dev loss: 0.069\n",
      "Dev acc: 0.913\n",
      "Epoch: 15/200 Iteration: 11900 Train loss: 0.213\n",
      "Dev loss: 0.180\n",
      "Dev acc: 0.798\n",
      "Epoch: 15/200 Iteration: 12000 Train loss: 0.070\n",
      "Dev loss: 0.093\n",
      "Dev acc: 0.881\n",
      "Epoch: 15/200 Iteration: 12100 Train loss: 0.043\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.891\n",
      "Epoch: 15/200 Iteration: 12200 Train loss: 0.070\n",
      "Dev loss: 0.074\n",
      "Dev acc: 0.896\n",
      "Epoch: 15/200 Iteration: 12300 Train loss: 0.034\n",
      "Dev loss: 0.069\n",
      "Dev acc: 0.894\n",
      "Epoch: 15/200 Iteration: 12400 Train loss: 0.065\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.897\n",
      "Epoch: 16/200 Iteration: 12500 Train loss: 0.038\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.891\n",
      "Epoch: 16/200 Iteration: 12600 Train loss: 0.052\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.896\n",
      "Epoch: 16/200 Iteration: 12700 Train loss: 0.041\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.897\n",
      "Epoch: 16/200 Iteration: 12800 Train loss: 0.047\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.901\n",
      "Epoch: 16/200 Iteration: 12900 Train loss: 0.039\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.902\n",
      "Epoch: 16/200 Iteration: 13000 Train loss: 0.026\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.904\n",
      "Epoch: 16/200 Iteration: 13100 Train loss: 0.079\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.904\n",
      "Epoch: 16/200 Iteration: 13200 Train loss: 0.030\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.904\n",
      "Epoch: 17/200 Iteration: 13300 Train loss: 0.041\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.905\n",
      "Epoch: 17/200 Iteration: 13400 Train loss: 0.047\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.905\n",
      "Epoch: 17/200 Iteration: 13500 Train loss: 0.033\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.907\n",
      "Epoch: 17/200 Iteration: 13600 Train loss: 0.045\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.902\n",
      "Epoch: 17/200 Iteration: 13700 Train loss: 0.036\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.907\n",
      "Epoch: 17/200 Iteration: 13800 Train loss: 0.043\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.905\n",
      "Epoch: 17/200 Iteration: 13900 Train loss: 0.038\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.904\n",
      "Epoch: 17/200 Iteration: 14000 Train loss: 0.032\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.906\n",
      "Epoch: 18/200 Iteration: 14100 Train loss: 0.053\n",
      "Dev loss: 0.082\n",
      "Dev acc: 0.907\n",
      "Epoch: 18/200 Iteration: 14200 Train loss: 0.011\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.907\n",
      "Epoch: 18/200 Iteration: 14300 Train loss: 0.041\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.909\n",
      "Epoch: 18/200 Iteration: 14400 Train loss: 0.029\n",
      "Dev loss: 0.082\n",
      "Dev acc: 0.909\n",
      "Epoch: 18/200 Iteration: 14500 Train loss: 0.016\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.910\n",
      "Epoch: 18/200 Iteration: 14600 Train loss: 0.034\n",
      "Dev loss: 0.077\n",
      "Dev acc: 0.909\n",
      "Epoch: 18/200 Iteration: 14700 Train loss: 0.023\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.908\n",
      "Epoch: 18/200 Iteration: 14800 Train loss: 0.190\n",
      "Dev loss: 0.108\n",
      "Dev acc: 0.872\n",
      "Epoch: 19/200 Iteration: 14900 Train loss: 0.049\n",
      "Dev loss: 0.074\n",
      "Dev acc: 0.902\n",
      "Epoch: 19/200 Iteration: 15000 Train loss: 0.058\n",
      "Dev loss: 0.077\n",
      "Dev acc: 0.905\n",
      "Epoch: 19/200 Iteration: 15100 Train loss: 0.060\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.907\n",
      "Epoch: 19/200 Iteration: 15200 Train loss: 0.073\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.905\n",
      "Epoch: 19/200 Iteration: 15300 Train loss: 0.024\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.904\n",
      "Epoch: 19/200 Iteration: 15400 Train loss: 0.051\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.909\n",
      "Epoch: 19/200 Iteration: 15500 Train loss: 0.047\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.909\n",
      "Epoch: 19/200 Iteration: 15600 Train loss: 0.027\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.910\n",
      "Epoch: 20/200 Iteration: 15700 Train loss: 0.030\n",
      "Dev loss: 0.073\n",
      "Dev acc: 0.911\n",
      "Epoch: 20/200 Iteration: 15800 Train loss: 0.081\n",
      "Dev loss: 0.094\n",
      "Dev acc: 0.870\n",
      "Epoch: 20/200 Iteration: 15900 Train loss: 0.018\n",
      "Dev loss: 0.064\n",
      "Dev acc: 0.911\n",
      "Epoch: 20/200 Iteration: 16000 Train loss: 0.026\n",
      "Dev loss: 0.062\n",
      "Dev acc: 0.912\n",
      "Epoch: 20/200 Iteration: 16100 Train loss: 0.034\n",
      "Dev loss: 0.057\n",
      "Dev acc: 0.912\n",
      "Epoch: 20/200 Iteration: 16200 Train loss: 0.015\n",
      "Dev loss: 0.064\n",
      "Dev acc: 0.909\n",
      "Epoch: 20/200 Iteration: 16300 Train loss: 0.026\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.912\n",
      "Epoch: 20/200 Iteration: 16400 Train loss: 0.026\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.911\n",
      "Epoch: 21/200 Iteration: 16500 Train loss: 0.016\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.913\n",
      "Epoch: 21/200 Iteration: 16600 Train loss: 0.022\n",
      "Dev loss: 0.068\n",
      "Dev acc: 0.913\n",
      "Epoch: 21/200 Iteration: 16700 Train loss: 0.047\n",
      "Dev loss: 0.077\n",
      "Dev acc: 0.912\n",
      "Epoch: 21/200 Iteration: 16800 Train loss: 0.038\n",
      "Dev loss: 0.067\n",
      "Dev acc: 0.913\n",
      "Epoch: 21/200 Iteration: 16900 Train loss: 0.030\n",
      "Dev loss: 0.069\n",
      "Dev acc: 0.913\n",
      "Epoch: 21/200 Iteration: 17000 Train loss: 0.047\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.912\n",
      "Epoch: 21/200 Iteration: 17100 Train loss: 0.017\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.912\n",
      "Epoch: 22/200 Iteration: 17200 Train loss: 0.026\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.912\n",
      "Epoch: 22/200 Iteration: 17300 Train loss: 0.025\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.909\n",
      "Epoch: 22/200 Iteration: 17400 Train loss: 0.013\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.911\n",
      "Epoch: 22/200 Iteration: 17500 Train loss: 0.032\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.912\n",
      "Epoch: 22/200 Iteration: 17600 Train loss: 0.009\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.914\n",
      "Epoch: 22/200 Iteration: 17700 Train loss: 0.032\n",
      "Dev loss: 0.062\n",
      "Dev acc: 0.912\n",
      "Epoch: 22/200 Iteration: 17800 Train loss: 0.012\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.912\n",
      "Epoch: 22/200 Iteration: 17900 Train loss: 0.030\n",
      "Dev loss: 0.069\n",
      "Dev acc: 0.914\n",
      "Epoch: 23/200 Iteration: 18000 Train loss: 0.013\n",
      "Dev loss: 0.061\n",
      "Dev acc: 0.914\n",
      "Epoch: 23/200 Iteration: 18100 Train loss: 0.018\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.913\n",
      "Epoch: 23/200 Iteration: 18200 Train loss: 0.007\n",
      "Dev loss: 0.063\n",
      "Dev acc: 0.914\n",
      "Epoch: 23/200 Iteration: 18300 Train loss: 0.020\n",
      "Dev loss: 0.073\n",
      "Dev acc: 0.915\n",
      "Epoch: 23/200 Iteration: 18400 Train loss: 0.032\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.910\n",
      "Epoch: 23/200 Iteration: 18500 Train loss: 0.027\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.915\n",
      "Epoch: 23/200 Iteration: 18600 Train loss: 0.009\n",
      "Dev loss: 0.074\n",
      "Dev acc: 0.915\n",
      "Epoch: 23/200 Iteration: 18700 Train loss: 0.033\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.913\n",
      "Epoch: 24/200 Iteration: 18800 Train loss: 0.031\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.914\n",
      "Epoch: 24/200 Iteration: 18900 Train loss: 0.013\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.913\n",
      "Epoch: 24/200 Iteration: 19000 Train loss: 0.002\n",
      "Dev loss: 0.065\n",
      "Dev acc: 0.914\n",
      "Epoch: 24/200 Iteration: 19100 Train loss: 0.014\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.916\n",
      "Epoch: 24/200 Iteration: 19200 Train loss: 0.042\n",
      "Dev loss: 0.077\n",
      "Dev acc: 0.914\n",
      "Epoch: 24/200 Iteration: 19300 Train loss: 0.019\n",
      "Dev loss: 0.077\n",
      "Dev acc: 0.914\n",
      "Epoch: 24/200 Iteration: 19400 Train loss: 0.024\n",
      "Dev loss: 0.074\n",
      "Dev acc: 0.913\n",
      "Epoch: 24/200 Iteration: 19500 Train loss: 0.009\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.910\n",
      "Epoch: 25/200 Iteration: 19600 Train loss: 0.034\n",
      "Dev loss: 0.073\n",
      "Dev acc: 0.902\n",
      "Epoch: 25/200 Iteration: 19700 Train loss: 0.008\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.914\n",
      "Epoch: 25/200 Iteration: 19800 Train loss: 0.021\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.910\n",
      "Epoch: 25/200 Iteration: 19900 Train loss: 0.043\n",
      "Dev loss: 0.089\n",
      "Dev acc: 0.911\n",
      "Epoch: 25/200 Iteration: 20000 Train loss: 0.008\n",
      "Dev loss: 0.085\n",
      "Dev acc: 0.915\n",
      "Epoch: 25/200 Iteration: 20100 Train loss: 0.010\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.910\n",
      "Epoch: 25/200 Iteration: 20200 Train loss: 0.026\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.913\n",
      "Epoch: 25/200 Iteration: 20300 Train loss: 0.022\n",
      "Dev loss: 0.097\n",
      "Dev acc: 0.913\n",
      "Epoch: 26/200 Iteration: 20400 Train loss: 0.012\n",
      "Dev loss: 0.090\n",
      "Dev acc: 0.911\n",
      "Epoch: 26/200 Iteration: 20500 Train loss: 0.020\n",
      "Dev loss: 0.067\n",
      "Dev acc: 0.911\n",
      "Epoch: 26/200 Iteration: 20600 Train loss: 0.017\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.912\n",
      "Epoch: 26/200 Iteration: 20700 Train loss: 0.021\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.915\n",
      "Epoch: 26/200 Iteration: 20800 Train loss: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 0.071\n",
      "Dev acc: 0.914\n",
      "Epoch: 26/200 Iteration: 20900 Train loss: 0.016\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.915\n",
      "Epoch: 26/200 Iteration: 21000 Train loss: 0.010\n",
      "Dev loss: 0.091\n",
      "Dev acc: 0.914\n",
      "Epoch: 27/200 Iteration: 21100 Train loss: 0.024\n",
      "Dev loss: 0.097\n",
      "Dev acc: 0.902\n",
      "Epoch: 27/200 Iteration: 21200 Train loss: 0.014\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.913\n",
      "Epoch: 27/200 Iteration: 21300 Train loss: 0.027\n",
      "Dev loss: 0.097\n",
      "Dev acc: 0.901\n",
      "Epoch: 27/200 Iteration: 21400 Train loss: 0.008\n",
      "Dev loss: 0.090\n",
      "Dev acc: 0.913\n",
      "Epoch: 27/200 Iteration: 21500 Train loss: 0.037\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.914\n",
      "Epoch: 27/200 Iteration: 21600 Train loss: 0.039\n",
      "Dev loss: 0.082\n",
      "Dev acc: 0.910\n",
      "Epoch: 27/200 Iteration: 21700 Train loss: 0.017\n",
      "Dev loss: 0.089\n",
      "Dev acc: 0.911\n",
      "Epoch: 27/200 Iteration: 21800 Train loss: 0.016\n",
      "Dev loss: 0.099\n",
      "Dev acc: 0.914\n",
      "Epoch: 28/200 Iteration: 21900 Train loss: 0.019\n",
      "Dev loss: 0.098\n",
      "Dev acc: 0.912\n",
      "Epoch: 28/200 Iteration: 22000 Train loss: 0.035\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.912\n",
      "Epoch: 28/200 Iteration: 22100 Train loss: 0.024\n",
      "Dev loss: 0.073\n",
      "Dev acc: 0.914\n",
      "Epoch: 28/200 Iteration: 22200 Train loss: 0.016\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.915\n",
      "Epoch: 28/200 Iteration: 22300 Train loss: 0.001\n",
      "Dev loss: 0.111\n",
      "Dev acc: 0.909\n",
      "Epoch: 28/200 Iteration: 22400 Train loss: 0.006\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.912\n",
      "Epoch: 28/200 Iteration: 22500 Train loss: 0.013\n",
      "Dev loss: 0.063\n",
      "Dev acc: 0.913\n",
      "Epoch: 28/200 Iteration: 22600 Train loss: 0.008\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.914\n",
      "Epoch: 29/200 Iteration: 22700 Train loss: 0.009\n",
      "Dev loss: 0.070\n",
      "Dev acc: 0.915\n",
      "Epoch: 29/200 Iteration: 22800 Train loss: 0.019\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.901\n",
      "Epoch: 29/200 Iteration: 22900 Train loss: 0.038\n",
      "Dev loss: 0.058\n",
      "Dev acc: 0.912\n",
      "Epoch: 29/200 Iteration: 23000 Train loss: 0.008\n",
      "Dev loss: 0.063\n",
      "Dev acc: 0.914\n",
      "Epoch: 29/200 Iteration: 23100 Train loss: 0.026\n",
      "Dev loss: 0.068\n",
      "Dev acc: 0.913\n",
      "Epoch: 29/200 Iteration: 23200 Train loss: 0.029\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.912\n",
      "Epoch: 29/200 Iteration: 23300 Train loss: 0.031\n",
      "Dev loss: 0.090\n",
      "Dev acc: 0.903\n",
      "Epoch: 29/200 Iteration: 23400 Train loss: 0.000\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.915\n",
      "Epoch: 30/200 Iteration: 23500 Train loss: 0.001\n",
      "Dev loss: 0.077\n",
      "Dev acc: 0.912\n",
      "Epoch: 30/200 Iteration: 23600 Train loss: 0.031\n",
      "Dev loss: 0.070\n",
      "Dev acc: 0.914\n",
      "Epoch: 30/200 Iteration: 23700 Train loss: 0.010\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.904\n",
      "Epoch: 30/200 Iteration: 23800 Train loss: 0.007\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.915\n",
      "Epoch: 30/200 Iteration: 23900 Train loss: 0.008\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.915\n",
      "Epoch: 30/200 Iteration: 24000 Train loss: 0.015\n",
      "Dev loss: 0.102\n",
      "Dev acc: 0.903\n",
      "Epoch: 30/200 Iteration: 24100 Train loss: 0.031\n",
      "Dev loss: 0.073\n",
      "Dev acc: 0.915\n",
      "Epoch: 30/200 Iteration: 24200 Train loss: 0.007\n",
      "Dev loss: 0.085\n",
      "Dev acc: 0.905\n",
      "Epoch: 31/200 Iteration: 24300 Train loss: 0.008\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.910\n",
      "Epoch: 31/200 Iteration: 24400 Train loss: 0.031\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.912\n",
      "Epoch: 31/200 Iteration: 24500 Train loss: 0.003\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.914\n",
      "Epoch: 31/200 Iteration: 24600 Train loss: 0.008\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.914\n",
      "Epoch: 31/200 Iteration: 24700 Train loss: 0.003\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.912\n",
      "Epoch: 31/200 Iteration: 24800 Train loss: 0.048\n",
      "Dev loss: 0.063\n",
      "Dev acc: 0.915\n",
      "Epoch: 31/200 Iteration: 24900 Train loss: 0.017\n",
      "Dev loss: 0.097\n",
      "Dev acc: 0.910\n",
      "Epoch: 32/200 Iteration: 25000 Train loss: 0.002\n",
      "Dev loss: 0.082\n",
      "Dev acc: 0.913\n",
      "Epoch: 32/200 Iteration: 25100 Train loss: 0.005\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.914\n",
      "Epoch: 32/200 Iteration: 25200 Train loss: 0.001\n",
      "Dev loss: 0.091\n",
      "Dev acc: 0.910\n",
      "Epoch: 32/200 Iteration: 25300 Train loss: 0.017\n",
      "Dev loss: 0.072\n",
      "Dev acc: 0.915\n",
      "Epoch: 32/200 Iteration: 25400 Train loss: 0.002\n",
      "Dev loss: 0.099\n",
      "Dev acc: 0.913\n",
      "Epoch: 32/200 Iteration: 25500 Train loss: 0.010\n",
      "Dev loss: 0.096\n",
      "Dev acc: 0.913\n",
      "Epoch: 32/200 Iteration: 25600 Train loss: 0.013\n",
      "Dev loss: 0.105\n",
      "Dev acc: 0.913\n",
      "Epoch: 32/200 Iteration: 25700 Train loss: 0.011\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.913\n",
      "Epoch: 33/200 Iteration: 25800 Train loss: 0.029\n",
      "Dev loss: 0.097\n",
      "Dev acc: 0.907\n",
      "Epoch: 33/200 Iteration: 25900 Train loss: 0.013\n",
      "Dev loss: 0.097\n",
      "Dev acc: 0.906\n",
      "Epoch: 33/200 Iteration: 26000 Train loss: 0.002\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.915\n",
      "Epoch: 33/200 Iteration: 26100 Train loss: 0.012\n",
      "Dev loss: 0.120\n",
      "Dev acc: 0.905\n",
      "Epoch: 33/200 Iteration: 26200 Train loss: 0.011\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.915\n",
      "Epoch: 33/200 Iteration: 26300 Train loss: 0.012\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.907\n",
      "Epoch: 33/200 Iteration: 26400 Train loss: 0.014\n",
      "Dev loss: 0.112\n",
      "Dev acc: 0.907\n",
      "Epoch: 33/200 Iteration: 26500 Train loss: 0.018\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.914\n",
      "Epoch: 34/200 Iteration: 26600 Train loss: 0.011\n",
      "Dev loss: 0.105\n",
      "Dev acc: 0.913\n",
      "Epoch: 34/200 Iteration: 26700 Train loss: 0.010\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.908\n",
      "Epoch: 34/200 Iteration: 26800 Train loss: 0.001\n",
      "Dev loss: 0.110\n",
      "Dev acc: 0.910\n",
      "Epoch: 34/200 Iteration: 26900 Train loss: 0.021\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.912\n",
      "Epoch: 34/200 Iteration: 27000 Train loss: 0.021\n",
      "Dev loss: 0.116\n",
      "Dev acc: 0.912\n",
      "Epoch: 34/200 Iteration: 27100 Train loss: 0.037\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.913\n",
      "Epoch: 34/200 Iteration: 27200 Train loss: 0.023\n",
      "Dev loss: 0.073\n",
      "Dev acc: 0.915\n",
      "Epoch: 34/200 Iteration: 27300 Train loss: 0.011\n",
      "Dev loss: 0.114\n",
      "Dev acc: 0.909\n",
      "Epoch: 35/200 Iteration: 27400 Train loss: 0.032\n",
      "Dev loss: 0.100\n",
      "Dev acc: 0.915\n",
      "Epoch: 35/200 Iteration: 27500 Train loss: 0.013\n",
      "Dev loss: 0.067\n",
      "Dev acc: 0.916\n",
      "Epoch: 35/200 Iteration: 27600 Train loss: 0.011\n",
      "Dev loss: 0.066\n",
      "Dev acc: 0.915\n",
      "Epoch: 35/200 Iteration: 27700 Train loss: 0.020\n",
      "Dev loss: 0.064\n",
      "Dev acc: 0.914\n",
      "Epoch: 35/200 Iteration: 27800 Train loss: 0.015\n",
      "Dev loss: 0.098\n",
      "Dev acc: 0.911\n",
      "Epoch: 35/200 Iteration: 27900 Train loss: 0.023\n",
      "Dev loss: 0.078\n",
      "Dev acc: 0.915\n",
      "Epoch: 35/200 Iteration: 28000 Train loss: 0.023\n",
      "Dev loss: 0.090\n",
      "Dev acc: 0.912\n",
      "Epoch: 35/200 Iteration: 28100 Train loss: 0.002\n",
      "Dev loss: 0.086\n",
      "Dev acc: 0.914\n",
      "Epoch: 36/200 Iteration: 28200 Train loss: 0.019\n",
      "Dev loss: 0.097\n",
      "Dev acc: 0.915\n",
      "Epoch: 36/200 Iteration: 28300 Train loss: 0.021\n",
      "Dev loss: 0.107\n",
      "Dev acc: 0.905\n",
      "Epoch: 36/200 Iteration: 28400 Train loss: 0.033\n",
      "Dev loss: 0.069\n",
      "Dev acc: 0.914\n",
      "Epoch: 36/200 Iteration: 28500 Train loss: 0.029\n",
      "Dev loss: 0.096\n",
      "Dev acc: 0.913\n",
      "Epoch: 36/200 Iteration: 28600 Train loss: 0.002\n",
      "Dev loss: 0.079\n",
      "Dev acc: 0.915\n",
      "Epoch: 36/200 Iteration: 28700 Train loss: 0.029\n",
      "Dev loss: 0.089\n",
      "Dev acc: 0.914\n",
      "Epoch: 36/200 Iteration: 28800 Train loss: 0.012\n",
      "Dev loss: 0.083\n",
      "Dev acc: 0.915\n",
      "Epoch: 37/200 Iteration: 28900 Train loss: 0.040\n",
      "Dev loss: 0.085\n",
      "Dev acc: 0.914\n",
      "Epoch: 37/200 Iteration: 29000 Train loss: 0.015\n",
      "Dev loss: 0.095\n",
      "Dev acc: 0.913\n",
      "Epoch: 37/200 Iteration: 29100 Train loss: 0.017\n",
      "Dev loss: 0.088\n",
      "Dev acc: 0.915\n",
      "Epoch: 37/200 Iteration: 29200 Train loss: 0.024\n",
      "Dev loss: 0.093\n",
      "Dev acc: 0.912\n",
      "Epoch: 37/200 Iteration: 29300 Train loss: 0.020\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.915\n",
      "Epoch: 37/200 Iteration: 29400 Train loss: 0.015\n",
      "Dev loss: 0.068\n",
      "Dev acc: 0.915\n",
      "Epoch: 37/200 Iteration: 29500 Train loss: 0.010\n",
      "Dev loss: 0.087\n",
      "Dev acc: 0.911\n",
      "Epoch: 37/200 Iteration: 29600 Train loss: 0.010\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.912\n",
      "Epoch: 38/200 Iteration: 29700 Train loss: 0.008\n",
      "Dev loss: 0.076\n",
      "Dev acc: 0.913\n",
      "Epoch: 38/200 Iteration: 29800 Train loss: 0.009\n",
      "Dev loss: 0.099\n",
      "Dev acc: 0.902\n",
      "Epoch: 38/200 Iteration: 29900 Train loss: 0.018\n",
      "Dev loss: 0.081\n",
      "Dev acc: 0.913\n",
      "Epoch: 38/200 Iteration: 30000 Train loss: 0.003\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.916\n",
      "Epoch: 38/200 Iteration: 30100 Train loss: 0.002\n",
      "Dev loss: 0.088\n",
      "Dev acc: 0.915\n",
      "Epoch: 38/200 Iteration: 30200 Train loss: 0.013\n",
      "Dev loss: 0.082\n",
      "Dev acc: 0.912\n",
      "Epoch: 38/200 Iteration: 30300 Train loss: 0.014\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.917\n",
      "Epoch: 38/200 Iteration: 30400 Train loss: 0.030\n",
      "Dev loss: 0.071\n",
      "Dev acc: 0.914\n",
      "Epoch: 39/200 Iteration: 30500 Train loss: 0.001\n",
      "Dev loss: 0.087\n",
      "Dev acc: 0.913\n",
      "Epoch: 39/200 Iteration: 30600 Train loss: 0.015\n",
      "Dev loss: 0.120\n",
      "Dev acc: 0.907\n",
      "Epoch: 39/200 Iteration: 30700 Train loss: 0.013\n",
      "Dev loss: 0.084\n",
      "Dev acc: 0.914\n",
      "Epoch: 39/200 Iteration: 30800 Train loss: 0.019\n",
      "Dev loss: 0.122\n",
      "Dev acc: 0.912\n",
      "Epoch: 39/200 Iteration: 30900 Train loss: 0.036\n",
      "Dev loss: 0.104\n",
      "Dev acc: 0.916\n",
      "Epoch: 39/200 Iteration: 31000 Train loss: 0.016\n",
      "Dev loss: 0.075\n",
      "Dev acc: 0.913\n",
      "Epoch: 39/200 Iteration: 31100 Train loss: 0.032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss: 0.127\n",
      "Dev acc: 0.905\n",
      "Epoch: 39/200 Iteration: 31200 Train loss: 0.029\n",
      "Dev loss: 0.096\n",
      "Dev acc: 0.914\n",
      "Epoch: 40/200 Iteration: 31300 Train loss: 0.013\n",
      "Dev loss: 0.105\n",
      "Dev acc: 0.915\n",
      "Epoch: 40/200 Iteration: 31400 Train loss: 0.020\n",
      "Dev loss: 0.084\n",
      "Dev acc: 0.917\n",
      "Epoch: 40/200 Iteration: 31500 Train loss: 0.009\n",
      "Dev loss: 0.093\n",
      "Dev acc: 0.912\n",
      "Epoch: 40/200 Iteration: 31600 Train loss: 0.025\n",
      "Dev loss: 0.102\n",
      "Dev acc: 0.915\n",
      "Epoch: 40/200 Iteration: 31700 Train loss: 0.001\n",
      "Dev loss: 0.102\n",
      "Dev acc: 0.916\n",
      "Epoch: 40/200 Iteration: 31800 Train loss: 0.001\n",
      "Dev loss: 0.093\n",
      "Dev acc: 0.915\n",
      "Epoch: 40/200 Iteration: 31900 Train loss: 0.008\n",
      "Dev loss: 0.096\n",
      "Dev acc: 0.915\n",
      "Epoch: 40/200 Iteration: 32000 Train loss: 0.013\n",
      "Dev loss: 0.095\n",
      "Dev acc: 0.914\n",
      "Epoch: 41/200 Iteration: 32100 Train loss: 0.023\n",
      "Dev loss: 0.087\n",
      "Dev acc: 0.915\n",
      "Epoch: 41/200 Iteration: 32200 Train loss: 0.008\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.915\n",
      "Epoch: 41/200 Iteration: 32300 Train loss: 0.007\n",
      "Dev loss: 0.080\n",
      "Dev acc: 0.916\n",
      "Epoch: 41/200 Iteration: 32400 Train loss: 0.008\n",
      "Dev loss: 0.092\n",
      "Dev acc: 0.914\n",
      "Epoch: 41/200 Iteration: 32500 Train loss: 0.008\n",
      "Dev loss: 0.093\n",
      "Dev acc: 0.904\n",
      "Epoch: 41/200 Iteration: 32600 Train loss: 0.010\n",
      "Dev loss: 0.088\n",
      "Dev acc: 0.913\n",
      "Epoch: 41/200 Iteration: 32700 Train loss: 0.016\n",
      "Dev loss: 0.090\n",
      "Dev acc: 0.914\n",
      "Epoch: 41/200 Iteration: 32800 Train loss: 0.009\n",
      "Dev loss: 0.085\n",
      "Dev acc: 0.914\n",
      "Epoch: 42/200 Iteration: 32900 Train loss: 0.021\n",
      "Dev loss: 0.088\n",
      "Dev acc: 0.914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-f196950713f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                       lstm_layers=lstm_layers)\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-8e6cad76af11>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch, train_writer, test_writer)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 summary,loss, state, _ = sess.run([model.merged,model.cost, \n\u001b[1;32m     21\u001b[0m                                                    \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                                    model.optimizer], feed_dict=feed)\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs=200\n",
    "for lstm_size in lstm_size_options:\n",
    "    for lstm_layers in lstm_layers_options:\n",
    "        for learning_rate in learning_rate_options:\n",
    "            log_string_train = 'output/logs/2/train/lr={},rl={},ru={}'.format(learning_rate, lstm_layers, lstm_size)\n",
    "            log_string_test = 'output/logs/2/test/lr={},rl={},ru={}'.format(learning_rate, lstm_layers, lstm_size)\n",
    "            train_writer = tf.summary.FileWriter(log_string_train)\n",
    "            test_writer = tf.summary.FileWriter(log_string_test)\n",
    "            \n",
    "            print(\"lstm size: {}\".format(lstm_size),\n",
    "                    \"nb layers : {}\".format(lstm_layers),\n",
    "                    \"learn rate : {:.3f}\".format(learning_rate))\n",
    "            \n",
    "            model = build_rnn(gl_embed=hands.W,\n",
    "                      embed_size=embed_size,\n",
    "                      batch_size=batch_size,\n",
    "                      learning_rate=learning_rate,\n",
    "                      lstm_size=lstm_size,\n",
    "                      lstm_layers=lstm_layers)\n",
    "\n",
    "            train(model, epochs, train_writer,test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "print(\"run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
