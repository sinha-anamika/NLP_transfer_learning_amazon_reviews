{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rnnlm' from '/home/reachanamikasinha/project/rnnlm.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "\n",
    "import json, os, re, shutil, sys, time\n",
    "import itertools, collections\n",
    "from importlib import reload\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# NumPy and SciPy for matrix ops\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# NLTK for NLP utils\n",
    "import nltk\n",
    "import gzip\n",
    "from collections import namedtuple\n",
    "# Helper libraries\n",
    "from w266_common import vocabulary, tf_embed_viz, glove_helper\n",
    "from w266_common import utils; reload(utils)\n",
    "import rnnlm; reload(rnnlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "#Using pretrained GLove embeddings\n",
    "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400003, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hands.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to read the amazon review data files\n",
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "#df = getDF('reviews_Toys_and_Games.json.gz') #old def function corresponding to the step bt step vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 210.61572980880737\n",
      "end getDF\n",
      "time taken to load data =  210.61674904823303\n"
     ]
    }
   ],
   "source": [
    "df_hnk = getDF('reviews_Home_and_Kitchen.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_vid = getDF('reviews_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_aut = getDF('reviews_Automotive.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_toys = getDF('reviews_Toys_and_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home and Kitchen reviews train, dev and test set dataframe shape: (2552355, 9) (850785, 9) (850786, 9)\n"
     ]
    }
   ],
   "source": [
    "#Create train,dev,test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_toys,devtest = train_test_split(df_toys, test_size=0.4, random_state=42)\n",
    "# dev_toys,test_toys = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "# print('Toy reviews train, dev and test set dataframe shape:',train_toys.shape,dev_toys.shape,test_toys.shape)\n",
    "\n",
    "#For Video games reviews\n",
    "# train_vid,devtest = train_test_split(df_vid, test_size=0.4)\n",
    "# dev_vid,test_vid = train_test_split(devtest,test_size = 0.5)\n",
    "# print('Video games reviews train, dev and test set dataframe shape:',train_vid.shape,dev_vid.shape,test_vid.shape)\n",
    "\n",
    "#For Auto reviews\n",
    "# train_aut,devtest = train_test_split(df_aut, test_size=0.4)\n",
    "# dev_aut,test_aut = train_test_split(devtest,test_size = 0.5)\n",
    "# print('Auto reviews train, dev and test set dataframe shape:',train_aut.shape,dev_aut.shape,test_aut.shape)\n",
    "\n",
    "#For Home and Kitchen reviews\n",
    "train_hnk,devtest = train_test_split(df_hnk, test_size=0.4)\n",
    "dev_hnk,test_hnk = train_test_split(devtest,test_size = 0.5)\n",
    "print('Home and Kitchen reviews train, dev and test set dataframe shape:',train_hnk.shape,dev_hnk.shape,test_hnk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             reviewerID        asin                             reviewerName  \\\n",
      "2624198   ASUB02QF54XHO  B003US4L4K                                      ChR   \n",
      "2203026  A3M82GUHMPNQEA  B002PABWJQ                                      Rae   \n",
      "1289454   AC46D9HI8ZK88  B000RH173A  Sirens List                     Florida   \n",
      "3244748  A3K9ZXPTAQLQ1J  B005L3NIRS                                monkey893   \n",
      "525993    AVCKAQN2TFMTS  B0002IES80                             Peter Hurley   \n",
      "\n",
      "        helpful                                         reviewText  overall  \\\n",
      "2624198  [0, 0]  Useful little storage box for rechargeable AA ...      4.0   \n",
      "2203026  [0, 0]  I purchased this tool along with the Artisan B...      5.0   \n",
      "1289454  [0, 0]  I AM IN LOOOOVE WITH THESE BLACK SATIN SHEETS....      5.0   \n",
      "3244748  [0, 0]  The whole brew cycle for a full 10 cups under ...      1.0   \n",
      "525993   [1, 1]  I just bought this item and have used it for s...      5.0   \n",
      "\n",
      "                                          summary  unixReviewTime   reviewTime  \n",
      "2624198                                     Usful      1395360000  03 21, 2014  \n",
      "2203026                              Amazing tool      1322870400   12 3, 2011  \n",
      "1289454      Classy chic Black Satin Queen set!!!      1309392000  06 30, 2011  \n",
      "3244748  It's possible that I got a broken one...      1389484800  01 12, 2014  \n",
      "525993              AWESOME Kitchen Tool - Update      1387929600  12 25, 2013  \n"
     ]
    }
   ],
   "source": [
    "#checking that we have different productids\n",
    "print(train_hnk.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to create a smaller sized train and dev data set. Enables testing accuracy for different sizes.\n",
    "#Also binarizes the labels. Ratings of 1,2 and to 0; Ratings of 4,5 to 1.\n",
    "\n",
    "def set_df_size(size,data_train,data_dev):\n",
    "    size_train = size\n",
    "    len_max_train = data_train[data_train.overall!=3].shape[0] #max possible length of train data set taking out the 3 ratings.\n",
    "    #print(\"Number of reviews with ratings != 3 in train set\",len_max_train)\n",
    "    temp_size_train = min(len_max_train,size_train)\n",
    "\n",
    "    len_max_dev = data_dev[data_dev.overall!=3].shape[0]\n",
    "    #print(\"Number of reviews with ratings != 3 in dev set\",len_max_dev)\n",
    "    temp_size_dev = min(len_max_dev,int(0.3*temp_size_train)) #making the dev set about 0.3 times the train set.\n",
    "\n",
    "    temp_train_data = data_train[data_train.overall != 3][:temp_size_train]\n",
    "    #print('Size of train data',temp_train_data.shape)\n",
    "    #print(temp_train_data.groupby('overall').count())\n",
    "    #print(temp_train_toys[:5])\n",
    "\n",
    "    temp_dev_data = data_dev[data_dev.overall!=3][:temp_size_dev]\n",
    "    #print('Size of dev data',temp_dev_data.shape)\n",
    "    #print(temp_dev_data.groupby('overall').count())\n",
    "    #print(temp_dev_data[:2])\n",
    "    \n",
    "    #Binarize ratings\n",
    "    temp_train_y = np.zeros(temp_size_train)\n",
    "    temp_train_y[temp_train_data.overall > 3] = 1\n",
    "    temp_dev_y = np.zeros(temp_size_dev)\n",
    "    temp_dev_y[temp_dev_data.overall>3] = 1\n",
    "    #print('binarized y shape',temp_train_y.shape,temp_dev_y.shape)\n",
    "    #print(temp_dev_y[:20],data_dev.overall[:20])\n",
    "    return temp_train_data,temp_dev_data,temp_train_y,temp_dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             reviewerID        asin reviewerName helpful  \\\n",
      "2619903  A32PA6OI1IV6AR  B003UDUH5W        Kevin  [0, 0]   \n",
      "1604019  A3GD869ZCKAFQV  B0018DXA5K     Paul-101  [0, 0]   \n",
      "3204287  A2CZMVXIR370RV  B005GPVN5K      Charles  [0, 0]   \n",
      "1879236  A3ATUV2XNCL93J  B001O2SNGG      Maureen  [0, 0]   \n",
      "235028   A2H36CZZNTDQCR  B00006JSUA     N. Monti  [0, 0]   \n",
      "\n",
      "                                                reviewText  overall  \\\n",
      "2619903  It's very good overall but the only problem is...      4.0   \n",
      "1604019  I try the CervAlign Pillow for 7 nights..........      1.0   \n",
      "3204287  Very easy to use and wife loves it! Cleaned ou...      5.0   \n",
      "1879236  The first few times I used this it worked grea...      2.0   \n",
      "235028   I was so excited to receive this skillet.  It ...      4.0   \n",
      "\n",
      "                                                   summary  unixReviewTime  \\\n",
      "2619903  Great quality but the bottom holes are a bit t...      1402704000   \n",
      "1604019                                             Pillow      1314403200   \n",
      "3204287                              Happy wife-Happy life      1390262400   \n",
      "1879236                      Worked great for a short time      1361750400   \n",
      "235028           Very useful pan, but does not cook evenly      1257033600   \n",
      "\n",
      "          reviewTime  \n",
      "2619903  06 14, 2014  \n",
      "1604019  08 27, 2011  \n",
      "3204287  01 21, 2014  \n",
      "1879236  02 25, 2013  \n",
      "235028    11 1, 2009  \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(dev_hnk.head(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] #list of keys that refer to each dataframe. Adding a new dataframe would require updating this list\n",
    "dict_train_df = {} #Dict to store train input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_dev_df = {} #Dict to store dev input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_train_y = {} #Dict to store binarized train data label for each domain\n",
    "dict_dev_y = {} #Dict to store binarized dev data label for each domain\n",
    "#print(len(dict_train_df))\n",
    "\n",
    "def create_sized_data(size = 10000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "#     key = list_df[0]\n",
    "    #print('Toys reviews\\n')\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "#     #print('\\n Video games reviews\\n')\n",
    "#     key = list_df[1]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_vid,dev_vid)\n",
    "#     #print('\\n Auto reviews\\n')\n",
    "#     key = list_df[2]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_aut,dev_aut)\n",
    "    #print('\\n Home and Kitchen reviews\\n')\n",
    "    key = list_df[3]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "    \n",
    "create_sized_data()\n",
    "#print(len(dict_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] #list of keys that refer to each dataframe. Adding a new dataframe would require updating this list\n",
    "dict_train_df = {} #Dict to store train input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_dev_df = {} #Dict to store dev input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_train_y = {} #Dict to store binarized train data label for each domain\n",
    "dict_dev_y = {} #Dict to store binarized dev data label for each domain\n",
    "#print(len(dict_train_df))\n",
    "\n",
    "def create_sized_data(size = 10000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "    key = list_df[3]\n",
    "    #print('Toys reviews\\n')\n",
    "\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "#     #print('\\n Video games reviews\\n')\n",
    "#     key = list_df[1]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_vid,dev_vid)\n",
    "#     #print('\\n Auto reviews\\n')\n",
    "#     key = list_df[2]\n",
    "#     dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_aut,dev_aut)\n",
    "#     #print('\\n Home and Kitchen reviews\\n')\n",
    "    key = list_df[3]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "    \n",
    "create_sized_data()\n",
    "#print(len(dict_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/reachanamikasinha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(dict_train_df['hnk'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n",
      "x_train_tokens_list length 800\n",
      "Vocabulary size: 4,372\n",
      "Total words  45945\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing steps\n",
    "\n",
    "#Changing to nltk punkt tokenizer as the periods are not getting removed\n",
    "print(dict_train_df[key].shape[0])\n",
    "\n",
    "train_cnt = collections.Counter()\n",
    "x_train_tokens_list = []\n",
    "start = time.time()\n",
    "for i in range(X_train.shape[0]):\n",
    "\n",
    "    x_train_tokens = word_tokenize(X_train.iloc[i])\n",
    "    \n",
    "    \n",
    "\n",
    "    #2. changing to lowercase and replacing numbers(are we losing any context by \n",
    "    #replacing all numbers in the review test? Are we losing any context here)\n",
    "    x_tokens_canonical = utils.canonicalize_words(x_train_tokens)\n",
    "    \n",
    "    x_train_tokens_list.append(x_tokens_canonical)\n",
    "    \n",
    "\n",
    "    #3. Build vocabulary\n",
    "    for items in x_tokens_canonical:\n",
    "            train_cnt[items] += 1\n",
    "            \n",
    "vocab = vocabulary.Vocabulary(train_cnt, size=None)  # size=None means unlimited\n",
    "total_words = sum(train_cnt.values())\n",
    "print(\"x_train_tokens_list length\", len(x_train_tokens_list))\n",
    "print(\"Vocabulary size: {:,}\".format(vocab.size))\n",
    "#print(\"Vocabulary dict: \", vocab.word_to_id)\n",
    "print(\"Total words \",total_words )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "x_test_tokens_list length 200\n",
      "Total words  12199\n",
      "Average number of words per review:  60.995\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing steps for dev set\n",
    "\n",
    "#Changing to nltk punkt tokenizer as the periods are not getting removed\n",
    "print(X_test.shape[0])\n",
    "\n",
    "test_cnt = collections.Counter()\n",
    "x_test_tokens_list = []\n",
    "start = time.time()\n",
    "for i in range(X_test.shape[0]):\n",
    "\n",
    "    x_test_tokens = word_tokenize(X_test.iloc[i])\n",
    "    \n",
    "    \n",
    "\n",
    "    #2. changing to lowercase and replacing numbers(are we losing any context by \n",
    "    #replacing all numbers in the review test? Are we losing any context here)\n",
    "    x_tokens_canonical = utils.canonicalize_words(x_test_tokens)\n",
    "    \n",
    "    x_test_tokens_list.append(x_tokens_canonical)\n",
    "    \n",
    "\n",
    "    #3. Build vocabulary\n",
    "    for items in x_tokens_canonical:\n",
    "            test_cnt[items] += 1\n",
    "            \n",
    "\n",
    "total_words = sum(test_cnt.values())\n",
    "print(\"x_test_tokens_list length\", len(x_test_tokens_list))\n",
    "\n",
    "print(\"Total words \",total_words )\n",
    "print('Average number of words per review: ', total_words/X_test.shape[0])\n",
    "#Average number of words per review helps us figure out the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saw', 'this', 'book', 'in', 'my', 'local', 'library', '.', 'borrowed', 'it', 'and', 'simply', 'loved', 'it', '.', 'could', \"n't\", 'wait', 'to', 'get', 'my', 'own', 'copy', '.', 'just', 'love', 'all', 'the', 'beautiful', 'pictures', 'in', 'here', '.', 'finally', 'got', 'it', 'from', 'amazon', 'and', 'it', \"'s\", 'mine', '!', '!', 'no', 'need', 'to', 'borrow', 'from', 'the', 'library', 'anymore', '.']\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tokens_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting all reviews to ids \n",
    "train_id_list = []\n",
    "for item in x_train_tokens_list:\n",
    "    train_id_list.append(vocab.words_to_ids(item))\n",
    "    \n",
    "test_id_list = []\n",
    "for item in x_test_tokens_list:\n",
    "    test_id_list.append(vocab.words_to_ids(item))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['saw', 'this', 'book', 'in', 'my', 'local', 'library', '.', 'borrowed', 'it', 'and', 'simply', 'loved', 'it', '.', 'could', \"n't\", 'wait', 'to', 'get', 'my', 'own', 'copy', '.', 'just', 'love', 'all', 'the', 'beautiful', 'pictures', 'in', 'here', '.', 'finally', 'got', 'it', 'from', 'amazon', 'and', 'it', \"'s\", 'mine', '!', '!', 'no', 'need', 'to', 'borrow', 'from', 'the', 'library', 'anymore', '.']\n"
     ]
    }
   ],
   "source": [
    "print((x_train_tokens_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 12, 10, 16, 17, 18, 19, 20, 7, 21, 22, 10, 23, 24, 25, 26, 27, 28, 6, 29, 10, 30, 31, 12, 32, 33, 13, 12, 34, 35, 36, 36, 37, 38, 19, 39, 32, 26, 9, 40, 10]\n"
     ]
    }
   ],
   "source": [
    "print((train_id_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'bought', 'this', 'because', 'i', 'have', \"n't\", 'found', 'time', 'to', 'take', 'the', 'class', 'and', 'found', 'it', \"'s\", 'pretty', 'easy', 'to', 'follow', 'along', 'and', 'teach', 'yourself', 'everything', 'in', 'the', 'book', '.', 'there', 'is', 'step', 'by', 'step', 'instructions', 'for', 'different', 'designs', 'and', 'ideas', 'and', 'lots', 'of', 'hints', 'and', 'tricks', 'for', 'working', 'with', 'fondant', 'and', 'gumpaste', '.']\n"
     ]
    }
   ],
   "source": [
    "print((x_test_tokens_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 195, 4, 197, 41, 83, 17, 247, 243, 19, 257, 26, 424, 13, 247, 12, 34, 326, 234, 19, 608, 1163, 13, 1282, 1192, 344, 6, 26, 5, 10, 380, 79, 623, 426, 623, 179, 43, 659, 399, 13, 415, 13, 260, 74, 3015, 13, 637, 43, 806, 70, 371, 13, 996, 10]\n"
     ]
    }
   ],
   "source": [
    "print((test_id_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4301, 76, 4302, 305, 4149, 570, 166, 76, 54, 41, 138, 4, 145, 223, 80, 19, 235, 95, 44, 704, 168, 76, 41, 4303, 26, 147, 6, 12, 4304, 3428, 970, 10, 915, 706, 1326, 706, 50, 17, 412, 706, 983, 343, 19, 235, 12, 43, 1457, 76, 12, 34, 23, 489, 2088, 43, 245, 36, 979, 13, 1612, 2496, 498, 289, 12, 232, 87, 2659, 10]\n"
     ]
    }
   ],
   "source": [
    "print(max((train_id_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest review: 2\n",
      "Longest review: 574\n"
     ]
    }
   ],
   "source": [
    "review_lengths = [len(review) for review in train_id_list]\n",
    "print(\"Shortest review:\", min(review_lengths))\n",
    "print(\"Longest review:\",max(review_lengths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBERQAFBRESC/w8aVS4nQDGIJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b06629c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.axes()\n",
    "sns.distplot(review_lengths)\n",
    "ax.set_title(\"Distribution of the review lengths\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>57.431250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>55.240267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>574.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  800.000000\n",
       "mean    57.431250\n",
       "std     55.240267\n",
       "min      2.000000\n",
       "25%     27.000000\n",
       "50%     37.000000\n",
       "75%     68.000000\n",
       "max    574.000000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(review_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding padding to each review less than max_length and cutting off reviews greater than max length\n",
    "max_length = 200\n",
    "\n",
    "train_ids, z = utils.pad_np_array(train_id_list, max_len=200) \n",
    "test_ids, z = utils.pad_np_array(test_id_list, max_len=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 200)\n"
     ]
    }
   ],
   "source": [
    "print(train_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  4  5  6  7  8  9 10 11 12 13 14 15 12 10 16 17 18 19 20  7 21 22 10 23\n",
      " 24 25 26 27 28  6 29 10 30 31 12 32 33 13 12 34 35 36 36 37 38 19 39 32 26\n",
      "  9 40 10  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(train_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219753\n",
      "800\n",
      "[   0  776   44 1112    2 1250  777   41  776    2 3275 2181 2181    2\n",
      "    2   41 2498    2    2    2    2    2 2181 3776   44    2    2    2\n",
      "   41 3275    2   44    2    2   10    2    2 3275 2181    2    2 2181\n",
      " 1112    2 3873    2   41 1250    2   44 2498 3873    2  776   41    2\n",
      "    2    2    2    2    2 2181    2    2 3873    2   41 1250   10    2\n",
      "    2 3776 2181 2935    2 3873 2498 2028 1250    2 1112   44   41 1250\n",
      "    2 1250 2181    2    2    2 1250    2    2    2    2 2181 1112 2498\n",
      "    2 3776]\n",
      "29     Saw this book in my local library.  Borrowed i...\n",
      "535    i purchased this for a friend who enjoys cake ...\n",
      "Name: reviewText, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_ids))\n",
    "print(len(X_train))\n",
    "print(train_ids[0:100])\n",
    "print(X_train[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,)\n",
      "(800, 200)\n",
      "(200, 200)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(train_ids.shape)\n",
    "print(test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "embed_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(ids, labels, batch_size=100):\n",
    "    \n",
    "    n_batches = len(ids)//batch_size\n",
    "    ids, labels = ids[:n_batches*batch_size], labels[:n_batches*batch_size]\n",
    "    \n",
    "    for ii in range(0, len(ids), batch_size):\n",
    "        yield ids[ii:ii+batch_size], labels[ii:ii+batch_size]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_rnn(gl_embed=hands.W,\n",
    "              embed_size=embed_size,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              lstm_size=lstm_size,\n",
    "              lstm_layers=lstm_layers):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #n_words = len(vocabulary_to_int)\n",
    "    \n",
    "    with tf.name_scope('inputs'):\n",
    "        inputs_ = tf.placeholder(tf.int32,[None, None],name='inputs_')\n",
    "    with tf.name_scope('labels'):\n",
    "        labels_ = tf.placeholder(tf.int32,[None, None],name='labels_')\n",
    "    with tf.name_scope('keep_prob'):    \n",
    "        keep_prob = tf.placeholder(tf.float32,name='keep_prob')\n",
    "        \n",
    "    with tf.name_scope('embedding'):\n",
    "#         embedding = tf.Variable(tf.random_normal((n_words,embed_size),-1,1),name='embedding_')\n",
    "#         embed = tf.nn.embedding_lookup(embedding,inputs_)\n",
    "        embedding=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n",
    "                                       initializer=tf.constant_initializer(gl_embed),trainable=False)\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs_)\n",
    "        \n",
    "    with tf.name_scope(\"RNN_cells\"):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "\n",
    "        # Stack up multiple LSTM layers, for deep learning\n",
    "        cell = tf.contrib.rnn.MultiRNNCell([drop]*lstm_layers)\n",
    "        \n",
    "        with tf.name_scope(\"RNN_init_state\"):\n",
    "            # Getting an initial state of all zeros\n",
    "            initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    with tf.name_scope(\"RNN_forward\"):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)\n",
    "        \n",
    "    with tf.name_scope('predictions'):\n",
    "        predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, \n",
    "                                                        activation_fn=tf.sigmoid,\n",
    "                                                        weights_initializer=\n",
    "                                                        tf.truncated_normal_initializer(stddev=0.1))   \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "        tf.summary.scalar('cost', cost)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    # Export the nodes \n",
    "    export_nodes = ['inputs_', 'labels_','initial_state', 'final_state',\n",
    "                    'keep_prob', 'cell', 'cost', 'predictions', 'optimizer',\n",
    "                    'accuracy','merged']\n",
    "    \n",
    "    Graph = namedtuple('Graph', export_nodes)\n",
    "    \n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-abb3680139c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m               \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0mlstm_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m               lstm_layers=lstm_layers)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-f7588a875773>\u001b[0m in \u001b[0;36mbuild_rnn\u001b[0;34m(gl_embed, embed_size, batch_size, learning_rate, lstm_size, lstm_layers)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#         embed = tf.nn.embedding_lookup(embedding,inputs_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         embedding=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n\u001b[0;32m---> 23\u001b[0;31m                                        initializer=tf.constant_initializer(gl_embed),trainable=False)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    803\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariable_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    806\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     logging.vlog(1, \"Created variable %s with shape %s and init %s\", v.name,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m           \u001b[0mexpected_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m               self._initial_value = ops.convert_to_tensor(\n\u001b[0;32m--> 303\u001b[0;31m                   initial_value(), name=\"initial_value\", dtype=dtype)\n\u001b[0m\u001b[1;32m    304\u001b[0m               shape = (self._initial_value.get_shape()\n\u001b[1;32m    305\u001b[0m                        if validate_shape else tensor_shape.unknown_shape())\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    777\u001b[0m           \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         init_val = lambda: initializer(  # pylint: disable=g-long-lambda\n\u001b[0;32m--> 779\u001b[0;31m             shape.as_list(), dtype=dtype, partition_info=partition_info)\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, partition_info, verify_shape)\u001b[0m\n\u001b[1;32m    198\u001b[0m       \u001b[0mverify_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     return constant_op.constant(\n\u001b[0;32m--> 200\u001b[0;31m         self.value, dtype=dtype, shape=shape, verify_shape=verify_shape)\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph = build_rnn(gl_embed=hands.W,\n",
    "              embed_size=embed_size,\n",
    "              batch_size=batch_size,\n",
    "              learning_rate=learning_rate,\n",
    "              lstm_size=lstm_size,\n",
    "              lstm_layers=lstm_layers)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    file_writer = tf.summary.FileWriter('output/logs/1', sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, epoch,train_writer,test_writer):\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "        iteration = 1\n",
    "        for e in range(epochs):\n",
    "            state = sess.run(model.initial_state)\n",
    "\n",
    "            for ii, (x, y) in enumerate(batch_iterator(train_ids, y_train, batch_size), 1):\n",
    "                \n",
    "                feed = {model.inputs_: x,\n",
    "                        model.labels_: y[:, None],\n",
    "                        model.keep_prob: 0.5,\n",
    "                        model.initial_state: state}\n",
    "                summary,loss, state, _ = sess.run([model.merged,model.cost, \n",
    "                                                   model.final_state, \n",
    "                                                   model.optimizer], feed_dict=feed)\n",
    "\n",
    "                if iteration%5==0:\n",
    "                    print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                          \"Iteration: {}\".format(iteration),\n",
    "                          \"Train loss: {:.3f}\".format(loss))\n",
    "\n",
    "                train_writer.add_summary(summary, iteration)\n",
    "\n",
    "                if iteration%25==0:\n",
    "                    val_acc = []\n",
    "                    val_state = sess.run(model.cell.zero_state(batch_size, tf.float32))\n",
    "                    for x, y in batch_iterator(test_ids, y_test, batch_size):\n",
    "                        feed = {model.inputs_: x,\n",
    "                                model.labels_: y[:, None],\n",
    "                                model.keep_prob: 1,\n",
    "                                model.initial_state: val_state}\n",
    "                        summary, batch_acc, val_state = sess.run([model.merged,model.accuracy, \n",
    "                                                         model.final_state], feed_dict=feed)\n",
    "                        #print('batch_acc', batch_acc)\n",
    "                        val_acc.append(batch_acc)\n",
    "\n",
    "                    test_writer.add_summary(summary,iteration)\n",
    "                    print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "\n",
    "                iteration +=1\n",
    "        saver.save(sess, \"output/checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  41  195    4 ...,    0    0    0]\n",
      " [  41 2656    6 ...,    0    0    0]\n",
      " [  41  101   44 ...,    0    0    0]\n",
      " ..., \n",
      " [   7  970   15 ...,    0    0    0]\n",
      " [  41   52    4 ...,    0    0    0]\n",
      " [ 142 1744  288 ...,    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size_options = [256]\n",
    "lstm_layers_options = [1]\n",
    "learning_rate_options = [0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm size: 256 nb layers : 1 learn rate : 0.001\n",
      "Epoch: 0/1 Iteration: 5 Train loss: 0.166\n",
      "Epoch: 0/1 Iteration: 10 Train loss: 0.186\n",
      "Epoch: 0/1 Iteration: 15 Train loss: 0.233\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size=50\n",
    "for lstm_size in lstm_size_options:\n",
    "    for lstm_layers in lstm_layers_options:\n",
    "        for learning_rate in learning_rate_options:\n",
    "            log_string_train = 'output/logs/2/train/lr={},rl={},ru={}'.format(learning_rate, lstm_layers, lstm_size)\n",
    "            log_string_test = 'output/logs/2/test/lr={},rl={},ru={}'.format(learning_rate, lstm_layers, lstm_size)\n",
    "            train_writer = tf.summary.FileWriter(log_string_train)\n",
    "            test_writer = tf.summary.FileWriter(log_string_test)\n",
    "            \n",
    "            print(\"lstm size: {}\".format(lstm_size),\n",
    "                    \"nb layers : {}\".format(lstm_layers),\n",
    "                    \"learn rate : {:.3f}\".format(learning_rate))\n",
    "            \n",
    "            model = build_rnn(gl_embed=hands.W,\n",
    "                      embed_size=embed_size,\n",
    "                      batch_size=batch_size,\n",
    "                      learning_rate=learning_rate,\n",
    "                      lstm_size=lstm_size,\n",
    "                      lstm_layers=lstm_layers)\n",
    "\n",
    "            train(model, epochs, train_writer,test_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm size: 256 nb layers : 1 learn rate : 0.001\n",
      "Epoch: 0/5 Iteration: 5 Train loss: 0.164\n",
      "Epoch: 0/5 Iteration: 10 Train loss: 0.178\n",
      "Epoch: 0/5 Iteration: 15 Train loss: 0.237\n",
      "Epoch: 1/5 Iteration: 20 Train loss: 0.208\n",
      "Epoch: 1/5 Iteration: 25 Train loss: 0.127\n",
      "Val acc: 0.790\n",
      "Epoch: 1/5 Iteration: 30 Train loss: 0.238\n",
      "Epoch: 2/5 Iteration: 35 Train loss: 0.175\n",
      "Epoch: 2/5 Iteration: 40 Train loss: 0.170\n",
      "Epoch: 2/5 Iteration: 45 Train loss: 0.202\n",
      "Epoch: 3/5 Iteration: 50 Train loss: 0.157\n",
      "Val acc: 0.790\n",
      "Epoch: 3/5 Iteration: 55 Train loss: 0.166\n",
      "Epoch: 3/5 Iteration: 60 Train loss: 0.158\n",
      "Epoch: 4/5 Iteration: 65 Train loss: 0.116\n",
      "Epoch: 4/5 Iteration: 70 Train loss: 0.147\n",
      "Epoch: 4/5 Iteration: 75 Train loss: 0.122\n",
      "Val acc: 0.790\n",
      "Epoch: 4/5 Iteration: 80 Train loss: 0.124\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size=50\n",
    "for lstm_size in lstm_size_options:\n",
    "    for lstm_layers in lstm_layers_options:\n",
    "        for learning_rate in learning_rate_options:\n",
    "            log_string_train = 'output/logs/2/train/lr={},rl={},ru={}'.format(learning_rate, lstm_layers, lstm_size)\n",
    "            log_string_test = 'output/logs/2/test/lr={},rl={},ru={}'.format(learning_rate, lstm_layers, lstm_size)\n",
    "            train_writer = tf.summary.FileWriter(log_string_train)\n",
    "            test_writer = tf.summary.FileWriter(log_string_test)\n",
    "            \n",
    "            print(\"lstm size: {}\".format(lstm_size),\n",
    "                    \"nb layers : {}\".format(lstm_layers),\n",
    "                    \"learn rate : {:.3f}\".format(learning_rate))\n",
    "            \n",
    "            model = build_rnn(gl_embed=hands.W,\n",
    "                      embed_size=embed_size,\n",
    "                      batch_size=batch_size,\n",
    "                      learning_rate=learning_rate,\n",
    "                      lstm_size=lstm_size,\n",
    "                      lstm_layers=lstm_layers)\n",
    "\n",
    "            train(model, epochs, train_writer,test_writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
