{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the save now works\n"
     ]
    }
   ],
   "source": [
    "# !sudo pip install -U nltk\n",
    "# !sudo pip install wget\n",
    "# !sudo pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n",
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "# Install a few python packages using pip\n",
    "#from common import utils\n",
    "from common import utils\n",
    "utils.require_package('nltk')\n",
    "utils.require_package(\"wget\")      # for fetching dataset\n",
    "#from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys, time\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "#comment or uncomment based on anamika/ arunima\n",
    "# Helper libraries\n",
    "# from common import utils, vocabulary, glove_helper\n",
    "\n",
    "# from common import utils, vocabulary\n",
    "from common import utils, vocabulary\n",
    "from common import glove_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read the amazon review data files\n",
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "#df = getDF('reviews_Toys_and_Games.json.gz') #old def function corresponding to the step bt step vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Using pretrained GLove embeddings\n",
    "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available\n",
    "hands.shape\n",
    "print(hands.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please note that i had to comment out the path. Please uncomment before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 126.62729096412659\n",
      "end getDF\n",
      "time taken to load data =  126.62751865386963\n"
     ]
    }
   ],
   "source": [
    "df_toys = getDF('/newvolume/reviews_Toys_and_Games.json.gz')\n",
    "#df_toys = getDF('reviews_Toys_and_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 84.79005575180054\n",
      "end getDF\n",
      "time taken to load data =  84.7906084060669\n"
     ]
    }
   ],
   "source": [
    "df_vid = getDF('/newvolume/reviews_Video_Games.json.gz')\n",
    "#df_vid = getDF('reviews_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 76.0858519077301\n",
      "end getDF\n",
      "time taken to load data =  76.08641147613525\n"
     ]
    }
   ],
   "source": [
    "df_aut = getDF('/newvolume/reviews_Automotive.json.gz')\n",
    "#df_aut = getDF('reviews_Automotive.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 243.88985800743103\n",
      "end getDF\n",
      "time taken to load data =  243.89057850837708\n"
     ]
    }
   ],
   "source": [
    "#df_hnk = getDF('reviews_Home_and_Kitchen.json.gz')\n",
    "df_hnk = getDF('/newvolume/reviews_Home_and_Kitchen.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Home and Kitchen reviews examples\n",
      "\n",
      "A210NOCSTBT4OD\n",
      "Have you ever thought about how you met your best friend? Was it normal, or was it wacky - like how Elias met Shohei? Pulling a boa constrictor snake named Mathilda out of your backpack can make a remarkable first impression! This book is about three best friends Elias, Honoria, and Shohei, who are united against \"That Which Is The Peshtigo School\". Their goal is to make it through the annual school science fair, but things don't always go as planned.Elias is part of a family made up of science fanatics who would do anything to win a science fair. Elias isn't exactly what you'd call the ambitious type, especially when it comes to science fairs. So he becomes like Galileo and \"retests\" one of his sibling's past projects. Honoria loves to be ambitious, especially when it comes to being a legal counsel extraordinaire. But when she faces a bigger challenge than beating Goliath Reed or getting a piranha to become vegetarian, she doesn't know if she can make it. Shohei is an all around slacker who tries to mooch off Elias instead of creating something on his own. His adoptive parents are constantly encouraging him to start \"hearing\" his ancestors. His mom has even turned Shohei's room into what looks like a walk-in Japanese museum exhibit!This book is laugh out loud hilarious and the more you read, the more exciting and unexpected it gets. I love the title on this book because it really made me laugh and want to read the book. I also like how people so different from one another can be such close friends. There is not much excitement in the beginning, but it builds up very quickly. So if you like that type of story, then this is the book for you.\n",
      "A28ILV4TOG8BH2\n",
      "The butter dish is serving us well, and keeping the butter fresh and healthy. Couldn't be happier with it, and the color is a pleasing green.\n"
     ]
    }
   ],
   "source": [
    "#Looking at a few examples of review text\n",
    "# print('Toys reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_toys['reviewerID'].iloc[i])\n",
    "#     print(df_toys['reviewText'].iloc[i])\n",
    "\n",
    "# print('\\n Video games reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_vid['reviewerID'].iloc[i])\n",
    "#     print(df_vid['reviewText'].iloc[i])\n",
    "    \n",
    "# print('\\n Automobile reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_aut['reviewerID'].iloc[i])\n",
    "#     print(df_aut['reviewText'].iloc[i])\n",
    "    \n",
    "print('\\n Home and Kitchen reviews examples\\n')\n",
    "for i in range(2):\n",
    "    print(df_hnk['reviewerID'].iloc[i])\n",
    "    print(df_hnk['reviewText'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy reviews train, dev and test set dataframe shape: (1351662, 9) (450554, 9) (450555, 9)\n",
      "Video games reviews train, dev and test set dataframe shape: (794851, 9) (264951, 9) (264951, 9)\n",
      "Auto reviews train, dev and test set dataframe shape: (824260, 9) (274754, 9) (274754, 9)\n",
      "Home and Kitchen reviews train, dev and test set dataframe shape: (2552355, 9) (850785, 9) (850786, 9)\n"
     ]
    }
   ],
   "source": [
    "# Create train,dev,test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_toys,devtest = train_test_split(df_toys, test_size=0.4, random_state=42)\n",
    "dev_toys,test_toys = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Toy reviews train, dev and test set dataframe shape:',train_toys.shape,dev_toys.shape,test_toys.shape)\n",
    "\n",
    "#For Video games reviews\n",
    "train_vid,devtest = train_test_split(df_vid, test_size=0.4, random_state=42)\n",
    "dev_vid,test_vid = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Video games reviews train, dev and test set dataframe shape:',train_vid.shape,dev_vid.shape,test_vid.shape)\n",
    "\n",
    "#For Auto reviews\n",
    "train_aut,devtest = train_test_split(df_aut, test_size=0.4, random_state=42)\n",
    "dev_aut,test_aut = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Auto reviews train, dev and test set dataframe shape:',train_aut.shape,dev_aut.shape,test_aut.shape)\n",
    "\n",
    "#For Home and Kitchen reviews\n",
    "train_hnk,devtest = train_test_split(df_hnk, test_size=0.4, random_state=42)\n",
    "dev_hnk,test_hnk = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Home and Kitchen reviews train, dev and test set dataframe shape:',train_hnk.shape,dev_hnk.shape,test_hnk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create a smaller sized train and dev data set. Enables testing accuracy for different sizes.\n",
    "#Also binarizes the labels. Ratings of 1,2 and to 0; Ratings of 4,5 to 1.\n",
    "\n",
    "def set_df_size(size,data_train,data_dev):\n",
    "    size_train = size\n",
    "    len_max_train = data_train[data_train.overall!=3].shape[0] #max possible length of train data set taking out the 3 ratings.\n",
    "    #print(\"Number of reviews with ratings != 3 in train set\",len_max_train)\n",
    "    temp_size_train = min(len_max_train,size_train)\n",
    "\n",
    "    len_max_dev = data_dev[data_dev.overall!=3].shape[0]\n",
    "    #print(\"Number of reviews with ratings != 3 in dev set\",len_max_dev)\n",
    "    temp_size_dev = min(len_max_dev,int(0.3*temp_size_train)) #making the dev set about 0.3 times the train set.\n",
    "\n",
    "    temp_train_data = data_train[data_train.overall != 3][:temp_size_train]\n",
    "    #print('Size of train data',temp_train_data.shape)\n",
    "    #print(temp_train_data.groupby('overall').count())\n",
    "    #print(temp_train_toys[:5])\n",
    "\n",
    "    temp_dev_data = data_dev[data_dev.overall!=3][:temp_size_dev]\n",
    "    #print('Size of dev data',temp_dev_data.shape)\n",
    "    #print(temp_dev_data.groupby('overall').count())\n",
    "    #print(temp_dev_data[:2])\n",
    "    \n",
    "    #Binarize ratings\n",
    "    temp_train_y = np.zeros(temp_size_train)\n",
    "    temp_train_y[temp_train_data.overall > 3] = 1\n",
    "    temp_dev_y = np.zeros(temp_size_dev)\n",
    "    temp_dev_y[temp_dev_data.overall>3] = 1\n",
    "    #print('binarized y shape',temp_train_y.shape,temp_dev_y.shape)\n",
    "    #print(temp_dev_y[:20],data_dev.overall[:20])\n",
    "    return temp_train_data,temp_dev_data,temp_train_y,temp_dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] #list of keys that refer to each dataframe. Adding a new dataframe would require updating this list\n",
    "dict_train_df = {} #Dict to store train input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_dev_df = {} #Dict to store dev input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_train_y = {} #Dict to store binarized train data label for each domain\n",
    "dict_dev_y = {} #Dict to store binarized dev data label for each domain\n",
    "#print(len(dict_train_df))\n",
    "\n",
    "size_initial = 500000\n",
    "def create_sized_data(size = 10000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "    key = list_df[0]\n",
    "    #print('Toys reviews\\n')\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "    #print('\\n Video games reviews\\n')\n",
    "    key = list_df[1]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_vid,dev_vid)\n",
    "    #print('\\n Auto reviews\\n')\n",
    "    key = list_df[2]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_aut,dev_aut)\n",
    "    #print('\\n Home and Kitchen reviews\\n')\n",
    "    key = list_df[3]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "    \n",
    "create_sized_data(size_initial)\n",
    "#create_sized_data(500)\n",
    "#print(len(dict_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_processor = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=0)\n",
    "#Note : Above function was used instead of the below, which is deprecated. \n",
    "# vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_length)\n",
    "\n",
    "def process_inputs(key, vocab_processor):\n",
    "    \n",
    "    start_vectorize = time.time()\n",
    "    x_train = dict_train_df[key].reviewText\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_df[key].reviewText\n",
    "    y_dev = dict_dev_y[key]\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    # Train the vocab_processor from the training set\n",
    "    x_train = vocab_processor.fit_transform(x_train)\n",
    "    # Transform our test set with the vocabulary processor\n",
    "    x_dev = vocab_processor.transform(x_dev)\n",
    "\n",
    "    # We need these to be np.arrays instead of generators\n",
    "    x_train = np.array(list(x_train))\n",
    "    print(x_train.shape)\n",
    "    x_dev = np.array(list(x_dev))\n",
    "    y_train = np.array(y_train).astype(int)\n",
    "    y_dev = np.array(y_dev).astype(int)\n",
    "    \n",
    "#     y_train = tf.expand_dims(y_train,1)\n",
    "#     y_dev = tf.expand_dims(y_dev,1)\n",
    "    print('y train shape',y_train.shape)\n",
    "\n",
    "    V = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % V)\n",
    "    end_vectorize = time.time()\n",
    "    print('Time taken to vectorize %d size dataframe'%x_train.shape[0],end_vectorize-start_vectorize)\n",
    "\n",
    "    # Return the transformed data and the number of words\n",
    "    return x_train, y_train, x_dev, y_dev, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys\n",
      "(500000,)\n",
      "(500000, 150)\n",
      "y train shape (500000,)\n",
      "Total words: 108144\n",
      "Time taken to vectorize 500000 size dataframe 79.72650527954102\n",
      "Number words in training corpus for toys 108144\n",
      "toys dataset id shapes (500000, 150) (150000, 150)\n",
      "sample review for domain toys It's just the model I was hoping for and more. It's challenging and has given me something more to learn in developing my model building skills. Revell as always makes good models to build! \n",
      "\n",
      "corresponding ids\n",
      " [ 114   41    1  378    6   15 1122    8    2   58  114  796    2   42  632\n",
      "   96  229   58    3  399   12 4088   14  378  541  768 6220   22  272  223\n",
      "   66  935    3  393    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0] \n",
      "\n",
      "vid\n",
      "(500000,)\n",
      "(500000, 150)\n",
      "y train shape (500000,)\n",
      "Total words: 162546\n",
      "Time taken to vectorize 500000 size dataframe 126.2020902633667\n",
      "Number words in training corpus for vid 162546\n",
      "vid dataset id shapes (500000, 150) (150000, 150)\n",
      "sample review for domain vid First things first. I love the Lego games. So, my view on this game is skewed. This game follows the typical Lego quality game with a slight change in game play. When playing with two players, the screen splits, which is great, because you don't have to drop out all the time. However, watch changing players or using Jack's compass when the screen is split because it will sometimes freeze up on you. If you like Legos, the other Lego games, or Pirates of the Carribean, then this game is for you. \n",
      "\n",
      "corresponding ids\n",
      " [  534   171    78     5    92     1  1669    29   182    24   976    19\n",
      "    11     9     6 15187    37     9  2416     1  1751  1669   285     9\n",
      "    15     4  2121   394    13     9    34   286    75    15   157   274\n",
      "     1   253 14173    69     6    42    77    10    81    16     3  1126\n",
      "    44    30     1    53   312   656  1849   274    32   244 19994 11197\n",
      "    57     1   253     6  2301    77     8    43   446  2316    47    19\n",
      "    10    84    10    26  9900     1    70  1669    29    32  5409     7\n",
      "     1 26731    96    11     9     6    12    10     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0] \n",
      "\n",
      "aut\n",
      "(500000,)\n",
      "(500000, 150)\n",
      "y train shape (500000,)\n",
      "Total words: 95702\n",
      "Time taken to vectorize 500000 size dataframe 73.10126399993896\n",
      "Number words in training corpus for aut 95702\n",
      "aut dataset id shapes (500000, 150) (150000, 150)\n",
      "sample review for domain aut THE OLD LIGHTS WERE FOGGED OVER WITH THE NEW ONES MAKES DRIVING AT NIGHT A LOT SAFER A LOT BRIGHTER AND THEY WERE EASY TO INSTALL. \n",
      "\n",
      "corresponding ids\n",
      " [  399  5252  5846  4792 59723  5118  1613   399  2958 10655  8982 11931\n",
      "  2236 13544   175  3136 38635   175  3136 21198   580  1920  4792  2799\n",
      "   699  4138     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0] \n",
      "\n",
      "hnk\n",
      "(500000,)\n",
      "(500000, 150)\n",
      "y train shape (500000,)\n",
      "Total words: 96380\n",
      "Time taken to vectorize 500000 size dataframe 85.39440655708313\n",
      "Number words in training corpus for hnk 96380\n",
      "hnk dataset id shapes (500000, 150) (150000, 150)\n",
      "sample review for domain hnk These barstools are amazing. After scouring stores and the internet, I decided that these would be my best bet. I needed sturdy stools to deal with heavy use from kids, family, ect. They are 1000% solid. When they arrive you have to screw the seat onto the base swivel mechanism, however all the important parts that make it solid are already in place. You also have to attach the solid wooden seat onto the frame. The seat isn't pre-drilled so that is a bit annoying. Not hard, but annoying. These stools are going to last a very long time, without a doubt. I wouldn't hesitate to buy more. They feel stronger, more solid, and of higher quality than stools I saw for $300 each. I was a little nervous they would look clunky, but they are beautiful with very nice lines. I wish I would have ordered them two months ago when I first saw them.I forgot to mention the box was delivered to my door within two days. Fastest shipping ever. \n",
      "\n",
      "corresponding ids\n",
      " [  177 11181    23   604   275  8919   737     3     1  2455     2   392\n",
      "    12    53    40    28    13   182  3929     2   243   224  2544     4\n",
      "   443    14   247    29    46   531   441  7701    99    23  3485   534\n",
      "   260    36  1991    20    15     4   886     1  1514   930     1   551\n",
      "  3056  1513   516    42     1  1093   500    12    85     6   534    23\n",
      "   458    11   277   202    95    15     4  1632     1   534  1316  1514\n",
      "   930     1   694    18  1514   463  6192    24    12     7     5   180\n",
      "  1355   293   237    19  1355   177  2544    23   231     4   190     5\n",
      "    27   154    54   170     5  1872     2   541  2330     4   104    49\n",
      "    99   272  2264    49   534     3     8   969    93    50  2544     2\n",
      "   598     9  2246   332     2    17     5    69  3150    36    40   159\n",
      "  5719    19    36    23   354    14    27    86  1888     2   370     2\n",
      "    40    15   225    41   108   221] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting reviews to ids for all domains and add padding.\n",
    "\n",
    "# Hyperparameters\n",
    "min_frequency = 1\n",
    "max_length = 150\n",
    "\n",
    "dict_vectorizers = {} #Dict to store the vocab_processor fit on each domain\n",
    "dict_train_ids = {} #Dict to store train data reviews as sparse matrix of word ids\n",
    "dict_dev_ids = {} #Dict to store dev data reviews as sparse matrix of word ids\n",
    "dict_cnn = {} #Dict to store cnn model developed on each domain. Assumes input features are developed using the corresponding count_vectorizer\n",
    "dict_dev_ypred = {} #Dict to store dev predictions\n",
    "dict_vocab_len = {} #Store vocab length of each domain\n",
    "for key in list_df:\n",
    "    \n",
    "    #Converting ratings to tokenized word id counts as a sparse matrix using count_vectorizer\n",
    "    dict_vectorizers[key] = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=min_frequency)\n",
    "    print(key)\n",
    "    dict_train_ids[key], dict_train_y[key],dict_dev_ids[key], dict_dev_ypred[key], dict_vocab_len[key] = process_inputs(key,dict_vectorizers[key])\n",
    "    \n",
    "    print(\"Number words in training corpus for\",key,(dict_vocab_len[key]))\n",
    "    print(key,'dataset id shapes',dict_train_ids[key].shape, dict_dev_ids[key].shape)\n",
    "\n",
    "    #Print a few examples for viewing\n",
    "    print('sample review for domain',key, dict_train_df[key].reviewText.iloc[3],'\\n')\n",
    "    print('corresponding ids\\n',dict_train_ids[key][3],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys (100000,)\n",
      "(array([58987, 24503,  8678,  3490,  1674,   922,   556,   370,   242,\n",
      "         159,    95,    68,    57,    43,    38,    31,    20,    10,\n",
      "           8,    49]), array([    0.,    50.,   100.,   150.,   200.,   250.,   300.,   350.,\n",
      "         400.,   450.,   500.,   550.,   600.,   650.,   700.,   750.,\n",
      "         800.,   850.,   900.,   950.,  1000.]))\n",
      "Number less than 100 83712\n",
      "Number less than 150 92223\n",
      "Number less than 175 94288\n",
      "Number less than 200 95652\n",
      "vid (100000,)\n",
      "(array([47996, 21688, 10292,  5920,  3620,  2440,  1692,  1282,   994,\n",
      "         680,   601,   459,   377,   307,   253,   194,   183,   143,\n",
      "         127,   752]), array([    0.,    50.,   100.,   150.,   200.,   250.,   300.,   350.,\n",
      "         400.,   450.,   500.,   550.,   600.,   650.,   700.,   750.,\n",
      "         800.,   850.,   900.,   950.,  1000.]))\n",
      "Number less than 100 69913\n",
      "Number less than 150 80111\n",
      "Number less than 175 83353\n",
      "Number less than 200 85960\n",
      "aut (100000,)\n",
      "(array([63741, 22809,  7245,  2918,  1404,   677,   456,   216,   166,\n",
      "         103,    87,    43,    27,    27,    20,    12,    10,     8,\n",
      "           5,    26]), array([   0.  ,   49.85,   99.7 ,  149.55,  199.4 ,  249.25,  299.1 ,\n",
      "        348.95,  398.8 ,  448.65,  498.5 ,  548.35,  598.2 ,  648.05,\n",
      "        697.9 ,  747.75,  797.6 ,  847.45,  897.3 ,  947.15,  997.  ]))\n",
      "Number less than 100 86739\n",
      "Number less than 150 93835\n",
      "Number less than 175 95556\n",
      "Number less than 200 96718\n",
      "hnk (100000,)\n",
      "(array([56142, 25500,  9051,  4115,  2030,  1087,   604,   433,   282,\n",
      "         185,   158,   116,    63,    47,    46,    22,    22,     9,\n",
      "          16,    72]), array([   0.  ,   49.95,   99.9 ,  149.85,  199.8 ,  249.75,  299.7 ,\n",
      "        349.65,  399.6 ,  449.55,  499.5 ,  549.45,  599.4 ,  649.35,\n",
      "        699.3 ,  749.25,  799.2 ,  849.15,  899.1 ,  949.05,  999.  ]))\n",
      "Number less than 100 81876\n",
      "Number less than 150 90765\n",
      "Number less than 175 93165\n",
      "Number less than 200 94837\n"
     ]
    }
   ],
   "source": [
    "#This code was used to pick max_length for all domains for the CNN, by using a sample of 100000, and a max_length of 10000 for analysis\n",
    "#it is not needed for running the CNN.\n",
    "# for key in list_df:\n",
    "#     length = np.count_nonzero(dict_train_ids[key],axis = 1)\n",
    "#     print(key,length.shape)\n",
    "#     print(np.histogram(length,bins = 20))\n",
    "#     print(\"Number less than 100\",np.count_nonzero(length[length <= 100]))\n",
    "#     print(\"Number less than 150\",np.count_nonzero(length[length <= 150]))\n",
    "#     print(\"Number less than 175\",np.count_nonzero(length[length <= 175]))\n",
    "#     print(\"Number less than 200\",np.count_nonzero(length[length <= 200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# print(y_dev.shape)\n",
    "# print(np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__( self, sequence_length, num_classes, vocab_size, learning_rate, momentum, embedding_size, \n",
    "                 gl_embed, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            #self.W = tf.get_variable(\"W_in\",[vocab_size, embedding_size],initializer =tf.random_uniform_initializer(0,1)) #from wildML\n",
    "            self.W=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n",
    "                                       initializer=tf.constant_initializer(gl_embed),trainable=True)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            #print('embedded_chars',self.embedded_chars.get_shape())\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            #print('embedded_chars_expanded',self.embedded_chars_expanded.get_shape())\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                #W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                Wname = \"w_%d\"%filter_size\n",
    "                W = tf.get_variable(Wname, shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.Variable(tf.constant(0.0, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d( self.embedded_chars_expanded, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "\n",
    "                # Apply nonlinearity\n",
    "                conv+= b\n",
    "                h = tf.nn.relu(conv, name=\"relu\")\n",
    "                #print('h',h.get_shape())\n",
    "\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1], strides=[1, 1, 1, 1],\n",
    "                    padding='VALID', name=\"pool\")\n",
    "                #print('pooled',pooled.get_shape())\n",
    "                pooled_outputs.append(pooled)\n",
    "                #print('pooled_outputs',type(pooled_outputs))\n",
    "                #print('pooled_outputs as array',type(np.array(pooled_outputs)),np.array(pooled_outputs).shape)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        #print('h_pool',self.h_pool.get_shape())\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        #print('h_pool_flat',self.h_pool_flat.get_shape())\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\"W\", shape=[num_filters_total, num_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            #print('self.scores',self.scores.get_shape())\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            self.pred_proba = tf.nn.softmax(self.scores, name=\"pred_proba\")\n",
    "            #print('self.predictions',self.predictions.get_shape())\n",
    "            \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "            #self.loss = tf.losses.mean_squared_error(self.input_y, self.scores)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "#             correct_pred = tf.equal(tf.cast(tf.round(self.scores), tf.int32), self.input_y)\n",
    "#             self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "        # AUC\n",
    "#         with tf.name_scope(\"auc\"):\n",
    "#             false_pos_rate, true_pos_rate, _ = roc_curve(self.input_y, self.pred_proba[:,1])\n",
    "#             self.auc = auc(false_pos_rate, true_pos_rate)\n",
    "            \n",
    "            \n",
    "        with tf.name_scope('train'):\n",
    "            #self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            self.optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate,momentum=momentum,use_nesterov=True).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(ids, labels, batch_size=100, Trainable=False):\n",
    "            #ids is input, X_train\n",
    "            #need to fix this to shuffle between epochs\n",
    "            \n",
    "            n_batches = len(ids)//batch_size\n",
    "            ids, labels = ids[:n_batches*batch_size], labels[:n_batches*batch_size]\n",
    "            if Trainable:\n",
    "                shuffle = np.random.permutation(np.arange(n_batches*batch_size))\n",
    "                ids, labels = ids[shuffle], labels[shuffle]\n",
    "   \n",
    "            for ii in range(0, len(ids), batch_size):\n",
    "                yield ids[ii:ii+batch_size], labels[ii:ii+batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "#embed_dim = 50 #use when not using pre-trained embeddings\n",
    "embed_dim = hands.shape[1]\n",
    "filter_sizes= [3,4,5]\n",
    "num_filters = 256\n",
    "l2_reg_lambda = 0\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "keep_prob = 0.8\n",
    "evaluate_train = 2 # of epochs at which to print test accuracy\n",
    "evaluate_dev = 2 # of epochs at which to estimate and print dev accuracy\n",
    "time_print = 4 # of epochs at which to print time taken\n",
    "num_classes = 2\n",
    "num_epochs = 15\n",
    "#num_checkpoints = 2\n",
    "#batch_size = 64\n",
    "batch_size=128\n",
    "\n",
    "# out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnn\"))\n",
    "# print(\"Model saving  to {}\\n\".format(out_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual training loop:\n",
    "\n",
    "def train_cnn(key, size=5000):\n",
    "     \n",
    "    x_train = dict_train_ids[key]\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_ids[key]\n",
    "    y_dev = dict_dev_ypred[key]\n",
    "    V = dict_vocab_len[key]\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "        \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('completed cnn creation')\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            size_folder =  \"size_\" + str(size) \n",
    "            out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "            #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key))\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            model_name = key \n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, model_name  + \"_model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            # Write vocabulary\n",
    "            ## vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                \n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=True), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    #avg_auc = total_auc/(batch_size*(len(x_train)//batch_size))\n",
    "                   # print(\"Train epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    \n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                    \n",
    "                    print('y_dev.shape',y_dev.shape)\n",
    "                    print('y_shuffled.shape',y_shuffled.shape)\n",
    "                    \n",
    "                    if np.array_equal(y_shuffled,y_dev):\n",
    "                        print(\"Yes\")\n",
    "                    right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                    #Calculate Accuracy\n",
    "                    new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "                     \n",
    "                    \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                    print(\"\\t\\tDev epoch {}, auc {:g}, new accuracy {:g}, right accuracy {:g},\".format(e,  roc_auc, new_acc, right_acc))\n",
    "                    #print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n",
    "                    \n",
    "        # Save model weights for future use.\n",
    "        \n",
    "        \n",
    "            #save_path = saver.save(sess, checkpoint_prefix, global_step=20,write_meta_graph=False)\n",
    "            save_path = saver.save(sess, checkpoint_prefix)\n",
    "            print(\"Saved model\", model_name, save_path)\n",
    "            \n",
    "            #calculate predictions and prediction probability    \n",
    "#             feed_dict={cnn.input_x:x_dev, cnn.input_y: y_dev, cnn.dropout_keep_prob: 1.0}\n",
    "#             y_pred, y_pred_proba = sess.run([cnn.predictions, cnn.pred_proba],feed_dict)\n",
    "            #print(y_pred, y_pred_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please make sure you change the size below so that the source model is saved with that name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys 100000\n",
      "completed cnn creation\n",
      "# batches = 781\n",
      "Train epoch 0, average loss 0.326412, average accuracy 0.870329,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 0, average loss 0.247981, average accuracy 0.905849,\n",
      "\t\tDev epoch 0, auc 0.917311, new accuracy 0.905849, right accuracy 0.905849,\n",
      "\t\t\t\t    Time taken for 0 epochs =  56.59470534324646\n",
      "Train epoch 2, average loss 0.19472, average accuracy 0.923085,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 2, average loss 0.190053, average accuracy 0.924713,\n",
      "\t\tDev epoch 2, auc 0.948155, new accuracy 0.924713, right accuracy 0.924713,\n",
      "Train epoch 4, average loss 0.147714, average accuracy 0.942772,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 4, average loss 0.187572, average accuracy 0.926382,\n",
      "\t\tDev epoch 4, auc 0.954211, new accuracy 0.926382, right accuracy 0.926382,\n",
      "\t\t\t\t    Time taken for 4 epochs =  189.73777174949646\n",
      "Train epoch 6, average loss 0.110283, average accuracy 0.958717,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 6, average loss 0.174863, average accuracy 0.932559,\n",
      "\t\tDev epoch 6, auc 0.956852, new accuracy 0.932559, right accuracy 0.932559,\n",
      "Train epoch 8, average loss 0.0799499, average accuracy 0.971271,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 8, average loss 0.177602, average accuracy 0.933093,\n",
      "\t\tDev epoch 8, auc 0.958329, new accuracy 0.933093, right accuracy 0.933093,\n",
      "\t\t\t\t    Time taken for 8 epochs =  322.9811291694641\n",
      "Train epoch 10, average loss 0.0567333, average accuracy 0.980834,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 10, average loss 0.184794, average accuracy 0.932726,\n",
      "\t\tDev epoch 10, auc 0.958333, new accuracy 0.932726, right accuracy 0.932726,\n",
      "Train epoch 12, average loss 0.0408483, average accuracy 0.987216,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 12, average loss 0.194073, average accuracy 0.934896,\n",
      "\t\tDev epoch 12, auc 0.958512, new accuracy 0.934896, right accuracy 0.934896,\n",
      "\t\t\t\t    Time taken for 12 epochs =  456.40864515304565\n",
      "Train epoch 14, average loss 0.0290297, average accuracy 0.991907,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 14, average loss 0.201882, average accuracy 0.931591,\n",
      "\t\tDev epoch 14, auc 0.958837, new accuracy 0.931591, right accuracy 0.931591,\n",
      "Saved model toys /newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints/toys_model\n",
      "vid 100000\n",
      "completed cnn creation\n",
      "# batches = 781\n",
      "Train epoch 0, average loss 0.398813, average accuracy 0.831576,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 0, average loss 0.322097, average accuracy 0.862614,\n",
      "\t\tDev epoch 0, auc 0.881232, new accuracy 0.862614, right accuracy 0.862614,\n",
      "\t\t\t\t    Time taken for 0 epochs =  34.95008635520935\n",
      "Train epoch 2, average loss 0.273329, average accuracy 0.885954,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 2, average loss 0.260473, average accuracy 0.894364,\n",
      "\t\tDev epoch 2, auc 0.92111, new accuracy 0.894364, right accuracy 0.894364,\n",
      "Train epoch 4, average loss 0.215066, average accuracy 0.912452,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 4, average loss 0.246178, average accuracy 0.900908,\n",
      "\t\tDev epoch 4, auc 0.930388, new accuracy 0.900908, right accuracy 0.900908,\n",
      "\t\t\t\t    Time taken for 4 epochs =  169.25235724449158\n",
      "Train epoch 6, average loss 0.166099, average accuracy 0.934799,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 6, average loss 0.245642, average accuracy 0.900975,\n",
      "\t\tDev epoch 6, auc 0.933907, new accuracy 0.900975, right accuracy 0.900975,\n",
      "Train epoch 8, average loss 0.12692, average accuracy 0.952105,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 8, average loss 0.244038, average accuracy 0.90595,\n",
      "\t\tDev epoch 8, auc 0.934683, new accuracy 0.90595, right accuracy 0.90595,\n",
      "\t\t\t\t    Time taken for 8 epochs =  303.5451157093048\n",
      "Train epoch 10, average loss 0.0917923, average accuracy 0.966479,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 10, average loss 0.25495, average accuracy 0.906884,\n",
      "\t\tDev epoch 10, auc 0.934148, new accuracy 0.906884, right accuracy 0.906884,\n",
      "Train epoch 12, average loss 0.0682957, average accuracy 0.976573,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 12, average loss 0.274015, average accuracy 0.907318,\n",
      "\t\tDev epoch 12, auc 0.933903, new accuracy 0.907318, right accuracy 0.907318,\n",
      "\t\t\t\t    Time taken for 12 epochs =  437.64132618904114\n",
      "Train epoch 14, average loss 0.0502499, average accuracy 0.983635,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 14, average loss 0.283107, average accuracy 0.90625,\n",
      "\t\tDev epoch 14, auc 0.933281, new accuracy 0.90625, right accuracy 0.90625,\n",
      "Saved model vid /newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints/vid_model\n",
      "aut 100000\n",
      "completed cnn creation\n",
      "# batches = 781\n",
      "Train epoch 0, average loss 0.341688, average accuracy 0.864477,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 0, average loss 0.274249, average accuracy 0.886952,\n",
      "\t\tDev epoch 0, auc 0.884795, new accuracy 0.886952, right accuracy 0.886952,\n",
      "\t\t\t\t    Time taken for 0 epochs =  34.67738389968872\n",
      "Train epoch 2, average loss 0.224533, average accuracy 0.909701,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 2, average loss 0.221307, average accuracy 0.909555,\n",
      "\t\tDev epoch 2, auc 0.929003, new accuracy 0.909555, right accuracy 0.909555,\n",
      "Train epoch 4, average loss 0.17435, average accuracy 0.930828,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 4, average loss 0.208902, average accuracy 0.9169,\n",
      "\t\tDev epoch 4, auc 0.937615, new accuracy 0.9169, right accuracy 0.9169,\n",
      "\t\t\t\t    Time taken for 4 epochs =  167.8264515399933\n",
      "Train epoch 6, average loss 0.132002, average accuracy 0.948954,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 6, average loss 0.198363, average accuracy 0.92261,\n",
      "\t\tDev epoch 6, auc 0.941728, new accuracy 0.92261, right accuracy 0.92261,\n",
      "Train epoch 8, average loss 0.0977073, average accuracy 0.963868,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 8, average loss 0.202741, average accuracy 0.921575,\n",
      "\t\tDev epoch 8, auc 0.942427, new accuracy 0.921575, right accuracy 0.921575,\n",
      "\t\t\t\t    Time taken for 8 epochs =  300.9673366546631\n",
      "Train epoch 10, average loss 0.0689277, average accuracy 0.976332,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 10, average loss 0.224909, average accuracy 0.922843,\n",
      "\t\tDev epoch 10, auc 0.940277, new accuracy 0.922843, right accuracy 0.922843,\n",
      "Train epoch 12, average loss 0.0498339, average accuracy 0.983695,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 12, average loss 0.239903, average accuracy 0.923044,\n",
      "\t\tDev epoch 12, auc 0.942537, new accuracy 0.923044, right accuracy 0.923044,\n",
      "\t\t\t\t    Time taken for 12 epochs =  434.042142868042\n",
      "Train epoch 14, average loss 0.0370719, average accuracy 0.988726,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 14, average loss 0.241078, average accuracy 0.923945,\n",
      "\t\tDev epoch 14, auc 0.940728, new accuracy 0.923945, right accuracy 0.923945,\n",
      "Saved model aut /newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints/aut_model\n",
      "hnk 100000\n",
      "completed cnn creation\n",
      "# batches = 781\n",
      "Train epoch 0, average loss 0.346576, average accuracy 0.855214,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 0, average loss 0.266563, average accuracy 0.888054,\n",
      "\t\tDev epoch 0, auc 0.913382, new accuracy 0.888054, right accuracy 0.888054,\n",
      "\t\t\t\t    Time taken for 0 epochs =  34.8063063621521\n",
      "Train epoch 2, average loss 0.215341, average accuracy 0.912892,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 2, average loss 0.207851, average accuracy 0.918102,\n",
      "\t\tDev epoch 2, auc 0.946659, new accuracy 0.918102, right accuracy 0.918102,\n",
      "Train epoch 4, average loss 0.164305, average accuracy 0.935849,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 4, average loss 0.213209, average accuracy 0.914196,\n",
      "\t\tDev epoch 4, auc 0.955386, new accuracy 0.914196, right accuracy 0.914196,\n",
      "\t\t\t\t    Time taken for 4 epochs =  168.50865769386292\n",
      "Train epoch 6, average loss 0.121427, average accuracy 0.954415,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 6, average loss 0.208898, average accuracy 0.919772,\n",
      "\t\tDev epoch 6, auc 0.9579, new accuracy 0.919772, right accuracy 0.919772,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 8, average loss 0.0876976, average accuracy 0.96816,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 8, average loss 0.204847, average accuracy 0.924112,\n",
      "\t\tDev epoch 8, auc 0.958948, new accuracy 0.924112, right accuracy 0.924112,\n",
      "\t\t\t\t    Time taken for 8 epochs =  301.99647784233093\n",
      "Train epoch 10, average loss 0.0628392, average accuracy 0.978453,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 10, average loss 0.199444, average accuracy 0.92725,\n",
      "\t\tDev epoch 10, auc 0.959642, new accuracy 0.92725, right accuracy 0.92725,\n",
      "Train epoch 12, average loss 0.0457157, average accuracy 0.985445,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 12, average loss 0.211818, average accuracy 0.92735,\n",
      "\t\tDev epoch 12, auc 0.959973, new accuracy 0.92735, right accuracy 0.92735,\n",
      "\t\t\t\t    Time taken for 12 epochs =  435.4455497264862\n",
      "Train epoch 14, average loss 0.0348752, average accuracy 0.989767,\n",
      "y_dev.shape (30000,)\n",
      "y_shuffled.shape (29952,)\n",
      "\t\tDev epoch 14, average loss 0.218774, average accuracy 0.92705,\n",
      "\t\tDev epoch 14, auc 0.960381, new accuracy 0.92705, right accuracy 0.92705,\n",
      "Saved model hnk /newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints/hnk_model\n"
     ]
    }
   ],
   "source": [
    "#Create and train the cnn models for all 4 domains\n",
    "#Pass the size to save the model name with size in different folders\n",
    "\n",
    "size_train = size_initial\n",
    "for key in list_df:\n",
    "    print(key, size_train)\n",
    "    train_cnn(key, size=size_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_accuracy(src_key, size, tar_key):\n",
    "    \n",
    "    batch_size=50\n",
    "    print('target',tar_key,'source', src_key)\n",
    "    V = dict_vocab_len[tar_key]\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key, size_folder))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "\n",
    "    \n",
    "    print(checkpoint_dir)\n",
    "    src_model = src_key\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "    graph_meta_file = checkpoint_dir + '/' + src_model +'_model.meta'\n",
    "    graph=tf.Graph()\n",
    "\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "    \n",
    "      #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "            new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "            new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    \n",
    "            x_dev = dict_dev_ids[tar_key]      \n",
    "            y_dev = dict_dev_ypred[tar_key]\n",
    "        \n",
    "            #create graph from saved model\n",
    "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "            input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "        \n",
    "            pred_proba = graph.get_operation_by_name(\"output/pred_proba\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        \n",
    "            y_pred = []\n",
    "            y_pred_proba = []\n",
    "            total_batch_acc = 0\n",
    "            num_batches = 0\n",
    "            y_shuffled = []\n",
    "            abs_y_pred_proba = []\n",
    "            for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                feed_dict = {input_x: x, input_y: y, dropout_keep_prob: 1.0}\n",
    "                batch_pred, batch_pred_proba  = sess.run([ predictions, pred_proba],feed_dict)\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                abs_y_pred_proba = np.concatenate([abs_y_pred_proba,np.absolute(batch_pred_proba[:,1] - batch_pred_proba[:,0])])\n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "\n",
    "                num_batches += 1           \n",
    "        \n",
    "            # Calculate auc\n",
    "            # false_pos_rate, true_pos_rate, _ = roc_curve(y_dev, y_pred_proba[:,1])\n",
    "            false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "            roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "            # print(src_key, tar_key, \"AUC\",\"{:.02%}\".format(roc_auc))\n",
    "            \n",
    "            #Calculate Accuracy\n",
    "            acc = accuracy_score(y_shuffled, y_pred, normalize=True )\n",
    "            #print('source',src_key, 'target',tar_key, \"accuracy\",\"{:.02%}\".format(acc))\n",
    "            #print(\"\")\n",
    "            f1_pos = f1_score(y_shuffled, y_pred, average = None)[1]\n",
    "            f1_neg = f1_score(y_shuffled, y_pred, average = None)[0]\n",
    "            f1_avg = f1_score(y_shuffled, y_pred, average = 'macro')\n",
    "        \n",
    "        #Save absolute_y_pred_proba\n",
    "        \n",
    "        #check if the batching process left remainders. This will result in incorrect length of y_pred_proba saved\n",
    "        if y_dev.shape[0] != abs_y_pred_proba.shape[0]:\n",
    "            print(\"Length of y_pred_proba does not match y_dev. Fix batch_size\")\n",
    "            print(\"Pred proba file not saved\")\n",
    "#         else:    \n",
    "#             file_name = \"src_\" + src_key + \"_tar_\" + tar_key + \"_\" + str(y_dev.shape[0])\n",
    "#             np.savez_compressed('test_file',pred_prob=abs_y_pred_proba)\n",
    "#             print( file_name, \"Saved file successfully\")\n",
    "    return acc, roc_auc, f1_pos, f1_neg, f1_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target toys source toys\n",
      "/newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints/toys_model\n",
      "target vid source toys\n",
      "/newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints/toys_model\n",
      "target aut source toys\n",
      "/newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints/toys_model\n",
      "target hnk source toys\n",
      "/newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/toys/size_100000/checkpoints/toys_model\n",
      "target toys source vid\n",
      "/newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints/vid_model\n",
      "target vid source vid\n",
      "/newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints/vid_model\n",
      "target aut source vid\n",
      "/newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints/vid_model\n",
      "target hnk source vid\n",
      "/newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/vid/size_100000/checkpoints/vid_model\n",
      "target toys source aut\n",
      "/newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints/aut_model\n",
      "target vid source aut\n",
      "/newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints/aut_model\n",
      "target aut source aut\n",
      "/newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints/aut_model\n",
      "target hnk source aut\n",
      "/newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/aut/size_100000/checkpoints/aut_model\n",
      "target toys source hnk\n",
      "/newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints/hnk_model\n",
      "target vid source hnk\n",
      "/newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints/hnk_model\n",
      "target aut source hnk\n",
      "/newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints/hnk_model\n",
      "target hnk source hnk\n",
      "/newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/size_100000/checkpoints/hnk_model\n",
      "\n",
      " Accuracy with transfer\n",
      "           toys       vid       aut       hnk\n",
      "toys  0.931633  0.756367    0.8206  0.811367\n",
      "vid     0.7463  0.906233  0.769567  0.779433\n",
      "aut   0.775133     0.759  0.923967  0.802267\n",
      "hnk   0.751433    0.7373  0.788933  0.927067\n",
      "\n",
      " AUC with transfer\n",
      "           toys       vid       aut       hnk\n",
      "toys  0.958905  0.509798  0.513877  0.507229\n",
      "vid   0.497448  0.933287  0.476135  0.522277\n",
      "aut   0.507536  0.498935  0.940801  0.494718\n",
      "hnk   0.504871  0.531176   0.44983  0.960401\n",
      "\n",
      " F1-positive with transfer\n",
      "           toys       vid       aut       hnk\n",
      "toys  0.959739   0.85741   0.90062  0.894526\n",
      "vid   0.851141  0.942845  0.868065  0.874103\n",
      "aut   0.870727  0.859584  0.956437  0.888922\n",
      "hnk   0.854586   0.84298  0.880964  0.956926\n",
      "\n",
      " F1-negative with transfer\n",
      "           toys       vid        aut       hnk\n",
      "toys  0.773545  0.163826  0.0790554  0.108398\n",
      "vid   0.142036  0.739126  0.0907536  0.110738\n",
      "aut   0.136899  0.150411   0.701401  0.100667\n",
      "hnk   0.144741  0.196554  0.0696444  0.762277\n",
      "\n",
      " F1-average with transfer\n",
      "           toys       vid       aut       hnk\n",
      "toys  0.866642  0.510618  0.489838  0.501462\n",
      "vid   0.496589  0.840986  0.479409  0.492421\n",
      "aut   0.503813  0.504998  0.828919  0.494795\n",
      "hnk   0.499663  0.519767  0.475304  0.859602\n"
     ]
    }
   ],
   "source": [
    "#calculate transfer accuracy for all domains, and save results in a dataframe\n",
    "list_df = ['toys','vid','aut','hnk'] \n",
    "#size = size_train\n",
    "size = size_initial\n",
    "transfer_results = pd.DataFrame(index=list_df,columns=list_df) #Dataframe to store accuracy on transfer. Col = Model, row = dataframe\n",
    "transfer_results_auc = pd.DataFrame(index=list_df,columns=list_df) #Dataframe to store AUC on transfer. Col = Model, row = dataframe\n",
    "transfer_results_f1_pos = pd.DataFrame(index=list_df,columns=list_df) #Dataframe to store f1-positive on transfer.\n",
    "transfer_results_f1_neg = pd.DataFrame(index=list_df,columns=list_df) #Dataframe to store f1-negative on transfer.\n",
    "transfer_results_f1_avg = pd.DataFrame(index=list_df,columns=list_df) #Dataframe to store f1-average on transfer.\n",
    "\n",
    "for s_key in list_df:\n",
    "    for t_key in list_df:\n",
    "        acc, roc_auc, f1_pos, f1_neg, f1_avg = predict_accuracy(s_key,size, t_key)\n",
    "        transfer_results[s_key][t_key] = acc\n",
    "        transfer_results_auc[s_key][t_key] = roc_auc\n",
    "        transfer_results_f1_pos[s_key][t_key] = f1_pos\n",
    "        transfer_results_f1_neg[s_key][t_key] = f1_neg\n",
    "        transfer_results_f1_avg[s_key][t_key] = f1_avg\n",
    "\n",
    "print('\\n Accuracy with transfer\\n',transfer_results)\n",
    "print('\\n AUC with transfer\\n',transfer_results_auc)\n",
    "print('\\n F1-positive with transfer\\n',transfer_results_f1_pos)\n",
    "print('\\n F1-negative with transfer\\n',transfer_results_f1_neg)\n",
    "print('\\n F1-average with transfer\\n',transfer_results_f1_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_transfer_vect = {} #Dictionary to store two domain vocab_vectorizer\n",
    "dict_transfer_train_ids = {} #Dictionary to store review ids of train set based on on two domains vocab_vectorizer\n",
    "dict_transfer_dev_ids = {} #Dictionary to store review ids of train set based on on two domains vocab_vectorizer\n",
    "for s_key in list_df:\n",
    "    dict_transfer_vect[s_key] = {}\n",
    "    dict_transfer_train_ids[s_key] = {}\n",
    "    dict_transfer_dev_ids[s_key] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note : size of src and tgt needs to be less than the original size read into dict_train_df with create_sized data.\n",
    "\n",
    "def process_transfer_data(src_key,tgt_key, size_train = 10000):\n",
    "      \n",
    "    #Create combined dataframe of reviewText from both domains\n",
    "    tmp_src_df = dict_train_df[src_key][:size_train] #picking the right sized subset from dict_train_df, dict_train_y\n",
    "    tmp_tgt_df = dict_train_df[tgt_key][:size_train]\n",
    "    tmp_src_df_dev = dict_dev_df[src_key][:np.int(size_train*0.3)] #picking the right sized subset from dict_train_df, dict_train_y\n",
    "    tmp_tgt_df_dev = dict_dev_df[tgt_key][:np.int(size_train*0.3)]\n",
    "    #print(tmp_src_df.shape,tmp_tgt_df.shape,tmp_src_df_dev.shape,tmp_tgt_df_dev.shape)\n",
    "    temp_two_df_reviews = pd.concat([tmp_src_df.reviewText,tmp_tgt_df.reviewText])\n",
    "    #print('combined df shape for',src_key,tgt_key,temp_two_df_reviews.shape)\n",
    "                \n",
    "    #create countVectorizer on combined dataframe of reviewText from both domains\n",
    "    dict_transfer_vect[src_key][tgt_key] = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=min_frequency)\n",
    "    dict_transfer_vect[src_key][tgt_key] = dict_transfer_vect[src_key][tgt_key].fit(temp_two_df_reviews)\n",
    "    print(\"Number words in training corpus for keys\",src_key,tgt_key,len(dict_transfer_vect[src_key][tgt_key].vocabulary_))\n",
    "                \n",
    "    #create id vectors of reviews for each df, train and dev set, using combined countVectorizer\n",
    "    #create id vectors of reviews for each df, train and dev set, using combined countVectorizer\n",
    "    dict_transfer_train_ids[src_key][tgt_key] = dict_transfer_vect[src_key][tgt_key].transform(tmp_src_df.reviewText)\n",
    "    dict_transfer_train_ids[tgt_key][src_key] = dict_transfer_vect[src_key][tgt_key].transform(tmp_tgt_df.reviewText)\n",
    "    dict_transfer_dev_ids[src_key][tgt_key] = dict_transfer_vect[src_key][tgt_key].transform(tmp_src_df_dev.reviewText)\n",
    "    dict_transfer_dev_ids[tgt_key][src_key] = dict_transfer_vect[src_key][tgt_key].transform(tmp_tgt_df_dev.reviewText)\n",
    "    # x_train = np.array(list(x_train))\n",
    "    dict_transfer_train_ids[src_key][tgt_key] = np.array(list(dict_transfer_train_ids[src_key][tgt_key]))\n",
    "    dict_transfer_train_ids[tgt_key][src_key] = np.array(list(dict_transfer_train_ids[tgt_key][src_key]))\n",
    "    dict_transfer_dev_ids[src_key][tgt_key] = np.array(list(dict_transfer_dev_ids[src_key][tgt_key]))\n",
    "    dict_transfer_dev_ids[tgt_key][src_key] = np.array(list(dict_transfer_dev_ids[tgt_key][src_key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source key vid target key aut\n",
      "Number words in training corpus for keys vid aut 212552\n",
      "vid train set shape (500000, 150)\n",
      "aut train set shape (500000, 150)\n",
      "vid dev set shape (150000, 150)\n",
      "aut dev set shape (150000, 150)\n"
     ]
    }
   ],
   "source": [
    "#Convert the train and dev data to ids using the combined source and target domain vocab_vectorizer\n",
    "size_initial = size_initial\n",
    "list_src = ['vid']\n",
    "list_tgt = ['aut']\n",
    "for s_key in list_src:\n",
    "    #print(s_key)\n",
    "    for t_key in list_tgt:\n",
    "        print('source key',s_key, 'target key',t_key)\n",
    "        process_transfer_data(s_key,t_key, size_train = size_initial)\n",
    "        print(s_key,'train set shape',dict_transfer_train_ids[s_key][t_key].shape)\n",
    "        print(t_key,'train set shape',dict_transfer_train_ids[t_key][s_key].shape)\n",
    "        print(s_key,'dev set shape',dict_transfer_dev_ids[s_key][t_key].shape)\n",
    "        print(t_key,'dev set shape',dict_transfer_dev_ids[t_key][s_key].shape)\n",
    "#         print(dict_train_df[s_key]['reviewText'].iloc[1])\n",
    "#         print(dict_transfer_train_ids[s_key][t_key][1])\n",
    "#         print(dict_train_df[t_key]['reviewText'].iloc[1])\n",
    "#         print(dict_transfer_train_ids[t_key][s_key][1])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transfer_cnn(skey,tkey,size=10000):\n",
    "     \n",
    "    x_train =  dict_transfer_train_ids[skey][tkey][:size]\n",
    "    y_train = dict_train_y[skey][:size]\n",
    "    x_dev = dict_transfer_dev_ids[skey][tkey][:np.int(size*0.3)] #Note : 0.3 is hard coded as the relative size of dev vs train.\n",
    "    y_dev = dict_dev_ypred[skey][:np.int(size*0.3)]\n",
    "    x_dev_tgt = dict_transfer_dev_ids[tkey][skey][:np.int(size*0.3)]\n",
    "    y_dev_tgt = dict_dev_ypred[tkey][:np.int(size*0.3)]\n",
    "    V = len(dict_transfer_vect[skey][tkey].vocabulary_)\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "        \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('completed cnn creation')\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            size_folder =  \"size_\" + str(size) \n",
    "            out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", skey, tkey, size_folder))\n",
    "            #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key))\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            model_name = ''.join([skey, tkey])\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, model_name  + \"_model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            # Write vocabulary\n",
    "            ## vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "            \n",
    "            tmp_ix = [2*e for e in range(np.int((num_epochs+2)/2))]\n",
    "            results = pd.DataFrame(index = tmp_ix,columns = ['size','acc','f1_avg','auc','f1_pos','f1_neg'])\n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                \n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=True), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    \n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                    \n",
    "#                     print('y_dev.shape',y_dev.shape)\n",
    "#                     print('y_shuffled.shape',y_shuffled.shape)\n",
    "                    \n",
    "                    if np.array_equal(y_shuffled,y_dev):\n",
    "                        print(\"Yes\")\n",
    "                    #right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                    #Calculate Accuracy\n",
    "                    #new_acc = accuracy_score(y_shuffled, y_pred, normalize=True )                   \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    f1_pos = f1_score(y_shuffled, y_pred, average = None)[1]\n",
    "                    f1_neg = f1_score(y_shuffled, y_pred, average = None)[0]\n",
    "                    f1_avg = f1_score(y_shuffled, y_pred, average = 'macro')\n",
    "                    \n",
    "                    results['acc'][e] = avg_acc\n",
    "                    results['f1_avg'][e] = f1_avg\n",
    "                    results['auc'][e] = roc_auc\n",
    "                    results['f1_pos'][e] = f1_pos\n",
    "                    results['f1_neg'][e] =  f1_neg\n",
    "                \n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\tDev epoch %d,average loss %0.3f,average accuracy %0.3f,auc %0.3f,f1_pos %0.3f,f1_neg %0.3f,f1_avg %0.3f\"\n",
    "                          %(e, avg_loss, avg_acc, roc_auc,f1_pos,f1_neg,f1_avg))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n",
    "                    \n",
    "            #Estimate accuracy on target dev set\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            num_batches = 0\n",
    "            total_auc = 0\n",
    "            y_pred = []\n",
    "            y_pred_proba = []\n",
    "            y_shuffled = []\n",
    "            total_batch_acc = 0\n",
    "            \n",
    "            \n",
    "            #Anamika added code for error analysis\n",
    "            y_pred_proba_pos = []\n",
    "            y_pred_proba_neg = []\n",
    "            \n",
    "            \n",
    "            for ii, (x, y) in enumerate(batch_generator(x_dev_tgt, y_dev_tgt, batch_size, Trainable=False), 1):\n",
    "\n",
    "                feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                total_loss += loss*len(x)\n",
    "                total_acc += accuracy*len(x)\n",
    "\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                y_pred_proba_pos= np.concatenate([y_pred_proba_pos, batch_pred_proba[:,1]])\n",
    "                y_pred_proba_neg= np.concatenate([y_pred_proba_neg, batch_pred_proba[:,0]])\n",
    "                \n",
    "                num_batches += 1\n",
    "                    \n",
    "            avg_loss = total_loss/(num_batches*batch_size)\n",
    "            avg_acc = total_acc/(num_batches*batch_size)\n",
    "\n",
    "#             print('y_dev.shape',y_dev.shape)\n",
    "#             print('y_shuffled.shape',y_shuffled.shape)\n",
    "\n",
    "            if np.array_equal(y_shuffled,y_dev):\n",
    "                print(\"Yes\")\n",
    "                #right_acc = total_batch_acc/(num_batches)\n",
    "\n",
    "            #Calculate Accuracy, AUC\n",
    "            #new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "            false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "            roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "            f1_pos = f1_score(y_shuffled, y_pred, average = None)[1]\n",
    "            f1_neg = f1_score(y_shuffled, y_pred, average = None)[0]\n",
    "            f1_avg = f1_score(y_shuffled, y_pred, average = 'macro')\n",
    "            #print(\"\\t\\t\",tkey,\"Dev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "            print(\"\\t\\t\",tkey,\"Dev epoch %d, average loss %0.3f,average accuracy %0.3f,auc %0.3f,f1_pos %0.3f,f1_neg %0.3f,f1_avg %0.3f\"\n",
    "                  %(e, avg_loss, avg_acc, roc_auc,f1_pos,f1_neg,f1_avg))\n",
    "\n",
    "        # Save model weights for future use.       \n",
    "            #save_path = saver.save(sess, checkpoint_prefix, global_step=20,write_meta_graph=False)\n",
    "            save_path = saver.save(sess, checkpoint_prefix)\n",
    "            print(\"Saved model\", model_name, save_path)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ##Anamika added code \n",
    "            \n",
    "              #for error analysis\n",
    "        print(\"ERROR ANALYSIS\")\n",
    "        print(len(y_shuffled))\n",
    "        src_key = skey\n",
    "        tar_key = tkey\n",
    "        print(\"src_key\", skey, \"tar_key\", tkey)\n",
    "        pos_err_pos = np.where((y_shuffled != y_pred) & (y_shuffled ==1))\n",
    "        neg_err_pos = np.where((y_shuffled != y_pred) & (y_shuffled ==0))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #for actual negatives that model predicted negatives(true negatives)\n",
    "        print(\"True negatives\")\n",
    "        #no_err_neg_probas = y_pred_proba_neg[np.where((y_shuffled == y_pred) & (y_shuffled ==0))]\n",
    "        no_err_neg_positions = np.where((y_shuffled == y_pred) & (y_shuffled ==0))\n",
    "        no_err_neg_probas = y_pred_proba_neg[no_err_neg_positions]\n",
    "        print(\"Values in no_err_neg_probas\", len(no_err_neg_probas))\n",
    "        #neg_err_pos_vals = \n",
    "        #if y_shuffled ==0 and y_pred == 1:\n",
    "        print(\"Correct neg probabilities > 0.9\", len(no_err_neg_probas[no_err_neg_probas >= 0.9]))\n",
    "        pos_nine_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==0) & (y_pred_proba_neg >= 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_nine_buckets[0][n]], \"Pred y\", y_pred[pos_nine_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_nine_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_nine_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_nine_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[t_key].reviewText.iloc[pos_nine_buckets[0][n]],'\\n')\n",
    "        \n",
    "        print(\"Correct neg probabilities between 0.8 and 0.9\", len(no_err_neg_probas[no_err_neg_probas >= 0.8 ])- len(no_err_neg_probas[no_err_neg_probas >= 0.9]))\n",
    "        pos_eight_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==0) & (y_pred_proba_neg >= 0.8) & (y_pred_proba_neg < 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_eight_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_eight_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_eight_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_eight_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "        print(\"Correct neg probabilities between 0.7 and 0.8\", len(no_err_neg_probas[no_err_neg_probas >= 0.7 ])- len(no_err_neg_probas[no_err_neg_probas >= 0.8]))\n",
    "        pos_seven_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==0) & (y_pred_proba_neg >= 0.7) & (y_pred_proba_neg < 0.8))\n",
    "        \n",
    "        for n in range(2):\n",
    "            print(\"actual y\", y_shuffled[pos_seven_buckets[0][n]], \"Pred y\", y_pred[pos_seven_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_seven_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_seven_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_seven_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "        print(\"Correct neg probabilities between 0.6 and 0.7\", len(no_err_neg_probas[no_err_neg_probas >= 0.6 ])- len(no_err_neg_probas[no_err_neg_probas >= 0.7]))\n",
    "        print(\"Correct neg probabilities between 0.5 and 0.6\", len(no_err_neg_probas[no_err_neg_probas >= 0.5 ])- len(no_err_neg_probas[no_err_neg_probas >= 0.6]))\n",
    "        print(\"Correct neg probabilities < 0.5\", len(no_err_neg_probas[no_err_neg_probas <0.5 ]))\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        #for actual negatives that model predicted positives(False positives)\n",
    "        print(\"False positives\")\n",
    "        neg_err_pos_probas = y_pred_proba_pos[neg_err_pos]\n",
    "        print(\"Values in neg_err_pos_probas\", len(neg_err_pos_probas))\n",
    "        #neg_err_pos_vals = \n",
    "        #if y_shuffled ==0 and y_pred == 1:\n",
    "        print(\"Pos probabilities > 0.9\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.9]))\n",
    "        pos_nine_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==0) & (y_pred_proba_pos >= 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_nine_buckets[0][n]], \"Pred y\", y_pred[pos_nine_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_nine_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_nine_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_nine_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_nine_buckets[0][n]],'\\n')\n",
    "        \n",
    "        print(\"Pos probabilities between 0.8 and 0.9\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.8 ])- len(neg_err_pos_probas[neg_err_pos_probas >= 0.9]))\n",
    "        pos_eight_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==0) & (y_pred_proba_pos >= 0.8) & (y_pred_proba_pos < 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_eight_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_eight_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_eight_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_eight_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "            \n",
    "        print(\"Pos probabilities between 0.7 and 0.8\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.7 ])- len(neg_err_pos_probas[neg_err_pos_probas >= 0.8]))\n",
    "        pos_seven_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==0) & (y_pred_proba_pos >= 0.7) & (y_pred_proba_pos < 0.8))\n",
    "        for n in range(2):\n",
    "            print(\"actual y\", y_shuffled[pos_seven_buckets[0][n]], \"Pred y\", y_pred[pos_seven_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_seven_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_seven_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_seven_buckets[0][n]]))\n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_seven_buckets[0][n]],'\\n')\n",
    "        print(\"Pos probabilities between 0.6 and 0.7\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.6 ])- len(neg_err_pos_probas[neg_err_pos_probas >= 0.7]))\n",
    "        print(\"Pos probabilities between 0.5 and 0.6\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.5 ])- len(neg_err_pos_probas[neg_err_pos_probas >= 0.6]))\n",
    "        print(\"Pos probabilities < 0.5\", len(neg_err_pos_probas[neg_err_pos_probas <0.5 ]))\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "  #for actual positives that model predicted positives(true positives)\n",
    "        print(\"True positives\")\n",
    "        #no_err_neg_probas = y_pred_proba_neg[np.where((y_shuffled == y_pred) & (y_shuffled ==0))]\n",
    "        no_err_pos_positions = np.where((y_shuffled == y_pred) & (y_shuffled ==1))\n",
    "        no_err_pos_probas = y_pred_proba_pos[no_err_pos_positions]\n",
    "        print(\"Values in no_err_pos_probas\", len(no_err_pos_probas))\n",
    "        #neg_err_pos_vals = \n",
    "        #if y_shuffled ==0 and y_pred == 1:\n",
    "        print(\"Correct positivr probabilities > 0.9\", len(no_err_pos_probas[no_err_pos_probas >= 0.9]))\n",
    "        pos_nine_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==1) & (y_pred_proba_pos >= 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_nine_buckets[0][n]], \"Pred y\", y_pred[pos_nine_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_nine_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_nine_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_nine_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_nine_buckets[0][n]],'\\n')\n",
    "        \n",
    "        print(\"Correct pos probabilities between 0.8 and 0.9\", len(no_err_pos_probas[no_err_pos_probas >= 0.8 ])- len(no_err_pos_probas[no_err_pos_probas >= 0.9]))\n",
    "        pos_eight_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==1) & (y_pred_proba_pos >= 0.8) & (y_pred_proba_pos < 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_eight_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_eight_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_eight_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_eight_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "        print(\"Correct pos probabilities between 0.7 and 0.8\", len(no_err_pos_probas[no_err_pos_probas >= 0.7 ])- len(no_err_pos_probas[no_err_pos_probas >= 0.8]))\n",
    "        pos_seven_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==1) & (y_pred_proba_pos >= 0.7) & (y_pred_proba_pos < 0.8))\n",
    "        for n in range(2):\n",
    "            print(\"actual y\", y_shuffled[pos_seven_buckets[0][n]], \"Pred y\", y_pred[pos_seven_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_seven_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_seven_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_seven_buckets[0][n]]))\n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_seven_buckets[0][n]],'\\n')\n",
    "        print(\"Correct pos probabilities between 0.6 and 0.7\", len(no_err_pos_probas[no_err_pos_probas >= 0.6 ])- len(no_err_pos_probas[no_err_pos_probas >= 0.7]))\n",
    "        print(\"Correct pos probabilities between 0.5 and 0.6\", len(no_err_pos_probas[no_err_pos_probas >= 0.5 ])- len(no_err_pos_probas[no_err_pos_probas >= 0.6]))\n",
    "        print(\"Correct pos probabilities < 0.5\", len(no_err_pos_probas[no_err_pos_probas <0.5 ]))\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")    \n",
    "        \n",
    "        \n",
    "#for actual positives that model predicted as negatives(False negatives)y_pred is 0\n",
    "\n",
    "        \n",
    "        print(\"False negatives\")\n",
    "        pos_err_neg_probas = y_pred_proba_neg[pos_err_pos]\n",
    "        print(\"Values in pos_err_neg_probas\", len(pos_err_neg_probas))\n",
    "        #neg_err_pos_vals = \n",
    "        #if y_shuffled ==0 and y_pred == 1:\n",
    "        print(\"Neg probabilities > 0.9\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.9]))\n",
    "        pos_nine_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==1) & (y_pred_proba_neg >= 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_nine_buckets[0][n]], \"Pred y\", y_pred[pos_nine_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_nine_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_nine_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_nine_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_nine_buckets[0][n]],'\\n')\n",
    "        \n",
    "        print(\"Neg probabilities between 0.8 and 0.9\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.8 ])- len(pos_err_neg_probas[pos_err_neg_probas >= 0.9]))\n",
    "        pos_eight_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==1) & (y_pred_proba_neg > 0.8) & (y_pred_proba_neg < 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_eight_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_eight_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_eight_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_eight_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "        print(\"Pos probabilities between 0.7 and 0.8\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.7 ])- len(pos_err_neg_probas[pos_err_neg_probas >= 0.8]))\n",
    "        pos_seven_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==1) & (y_pred_proba_neg > 0.7) & (y_pred_proba_neg < 0.8))\n",
    "        for n in range(2):\n",
    "            print(\"actual y\", y_shuffled[pos_seven_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_seven_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_seven_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_seven_buckets[0][n]]))\n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_seven_buckets[0][n]],'\\n')\n",
    "        print(\"Pos probabilities between 0.6 and 0.7\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.6 ])- len(pos_err_neg_probas[pos_err_neg_probas >= 0.7]))\n",
    "        print(\"Pos probabilities between 0.5 and 0.6\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.5 ])- len(pos_err_neg_probas[pos_err_neg_probas >= 0.6]))\n",
    "        print(\"Pos probabilities < 0.5\", len(pos_err_neg_probas[pos_err_neg_probas <0.5 ]))\n",
    "        \n",
    "    return results\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source key vid target key aut\n",
      "completed cnn creation\n",
      "# batches = 3906\n",
      "Train epoch 0, average loss 0.311937, average accuracy 0.86922,\n",
      "\tDev epoch 0,average loss 0.261,average accuracy 0.896,auc 0.936,f1_pos 0.935,f1_neg 0.733,f1_avg 0.834\n",
      "\t\t\t\t    Time taken for 0 epochs =  174.28555417060852\n",
      "Train epoch 2, average loss 0.210967, average accuracy 0.915425,\n",
      "\tDev epoch 2,average loss 0.211,average accuracy 0.917,auc 0.952,f1_pos 0.949,f1_neg 0.775,f1_avg 0.862\n",
      "Train epoch 4, average loss 0.176388, average accuracy 0.93065,\n",
      "\tDev epoch 4,average loss 0.200,average accuracy 0.922,auc 0.956,f1_pos 0.952,f1_neg 0.781,f1_avg 0.867\n",
      "\t\t\t\t    Time taken for 4 epochs =  845.0624132156372\n",
      "Train epoch 6, average loss 0.151406, average accuracy 0.940814,\n",
      "\tDev epoch 6,average loss 0.207,average accuracy 0.922,auc 0.956,f1_pos 0.953,f1_neg 0.772,f1_avg 0.863\n",
      "Train epoch 8, average loss 0.132072, average accuracy 0.948855,\n",
      "\tDev epoch 8,average loss 0.207,average accuracy 0.924,auc 0.956,f1_pos 0.953,f1_neg 0.788,f1_avg 0.871\n",
      "\t\t\t\t    Time taken for 8 epochs =  1515.5400259494781\n",
      "Train epoch 10, average loss 0.115795, average accuracy 0.955473,\n",
      "\tDev epoch 10,average loss 0.218,average accuracy 0.924,auc 0.953,f1_pos 0.954,f1_neg 0.787,f1_avg 0.871\n",
      "Train epoch 12, average loss 0.101698, average accuracy 0.961248,\n",
      "\tDev epoch 12,average loss 0.234,average accuracy 0.922,auc 0.955,f1_pos 0.953,f1_neg 0.778,f1_avg 0.865\n",
      "\t\t\t\t    Time taken for 12 epochs =  2185.7864241600037\n",
      "Train epoch 14, average loss 0.0908741, average accuracy 0.965404,\n",
      "\tDev epoch 14,average loss 0.239,average accuracy 0.923,auc 0.952,f1_pos 0.953,f1_neg 0.785,f1_avg 0.869\n",
      "\t\t aut Dev epoch 14, average loss 0.240,average accuracy 0.912,auc 0.930,f1_pos 0.949,f1_neg 0.696,f1_avg 0.822\n",
      "Saved model vidaut /newvolume/project_new/runs/vid/aut/size_500000/checkpoints/vidaut_model\n",
      "ERROR ANALYSIS\n",
      "149888\n",
      "src_key vid tar_key aut\n",
      "True negatives\n",
      "Values in no_err_neg_probas 15132\n",
      "Correct neg probabilities > 0.9 10177\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.00136260362342\n",
      "Neg prob value 0.998637378216\n",
      "review length 50\n",
      "I'm all for the 2nd amendment. Loved what this said, but it NEVER DELIVERED! Its been months now since it said it shipped, and still nothing! I don't know what else to add to it, giving it even 1 star is too much seeing as i don't even have it. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.0427438095212\n",
      "Neg prob value 0.957256138325\n",
      "review length 43\n",
      "Good thing it was not the bulb that requires me to remove the battery to get to. I called the manufacturer and there is no warranty. I would have expected at least a year. Some manufacturers give their bulbs a 2 year warranty. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 9.34033141675e-06\n",
      "Neg prob value 0.999990701675\n",
      "review length 40\n",
      "I bought 3 as Christmas gifts for my family. What a total waste of money!  After 10 minutes of being plugged in, the stupid thing couldn't even have melted a light frost!  We threw all of them in the trash! \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.00128888629843\n",
      "Neg prob value 0.998711109161\n",
      "review length 80\n",
      "This polish is not recommended for glasses but I got it anyhow to try and get some scratches out of my prescription sunglasses. It did not work. However the manufacturer never claimed it would work on glasses, may even have said not to use it on glasses so this rating isn't fair for what the product was meant for. I'm going to use it on my car headlights and see if it cleans them up.But don't buy for glasses. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.000374856143026\n",
      "Neg prob value 0.999625086784\n",
      "review length 70\n",
      "This is the first Dorman product that the product selector did not work for me.  The product that I received was for a 2 door coupe instead of the 4 door sedan.  Since i was not able to install it until today (weather permitting), I can not return it because its past some date.  Makes no sense. Wont be ordering car parts from amazon anymore, I'll just use Advance Auto. \n",
      "\n",
      "Correct neg probabilities between 0.8 and 0.9 1838\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.110137201846\n",
      "Neg prob value 0.889862835407\n",
      "review length 130\n",
      "I received this i-tec 6 in 1 portable for Christmas.  It seemed well and I have used it a few times, since then, for some light duty work.  Last week, I had it on the AC charger and asked a friend to unplug it.  He unplugged the wall charger, but not the plug on the 6 in 1.  I came back a few days later and it was deader than a door nail.  I tried bring it back up, but it doesn't seem to take a charge at all.  The charging transformer gets warm and I can get three weak lights out of the LED work light.  Nothing on the voltmeter.There is no mention, in the destructions, about replacing the battery.  I can't believe it gave up the ghost that easily. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.109445571899\n",
      "Neg prob value 0.890554487705\n",
      "review length 30\n",
      "My product was damaged and leaking Hazmat comicals. I tryed to return or find a way to return to the sender, but wasnt able. Right now I am sitting with broken product. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.197150051594\n",
      "Neg prob value 0.802849948406\n",
      "review length 150\n",
      "Product looks good and was easy to install. However, that's the end of my positive comments about this product...!I read the reviews about install nightmares.  Those had to have been written by somebody who has a hard time telling a screwdriver from a wrench. Removal and install took a total of 10 minutes.  It would have taken less, but the seal was not pre-attached to the light. But I'm getting ahead of myself...The product comes with the light.... That's it... Nothing else.. No purchase warranty or company contact information. No advertisement of additional products or even a sticker to advertise their company.  No installation instructions, No additional hardware, like screws or washers. So you better pray that your existing screws and washers are not rusted or stripped out.The product descriptions claims &#34;Super bright LED for superior visibility&#34;. I completely disagree with that claim. It's barley bright enough to see during the day, much less at night. My previous light (also aftermarket) was bright enough to blind you during the day. Not this one.Product quality of manufacture is sketchy. Mount holes were both closed with extra bleed over plastic from casting. Was nothing to pop the whole out, but does speak to quality of craftsmanship.  There were other places on the internal of the light that had the same bleed over from casting but had no affect on the install.Product comes with no manufactures warranty.Overall I would never order from this company (Spyder Auto) again.  I'm just praying it lasts longer than 12 months... \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.1038621068\n",
      "Neg prob value 0.8961378932\n",
      "review length 51\n",
      "These look like they might fit, Amazon says they are correct for my car. However they do not fit a Dodge Magnum 2005 r/t. I have contacted the seller, if they send me the correct part I will update. As for quality of the product, it appears to be OK. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.132308766246\n",
      "Neg prob value 0.867691218853\n",
      "review length 46\n",
      "The connections were broke when the item arrived to my door step I took it out of the package and tried it right away and that's when I found out the wires in side the prong was not connected I had to reconnect them my own..! \n",
      "\n",
      "Correct neg probabilities between 0.7 and 0.8 1252\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.299586504698\n",
      "Neg prob value 0.700413525105\n",
      "review length 49\n",
      "I received this i-tec 6 in 1 portable for Christmas.  It seemed well and I have used it a few times, since then, for some light duty work.  Last week, I had it on the AC charger and asked a friend to unplug it.  He unplugged the wall charger, but not the plug on the 6 in 1.  I came back a few days later and it was deader than a door nail.  I tried bring it back up, but it doesn't seem to take a charge at all.  The charging transformer gets warm and I can get three weak lights out of the LED work light.  Nothing on the voltmeter.There is no mention, in the destructions, about replacing the battery.  I can't believe it gave up the ghost that easily. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.219688177109\n",
      "Neg prob value 0.780311822891\n",
      "review length 31\n",
      "My product was damaged and leaking Hazmat comicals. I tryed to return or find a way to return to the sender, but wasnt able. Right now I am sitting with broken product. \n",
      "\n",
      "Correct neg probabilities between 0.6 and 0.7 968\n",
      "Correct neg probabilities between 0.5 and 0.6 897\n",
      "Correct neg probabilities < 0.5 0\n",
      "\n",
      "\n",
      "False positives\n",
      "Values in neg_err_pos_probas 6642\n",
      "Pos probabilities > 0.9 2739\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.995520472527\n",
      "Neg prob value 0.0044795004651\n",
      "review length 18\n",
      "I should have gotten an invoice with the k40 k-30 c b antenna so I can a get a replactment. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.902715146542\n",
      "Neg prob value 0.097284860909\n",
      "review length 149\n",
      "I had one of these in my workshed for a couple of years and finally decided to try it out. It didn't work, wouldn't get cold or warm. I figured it was just broken.  Found one at a yard sale for $20 so I got it as I was making a trip and wanted something cold to drink.  It didn't work either. Figured I got burnt for the 20. Found one on Amazon and decided to try it. I put ice cold soda in it from the fridge, put it in the car and started on a 300 mile trip. I made sure the unit was turned on and running. After about 150 miles, I decided I wanted to drink. Has anyone ever drank warm Mountain Dew??  The cans were cold when they went in but warm when I stopped to have one. Yes the unit was on the cold mode. Given a choice between buying a Yugo and one of these, I would go for the Yugo. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.997819900513\n",
      "Neg prob value 0.00218007992953\n",
      "review length 59\n",
      "I BOUGHT THESE FOR MY 2003 SL500 TO HELP WITH BRAKE DUST PROBLEM AS I KEEP IT IMMACULANT AS IT ONLY HAS 28K MILES,BUT THESE THING INSTALLED WITH NEW ROTORS AND BROKE IN/ SEATED PERFECTLY ARE THE SUEALINGEST THINGS EVER. DO NOT BUY UNLESS YOU WANT CONSTANT SQUEALING. WILL BE BUYING FACTORY AGAIN AND WASTING A LOT OF MONEY. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.946280181408\n",
      "Neg prob value 0.0537197999656\n",
      "review length 26\n",
      "great lighting, not as blue as they claim.Sylvania Ultra would be the better choice. will never purchase these againnot worth the money at allonly last 5 months \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.943096995354\n",
      "Neg prob value 0.0569030679762\n",
      "review length 43\n",
      "I thought this broom will be softer and make it easier to clean, but the brush is actually hard and not as flexible as I expected it to be. The straws started to drop out at the first time when I used it. \n",
      "\n",
      "Pos probabilities between 0.8 and 0.9 1207\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.803172230721\n",
      "Neg prob value 0.196827724576\n",
      "review length 46\n",
      "Amazon's fitment guide says it will fit a 2006 Jetta.  The instructions that come with the blade show the connection system that a Jetta uses.  But there is no hole in the wiper blade bracket for the peg on a Jetta wiper arm to slide into. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.832234382629\n",
      "Neg prob value 0.167765617371\n",
      "review length 24\n",
      "the ASIN clearly lists the SKU as a the manufacturer part number for a 6 pack but amazon sent me 1, its going back. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.815199315548\n",
      "Neg prob value 0.184800699353\n",
      "review length 54\n",
      "I own a custom shop the smithbilt bumper are not easy to install .It was a 2 hour job by professionals installers getting it to fit .Thank god we had three people a two post lift and a sledge hammer to get it over the rear frame .As far as looks go they look great . \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.834100663662\n",
      "Neg prob value 0.165899306536\n",
      "review length 126\n",
      "I'm hoping that I have an opportunity to actually use this product in order to rate its functionality, but at the moment I am frustrated because all that I received in my Amazon shipment box was a bright blue printed piece of cardboard with pre-cut holes where the strainers SHOULD have been. Alas, no strainers. No shrinkwrap. No product to use. I have initiated a replacement process with Amazon -- although it appears that one must actually *return* something and so, in order to facilitate a speedy resolution, I have dutifully printed out the return authorization and slipped it into the return envelope with the cardboard. I hope to update this review within a week or so with an actual product review. But for now? Boo. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.870553016663\n",
      "Neg prob value 0.129446938634\n",
      "review length 102\n",
      "I have \"KIA Optima 2008\". When I checked 'Find Parts' option, it said \"this product fits your vehicle.\" So, I ordered this filter.However, it is smaller than it should be. When I compare this product with OEM which was there when purchasing, each length is shorter at every dimension (1/2'', 1/4'', 3/32''), which probably means that it can't block pollutants in the air as advertised. So, I had to return this product. Anyway, Amazon is always awesome in terms of customer services.If there's anyone who can find the right one for Kia 2008, please give a comment. \n",
      "\n",
      "Pos probabilities between 0.7 and 0.8 935\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.775808393955\n",
      "Neg prob value 0.224191591144\n",
      "review length 149\n",
      "This armrest is ok, but I feel it could be better.  It's better than nothing, but it was a gift to my dad and I was a little disappointed.  I have 2 main problems with it: 1. It's just too soft.  That sounds weird to say, but it really is just too soft.  I wish they had used a higher-density foam inside.  2. I also wish it sat farther forward.  I'm about 5'10\" and my dad is about the same and when the seat is adjusted to a normal and comfortable position for our size, the pad is so far back between the seats that our elbows only make contact with the front edge of the armrest.  I'm considering cutting the bracket and rewelding it farther forward, about flush with the front of the top of the center console, so our elbows have something to sit on.  Honestly, for the money, I'd look at other armrests out there. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.727967739105\n",
      "Neg prob value 0.272032290697\n",
      "review length 44\n",
      "the price is right.  The fit was decent  (2009 audi a4).  the nice smell etc  didn't last too long.  Overall, i feel my previous cabin filter element worked better.  I'll be happy to replace this one again with the one from TTY.  Look elsewhere. \n",
      "\n",
      "Pos probabilities between 0.6 and 0.7 898\n",
      "Pos probabilities between 0.5 and 0.6 863\n",
      "Pos probabilities < 0.5 0\n",
      "\n",
      "\n",
      "True positives\n",
      "Values in no_err_pos_probas 121564\n",
      "Correct positivr probabilities > 0.9 106350\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.999777495861\n",
      "Neg prob value 0.000222571441554\n",
      "review length 20\n",
      "These LED bulbs are very bright white. Fit great easy to install and replace existing bulbs. I highly recommend them. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.958047270775\n",
      "Neg prob value 0.0419527478516\n",
      "review length 103\n",
      "The picture Amazon shows is not accurate.  The fan does not have the \"legs\" on the bottom extending toward the rear and the controls are on the side, not the front.  The picture is close otherwise.  I've read earlier reviews claiming the fan did not put out much of a \"breeze\".  Mine puts out a nice breeze, esp. in the high output setting.  I have not tried it with 8 D cell batteries, but it works fine with a power adapter hooked into a 110V outlet.  Without the \"legs\", it's not very stable and seems to be designed to hang from it's handle. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.99999332428\n",
      "Neg prob value 6.66876076139e-06\n",
      "review length 48\n",
      "This is a great item!  Anyone who drives my car laughs until they hit the gas.  Then they say it should be in yellow as a caution sign, because my car is fast.The lettering and logo could be a little cleaner, but is cool as it is! \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.999424815178\n",
      "Neg prob value 0.000575243786443\n",
      "review length 150\n",
      "Couldn't live without a Cruise Control in my 1997 Toyota Corolla which I just bought! Rostra offered left and right column mount controls as well as dash mount units. I would definitely suggest the column mount. I picked the right side mounted unit as I only had the windshield washer stalk on that side. Left side already had the stalks for the lights and wheel tilt and I thought it might be a little crowded. Turns out after installing it that it easily would have fit on the left (different part number). Depending on your preference or experience with other vehicles, either side would have worked fine. Dropping off the bottom of the steering column was easy on my vehicle and then it was simple to drill the hole to mount it through. You will want to be certain that you check for clearance on the column itself for the stub of the stalk and the wiring. You will also want to be sure that if your other column mounted items move, that they don't move into the location of the new cruise control switch. Otherwise, after finding a good location for installing it and then the hook-up to the actual cruise control unit was a breeze with just a simple pig tail to plug in. The &#34;touch&#34; to set the controls on this control switch is very &#34;soft.&#34; I sometimes brush by it when tuning the radio and disengage it. The only reason I gave this a 4 instead of 5 star is because this comes with a green LED light on it and it is so pale that during daylight hours it is next to impossible to tell if it is on or not. This really should be improved. I spend time looking away from the road and attempting to shield around the light to see if it is indeed on or not. There's no good reason for this to not be a more readily visible light. It shows fine after it is rather dark. After about 2000 miles of mixed driving I'm happy with the unit. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.998939692974\n",
      "Neg prob value 0.00106030900497\n",
      "review length 45\n",
      "Surprisingly for the cost, these actually do a great job of keeping the wind out of my eyes.  By no means are they &#34;High quality&#34;, but what a bargain!  Would highly recommend if you are looking for an inexpensive option while you ride. \n",
      "\n",
      "Correct pos probabilities between 0.8 and 0.9 7375\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.892863214016\n",
      "Neg prob value 0.107136808336\n",
      "review length 106\n",
      "I bought this ScanGauge to read the OBDII trouble codes on my 2007 Dodge Sprinter.It worked well, giving me the trouble code for a glow plug failure.Finding the trouble code description for your car manufacturer can take some time searching on the web.But once you've found a good source it goes faster.I also used it for Instant and average MPG on my '02 Honda.Worked well for that as well.Takes a few minutes to set it up for your specific vehicle,but that was explained well in the User Manual and Quick Start Guide.Overall, very satisfied with my purchase. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.879684567451\n",
      "Neg prob value 0.120315410197\n",
      "review length 71\n",
      "WeatherTech! What more can you say? I had the run of he mill Bug Deflector. I got a dealand when I got it,I got what I paid for,can I say it \"cheap crap\" WeatherTech is quality.Not only can you see, you can feel it. I intend to buy more WeatherTech products soon.I know what i'm getting is \"GOOD OLD USA MADE\".............Need I say more? \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.80591750145\n",
      "Neg prob value 0.194082528353\n",
      "review length 112\n",
      "I got this one after I'd returned a different one I got from Bean Garage, which was designed too low to rest an elbow on.  This one works, but I wish it had a more stable lid hinge (or something like a latch or pin) to keep the lid from sliding sideways when you move your arm on it.  It would also be better if it were designed so that you could lift it up to put your seatbelt on (maybe a second hinge about halfway down?).  When you go to put your seatbelt on, it's pretty cramped between one's hip and the storage box (and I'm not a real big person). \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.806693017483\n",
      "Neg prob value 0.193306937814\n",
      "review length 20\n",
      "Easy to install! Bought this for my husband for Christmas. The dealership wanted $400, but it only took 30 minutes. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.846930980682\n",
      "Neg prob value 0.15306904912\n",
      "review length 31\n",
      "I used this to replace the plug on a fan that gets bumped around a lot. Heavy duty enough to take it but no larger than the original. Very securely made. \n",
      "\n",
      "Correct pos probabilities between 0.7 and 0.8 3628\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.727799773216\n",
      "Neg prob value 0.272200196981\n",
      "review length 89\n",
      "I have a Hummer H3. It has the built-in channels on the roof. I used these adapters to mount a Surco S5060 Safari Rack. These adapters fit perfectly, but it does take some manuvering to position the square nut that goes in the channel. Also, the screws for these nuts are too short once the adapter is in position, so I had to dig up some longer bolts in my shed. Once in place the adapters are pretty study, just don't expect to tote an engine block on your roof. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.759892582893\n",
      "Neg prob value 0.240107432008\n",
      "review length 19\n",
      "These fit perfect on my Yamaha 650 Custom. Took about 30 minutes to attach with great results. Look good ! \n",
      "\n",
      "Correct pos probabilities between 0.6 and 0.7 2406\n",
      "Correct pos probabilities between 0.5 and 0.6 1805\n",
      "Correct pos probabilities < 0.5 0\n",
      "\n",
      "\n",
      "False negatives\n",
      "Values in pos_err_neg_probas 6550\n",
      "Neg probabilities > 0.9 1619\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0331247746944\n",
      "Neg prob value 0.966875255108\n",
      "review length 45\n",
      "Well made rotors. It looks like no matter how much you pay you get rotors of good quality made in China. So do not spend extra, get these rotors and spend your money on good brake pads (I only use original pads from a dealer) \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.000330451555783\n",
      "Neg prob value 0.999669551849\n",
      "review length 61\n",
      "stoner 91036 trim shine was not what i wanted it didnot work and i wish i could get a refund if i can it not anygood at all i think you should stop selling it to customer now tell me how can i get a refund asap thank you for the chance to to give me truth about this product it dosnt work \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0590866692364\n",
      "Neg prob value 0.940913319588\n",
      "review length 31\n",
      "Nice quality gloves.  However the Large size was tight on my hands, even tho I always take a large.  Seller quickly offered a return and I have ordered an XL pair. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.022493802011\n",
      "Neg prob value 0.977506160736\n",
      "review length 42\n",
      "This was used on a 1998 Jeep wrangler. 4.0 with a new Banks header.Ports were much larger than stock 98. No serious mods needed. Looks good.Arrived on time. Packaged poorly.Only real disappointment was &#34;made in china&#34;! \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0290044043213\n",
      "Neg prob value 0.970995604992\n",
      "review length 51\n",
      "Direct replacement for my 2009 Honda Fit. This only required four tabs to be unhooked from the filter housing for installation. No tools required other than a rag or gloves to keep your hands clean. Three minute installation if you know what a filter housing looks like. No mechanical experience required. \n",
      "\n",
      "Neg probabilities between 0.8 and 0.9 1146\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.178155764937\n",
      "Neg prob value 0.821844160557\n",
      "review length 150\n",
      "Well made with fit and function as expected. Curved lens are close to my eyes, so if that bugs you, you might want to check out another style. Once moving the vents allow just enough air flow to prevent fogging but without tearing me up. I need prescription lens and my local place admitted they couldn't do it for what Bobster charges (~ $90). I sent them in to SoCa and the fun began. Bobster customer service is an answering machine. It takes at least three calls and/or e-mails to get a reply. Sometimes they call, sometimes an e-mail. Maybe.After several hoops and delays on their end, they were &#34;rushed&#34; ordered to their eye guys. After two months, I got them back the same day I receive a message telling me to mail them back because they had switched mine with another customer's. Back they go the next day and then they e-mail me stating their was no mix up just keep them. Little late, Bobster.Last weekend was cold and windy for our Toys for Tots ride and they functioned very well. Comfortable even with the strap under the helmet; no fogging even with a face mask on. I would buy them again and recommend them to a friend.Good product, but poor to confusing customer service at Bobster. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.15640860796\n",
      "Neg prob value 0.84359139204\n",
      "review length 47\n",
      "Used on my 93' Yamaha Seca. I use both straps along with the 4 magnets. Holds well does not budge. One strap actually came un-snapped during one ride and I did not even know. The bag did not move an inch. Not the most pretty bag, but practical. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.151748180389\n",
      "Neg prob value 0.848251819611\n",
      "review length 45\n",
      "The lip that lies on the hood does not come in full contact so leaves and debris gather there..once on the highway they blow away but not a good feature. It even says in the manual it's not supposed to contact.  Looks great though. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.133194565773\n",
      "Neg prob value 0.866805374622\n",
      "review length 48\n",
      "SUPER BRIGHT. I have other LEDs in the interior light of my car but when I turn this light on it blows all the other ones away. Wouldn't recommend turning on when driving!! Brightest dome/interior light I've had and the product does not feel cheap either! \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.101501651108\n",
      "Neg prob value 0.898498356342\n",
      "review length 60\n",
      "Our Toilet was leaking near one of the bolt, so we read online that we can replace the bolts inside the toilet to fix the problem. The video instructed to use Teflon Sealant. And this worked great.Too bad we don't have to replace this old Toilet with a new one that would've cost us more than $200 to replace. \n",
      "\n",
      "Pos probabilities between 0.7 and 0.8 1122\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.258175164461\n",
      "Neg prob value 0.741824865341\n",
      "review length 21\n",
      "Nice vest. Looked just like the one he wanted at a local biker shop in our area. Thank you. God bless. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.204489991069\n",
      "Neg prob value 0.795510053635\n",
      "review length 61\n",
      "C-Tek is by Centric.  And this is their introductory line.  And they don't skimp on quality!  This is not cheap Ebay crap!  I inspected the rotor under intense scrutiny prior to mounting them.  I must say it looks like a $100 part!  There's no black coating on the hub, but you wont need that in Southern CA.  Quiet, smooth an efficient. \n",
      "\n",
      "Pos probabilities between 0.6 and 0.7 1202\n",
      "Pos probabilities between 0.5 and 0.6 1461\n",
      "Pos probabilities < 0.5 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>acc</th>\n",
       "      <th>f1_avg</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_pos</th>\n",
       "      <th>f1_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500000</td>\n",
       "      <td>0.895942</td>\n",
       "      <td>0.834385</td>\n",
       "      <td>0.936008</td>\n",
       "      <td>0.935354</td>\n",
       "      <td>0.733417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500000</td>\n",
       "      <td>0.917025</td>\n",
       "      <td>0.862185</td>\n",
       "      <td>0.951976</td>\n",
       "      <td>0.94912</td>\n",
       "      <td>0.77525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500000</td>\n",
       "      <td>0.921555</td>\n",
       "      <td>0.866685</td>\n",
       "      <td>0.956155</td>\n",
       "      <td>0.952213</td>\n",
       "      <td>0.781157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>500000</td>\n",
       "      <td>0.921888</td>\n",
       "      <td>0.862531</td>\n",
       "      <td>0.955513</td>\n",
       "      <td>0.952863</td>\n",
       "      <td>0.7722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>500000</td>\n",
       "      <td>0.92359</td>\n",
       "      <td>0.870835</td>\n",
       "      <td>0.955831</td>\n",
       "      <td>0.953382</td>\n",
       "      <td>0.788288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>500000</td>\n",
       "      <td>0.923783</td>\n",
       "      <td>0.870528</td>\n",
       "      <td>0.953238</td>\n",
       "      <td>0.953564</td>\n",
       "      <td>0.787492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>500000</td>\n",
       "      <td>0.922242</td>\n",
       "      <td>0.86535</td>\n",
       "      <td>0.955062</td>\n",
       "      <td>0.952874</td>\n",
       "      <td>0.777826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>500000</td>\n",
       "      <td>0.923116</td>\n",
       "      <td>0.869304</td>\n",
       "      <td>0.952498</td>\n",
       "      <td>0.953167</td>\n",
       "      <td>0.78544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      size       acc    f1_avg       auc    f1_pos    f1_neg\n",
       "0   500000  0.895942  0.834385  0.936008  0.935354  0.733417\n",
       "2   500000  0.917025  0.862185  0.951976   0.94912   0.77525\n",
       "4   500000  0.921555  0.866685  0.956155  0.952213  0.781157\n",
       "6   500000  0.921888  0.862531  0.955513  0.952863    0.7722\n",
       "8   500000   0.92359  0.870835  0.955831  0.953382  0.788288\n",
       "10  500000  0.923783  0.870528  0.953238  0.953564  0.787492\n",
       "12  500000  0.922242   0.86535  0.955062  0.952874  0.777826\n",
       "14  500000  0.923116  0.869304  0.952498  0.953167   0.78544"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_epochs = 1\n",
    "num_epochs = 15\n",
    "results_transfer = pd.DataFrame()\n",
    "s_key = 'vid' #Note : these need to be the same or a subset of the keys in the process_transfer input function which does the combined vocabulary preprocessing.\n",
    "t_key = 'aut'\n",
    "print('source key',s_key, 'target key',t_key)\n",
    "results = train_transfer_cnn(s_key,t_key,size_initial)\n",
    "results['size'] = size_initial\n",
    "results_transfer = pd.concat([results_transfer,results])\n",
    "results_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updated continue_train for adding samples from target domain to continue to train on source domain.\n",
    "def continue_transfer_train(skey,size,tkey,tgt_train_df,tgt_train_y): \n",
    "#Note size is size of source domain train set for picking the right sized model parameters\n",
    "    \n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"testruns\", src_key))\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", skey, tkey, size_folder))\n",
    "    #saved model being picked is the one that was trained on source domain, but with the vocabulary of both domains combined\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    print(checkpoint_dir)\n",
    "    src_model = ''.join([skey, tkey])\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "#     graph_meta_file = checkpoint_dir + '/' + src_model +'01_model.meta'\n",
    "    graph=tf.Graph()\n",
    "    \n",
    "    x_train = tgt_train_df\n",
    "    y_train = tgt_train_y\n",
    "    x_dev = dict_transfer_dev_ids[tkey][skey][:np.int(size*0.3)]\n",
    "    y_dev = dict_dev_ypred[tkey][:np.int(size*0.3)]\n",
    "    V = len(dict_transfer_vect[skey][tkey].vocabulary_)\n",
    "    \n",
    "    #create a dataframe to store the results together and pass back out of the function\n",
    "    tmp_ix = [2*e for e in range(int((num_epochs+2)/2))]\n",
    "    results = pd.DataFrame(index = tmp_ix,columns = ['size','acc','auc','f1_neg','f1_pos','f1_avg'])\n",
    "       \n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:           \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    " \n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "          #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "#             new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "#             new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "            \n",
    "            \n",
    "            #initializing weights from a previous session \n",
    "            initialising_model = src_model+'_model'\n",
    "            print(\" RESTORING SESSION FOR WEIGHTS INITIALIZATION\")\n",
    "            # Exclude output layer weights from variables we will restore\n",
    "            variables_to_restore = [v for v in tf.global_variables()]\n",
    "            # Replace variables scope with that of the current model\n",
    "            loader = tf.train.Saver({v.op.name.replace(src_model, initialising_model): v for v in variables_to_restore})\n",
    "            load_path = checkpoint_dir + '/' + initialising_model \n",
    "            #load_path = checkpoint_dir  \n",
    "            loader.restore(sess, load_path)\n",
    "            print(\" Model loaded from: \" + load_path) \n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "           \n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=True), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    #avg_auc = total_auc/(batch_size*(len(x_train)//batch_size))\n",
    "                   # print(\"Train epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    #Anamika added code for error analysis\n",
    "                    y_pred_proba_pos = []\n",
    "                    y_pred_proba_neg = []\n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size, Trainable=False), 1):\n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        y_pred_proba_pos= np.concatenate([y_pred_proba_pos, batch_pred_proba[:,1]])\n",
    "                        y_pred_proba_neg= np.concatenate([y_pred_proba_neg, batch_pred_proba[:,0]])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                                    \n",
    "                    #right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                    #Calculate Accuracy\n",
    "                    #new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "                     \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    f1_pos = f1_score(y_shuffled, y_pred, average = None)[1]\n",
    "                    f1_neg = f1_score(y_shuffled, y_pred, average = None)[0]\n",
    "                    f1_avg = f1_score(y_shuffled, y_pred, average = 'macro')\n",
    "                    \n",
    "                    results['acc'][e] = avg_acc\n",
    "                    results['auc'][e] = roc_auc\n",
    "                    results['f1_avg'][e] = f1_avg\n",
    "                    results['f1_pos'][e] = f1_pos\n",
    "                    results['f1_neg'][e] = f1_neg\n",
    "                    \n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\t\",tkey,\"Dev epoch %d, average loss %0.3f,average accuracy %0.3f,auc %0.3f,f1_pos %0.3f,f1_neg %0.3f,f1_avg %0.3f\"\n",
    "                  %(e, avg_loss, avg_acc, roc_auc,f1_pos,f1_neg,f1_avg))\n",
    "                    #print(\"\\t\\tDev epoch {}, auc {:g}, new accuracy {:g}, right accuracy {:g},\".format(e,  roc_auc, new_acc, right_acc))\n",
    "                    #print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n",
    "                    \n",
    "                  ##Anamika added code \n",
    "            \n",
    "              #for error analysis\n",
    "        print(\"ERROR ANALYSIS\")\n",
    "        print(len(y_shuffled))\n",
    "        src_key = skey\n",
    "        tar_key = tkey\n",
    "        print(\"src_key\", skey, \"tar_key\", tkey)\n",
    "        pos_err_pos = np.where((y_shuffled != y_pred) & (y_shuffled ==1))\n",
    "        neg_err_pos = np.where((y_shuffled != y_pred) & (y_shuffled ==0))\n",
    "        \n",
    "        \n",
    "        \n",
    "        #for actual negatives that model predicted negatives(true negatives)\n",
    "        print(\"True negatives\")\n",
    "        #no_err_neg_probas = y_pred_proba_neg[np.where((y_shuffled == y_pred) & (y_shuffled ==0))]\n",
    "        no_err_neg_positions = np.where((y_shuffled == y_pred) & (y_shuffled ==0))\n",
    "        no_err_neg_probas = y_pred_proba_neg[no_err_neg_positions]\n",
    "        print(\"Values in no_err_neg_probas\", len(no_err_neg_probas))\n",
    "        #neg_err_pos_vals = \n",
    "        #if y_shuffled ==0 and y_pred == 1:\n",
    "        print(\"Correct neg probabilities > 0.9\", len(no_err_neg_probas[no_err_neg_probas >= 0.9]))\n",
    "        pos_nine_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==0) & (y_pred_proba_neg >= 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_nine_buckets[0][n]], \"Pred y\", y_pred[pos_nine_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_nine_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_nine_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_nine_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[t_key].reviewText.iloc[pos_nine_buckets[0][n]],'\\n')\n",
    "        \n",
    "        print(\"Correct neg probabilities between 0.8 and 0.9\", len(no_err_neg_probas[no_err_neg_probas >= 0.8 ])- len(no_err_neg_probas[no_err_neg_probas >= 0.9]))\n",
    "        pos_eight_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==0) & (y_pred_proba_neg >= 0.8) & (y_pred_proba_neg < 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_eight_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_eight_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_eight_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_eight_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "        print(\"Correct neg probabilities between 0.7 and 0.8\", len(no_err_neg_probas[no_err_neg_probas >= 0.7 ])- len(no_err_neg_probas[no_err_neg_probas >= 0.8]))\n",
    "        pos_seven_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==0) & (y_pred_proba_neg >= 0.7) & (y_pred_proba_neg < 0.8))\n",
    "        \n",
    "        for n in range(2):\n",
    "            print(\"actual y\", y_shuffled[pos_seven_buckets[0][n]], \"Pred y\", y_pred[pos_seven_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_seven_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_seven_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_seven_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "        print(\"Correct neg probabilities between 0.6 and 0.7\", len(no_err_neg_probas[no_err_neg_probas >= 0.6 ])- len(no_err_neg_probas[no_err_neg_probas >= 0.7]))\n",
    "        print(\"Correct neg probabilities between 0.5 and 0.6\", len(no_err_neg_probas[no_err_neg_probas >= 0.5 ])- len(no_err_neg_probas[no_err_neg_probas >= 0.6]))\n",
    "        print(\"Correct neg probabilities < 0.5\", len(no_err_neg_probas[no_err_neg_probas <0.5 ]))\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        #for actual negatives that model predicted positives(False positives)\n",
    "        print(\"False positives\")\n",
    "        neg_err_pos_probas = y_pred_proba_pos[neg_err_pos]\n",
    "        print(\"Values in neg_err_pos_probas\", len(neg_err_pos_probas))\n",
    "        #neg_err_pos_vals = \n",
    "        #if y_shuffled ==0 and y_pred == 1:\n",
    "        print(\"Pos probabilities > 0.9\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.9]))\n",
    "        pos_nine_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==0) & (y_pred_proba_pos >= 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_nine_buckets[0][n]], \"Pred y\", y_pred[pos_nine_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_nine_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_nine_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_nine_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_nine_buckets[0][n]],'\\n')\n",
    "        \n",
    "        print(\"Pos probabilities between 0.8 and 0.9\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.8 ])- len(neg_err_pos_probas[neg_err_pos_probas >= 0.9]))\n",
    "        pos_eight_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==0) & (y_pred_proba_pos >= 0.8) & (y_pred_proba_pos < 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_eight_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_eight_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_eight_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_eight_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "            \n",
    "        print(\"Pos probabilities between 0.7 and 0.8\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.7 ])- len(neg_err_pos_probas[neg_err_pos_probas >= 0.8]))\n",
    "        pos_seven_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==0) & (y_pred_proba_pos >= 0.7) & (y_pred_proba_pos < 0.8))\n",
    "        for n in range(2):\n",
    "            print(\"actual y\", y_shuffled[pos_seven_buckets[0][n]], \"Pred y\", y_pred[pos_seven_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_seven_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_seven_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_seven_buckets[0][n]]))\n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_seven_buckets[0][n]],'\\n')\n",
    "        print(\"Pos probabilities between 0.6 and 0.7\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.6 ])- len(neg_err_pos_probas[neg_err_pos_probas >= 0.7]))\n",
    "        print(\"Pos probabilities between 0.5 and 0.6\", len(neg_err_pos_probas[neg_err_pos_probas >= 0.5 ])- len(neg_err_pos_probas[neg_err_pos_probas >= 0.6]))\n",
    "        print(\"Pos probabilities < 0.5\", len(neg_err_pos_probas[neg_err_pos_probas <0.5 ]))\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "  #for actual positives that model predicted positives(true positives)\n",
    "        print(\"True positives\")\n",
    "        #no_err_neg_probas = y_pred_proba_neg[np.where((y_shuffled == y_pred) & (y_shuffled ==0))]\n",
    "        no_err_pos_positions = np.where((y_shuffled == y_pred) & (y_shuffled ==1))\n",
    "        no_err_pos_probas = y_pred_proba_pos[no_err_pos_positions]\n",
    "        print(\"Values in no_err_pos_probas\", len(no_err_pos_probas))\n",
    "        #neg_err_pos_vals = \n",
    "        #if y_shuffled ==0 and y_pred == 1:\n",
    "        print(\"Correct positivr probabilities > 0.9\", len(no_err_pos_probas[no_err_pos_probas >= 0.9]))\n",
    "        pos_nine_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==1) & (y_pred_proba_pos >= 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_nine_buckets[0][n]], \"Pred y\", y_pred[pos_nine_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_nine_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_nine_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_nine_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_nine_buckets[0][n]],'\\n')\n",
    "        \n",
    "        print(\"Correct pos probabilities between 0.8 and 0.9\", len(no_err_pos_probas[no_err_pos_probas >= 0.8 ])- len(no_err_pos_probas[no_err_pos_probas >= 0.9]))\n",
    "        pos_eight_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==1) & (y_pred_proba_pos >= 0.8) & (y_pred_proba_pos < 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_eight_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_eight_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_eight_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_eight_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "        print(\"Correct pos probabilities between 0.7 and 0.8\", len(no_err_pos_probas[no_err_pos_probas >= 0.7 ])- len(no_err_pos_probas[no_err_pos_probas >= 0.8]))\n",
    "        pos_seven_buckets = np.where((y_shuffled == y_pred) & (y_shuffled ==1) & (y_pred_proba_pos >= 0.7) & (y_pred_proba_pos < 0.8))\n",
    "        for n in range(2):\n",
    "            print(\"actual y\", y_shuffled[pos_seven_buckets[0][n]], \"Pred y\", y_pred[pos_seven_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_seven_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_seven_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_seven_buckets[0][n]]))\n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_seven_buckets[0][n]],'\\n')\n",
    "        print(\"Correct pos probabilities between 0.6 and 0.7\", len(no_err_pos_probas[no_err_pos_probas >= 0.6 ])- len(no_err_pos_probas[no_err_pos_probas >= 0.7]))\n",
    "        print(\"Correct pos probabilities between 0.5 and 0.6\", len(no_err_pos_probas[no_err_pos_probas >= 0.5 ])- len(no_err_pos_probas[no_err_pos_probas >= 0.6]))\n",
    "        print(\"Correct pos probabilities < 0.5\", len(no_err_pos_probas[no_err_pos_probas <0.5 ]))\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"\")    \n",
    "        \n",
    "        \n",
    "#for actual positives that model predicted as negatives(False negatives)y_pred is 0\n",
    "\n",
    "        \n",
    "        print(\"False negatives\")\n",
    "        pos_err_neg_probas = y_pred_proba_neg[pos_err_pos]\n",
    "        print(\"Values in pos_err_neg_probas\", len(pos_err_neg_probas))\n",
    "        #neg_err_pos_vals = \n",
    "        #if y_shuffled ==0 and y_pred == 1:\n",
    "        print(\"Neg probabilities > 0.9\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.9]))\n",
    "        pos_nine_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==1) & (y_pred_proba_neg >= 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_nine_buckets[0][n]], \"Pred y\", y_pred[pos_nine_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_nine_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_nine_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_nine_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_nine_buckets[0][n]],'\\n')\n",
    "        \n",
    "        print(\"Neg probabilities between 0.8 and 0.9\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.8 ])- len(pos_err_neg_probas[pos_err_neg_probas >= 0.9]))\n",
    "        pos_eight_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==1) & (y_pred_proba_neg > 0.8) & (y_pred_proba_neg < 0.9))\n",
    "        for n in range(5):\n",
    "            print(\"actual y\", y_shuffled[pos_eight_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_eight_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_eight_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_eight_buckets[0][n]]))\n",
    "            \n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_eight_buckets[0][n]],'\\n')\n",
    "        print(\"Pos probabilities between 0.7 and 0.8\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.7 ])- len(pos_err_neg_probas[pos_err_neg_probas >= 0.8]))\n",
    "        pos_seven_buckets = np.where((y_shuffled != y_pred) & (y_shuffled ==1) & (y_pred_proba_neg > 0.7) & (y_pred_proba_neg < 0.8))\n",
    "        for n in range(2):\n",
    "            print(\"actual y\", y_shuffled[pos_seven_buckets[0][n]], \"Pred y\", y_pred[pos_eight_buckets[0][n]])\n",
    "            print('Pos prob value', y_pred_proba_pos[pos_seven_buckets[0][n]])\n",
    "            print('Neg prob value', y_pred_proba_neg[pos_seven_buckets[0][n]])\n",
    "            print('review length', np.count_nonzero(dict_dev_ids[tar_key][pos_seven_buckets[0][n]]))\n",
    "            print(dict_dev_df[tar_key].reviewText.iloc[pos_seven_buckets[0][n]],'\\n')\n",
    "        print(\"Pos probabilities between 0.6 and 0.7\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.6 ])- len(pos_err_neg_probas[pos_err_neg_probas >= 0.7]))\n",
    "        print(\"Pos probabilities between 0.5 and 0.6\", len(pos_err_neg_probas[pos_err_neg_probas >= 0.5 ])- len(pos_err_neg_probas[pos_err_neg_probas >= 0.6]))\n",
    "        print(\"Pos probabilities < 0.5\", len(pos_err_neg_probas[pos_err_neg_probas <0.5 ]))\n",
    "           \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    return results\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate the predicted probability for positive and negative class on target train set using source model\n",
    "#source model is the one built on the combined vocabulary of both\n",
    "def predict_transfer_probability(src_key, size, tar_key):\n",
    "    #size here is full target train set size - so we can calculate uncertainty on all of it before sorting\n",
    "    \n",
    "    batch_size=50\n",
    "    print('target',tar_key,'source', src_key)\n",
    "    V = len(dict_transfer_vect[src_key][tar_key].vocabulary_)\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key, tar_key, size_folder))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))    \n",
    "    print(checkpoint_dir)\n",
    "    src_model = ''.join([src_key, tar_key])\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "    graph_meta_file = checkpoint_dir + '/' + src_model +'_model.meta'\n",
    "    graph=tf.Graph()\n",
    "    \n",
    "    x_train = dict_transfer_train_ids[tar_key][src_key][:size]\n",
    "    y_train = dict_train_y[tar_key][:size]\n",
    "\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "    \n",
    "      #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "            new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "            new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "        \n",
    "            #create graph from saved model\n",
    "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "            input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "        \n",
    "            pred_proba = graph.get_operation_by_name(\"output/pred_proba\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        \n",
    "            y_pred = []\n",
    "            y_pred_proba = []\n",
    "            total_batch_acc = 0\n",
    "            num_batches = 0\n",
    "            y_shuffled = []\n",
    "            abs_y_pred_proba = []\n",
    "            for ii, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size, Trainable=False), 1):\n",
    "                        \n",
    "                feed_dict = {input_x: x, input_y: y, dropout_keep_prob: 1.0}\n",
    "                batch_pred, batch_pred_proba  = sess.run([ predictions, pred_proba],feed_dict)\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                abs_y_pred_proba = np.concatenate([abs_y_pred_proba,np.absolute(batch_pred_proba[:,1] - batch_pred_proba[:,0])])\n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "\n",
    "                num_batches += 1       \n",
    "            #y_pred = np.array(y_pred_list)         \n",
    "              \n",
    "            new_acc = total_batch_acc/(num_batches)\n",
    "            print(new_acc)\n",
    "        \n",
    "            # Calculate auc\n",
    "            # false_pos_rate, true_pos_rate, _ = roc_curve(y_dev, y_pred_proba[:,1])\n",
    "            false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "            roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "            print(src_key, tar_key, \"AUC\",\"{:.02%}\".format(roc_auc))\n",
    "            \n",
    "            #Calculate Accuracy\n",
    "            acc = accuracy_score(y_shuffled, y_pred, normalize=True )\n",
    "            #print(np.sum(y_shuffled==y_pred)/y_pred.shape[0])\n",
    "            print('source',src_key, 'target',tar_key, \"accuracy\",\"{:.02%}\".format(acc))\n",
    "            print(\"\")\n",
    "        \n",
    "        #Save absolute_y_pred_proba\n",
    "        \n",
    "        #check if the batching process left remainders. This will result in incorrect length of y_pred_proba saved\n",
    "        if y_train.shape[0] != abs_y_pred_proba.shape[0]:\n",
    "            print(\"Length of y_pred_proba does not match y_dev. Fix batch_size\")\n",
    "            print(\"Pred proba file not saved\")\n",
    "        else:\n",
    "            return abs_y_pred_proba\n",
    "#             file_name = \"src_\" + src_key + \"_tar_\" + tar_key + \"_\" + \"train\" + str(y_train.shape[0])\n",
    "#             np.savez_compressed(file_name,pred_prob=abs_y_pred_proba)\n",
    "#             print( file_name, \"Saved file successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source domain vid Target Domain aut\n",
      "Training on target sample of size: 150000\n",
      "(150000, 150) (150000,)\n",
      "/newvolume/project_new/runs/vid/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/project_new/runs/vid/aut/size_500000/checkpoints/vidaut_model\n",
      " Model loaded from: /newvolume/project_new/runs/vid/aut/size_500000/checkpoints/vidaut_model\n",
      "# batches = 1171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f97c93c7a158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtgt_train_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_train_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtgt_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_train_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontinue_transfer_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_train_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtgt_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mresults_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults_random\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-fad363200bc9>\u001b[0m in \u001b[0;36mcontinue_transfer_train\u001b[0;34m(skey, size, tkey, tgt_train_df, tgt_train_y)\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_y\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                    \u001b[0;31m# _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mtotal_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Continue training with adding random samples from target domain\n",
    "num_epochs = 15\n",
    "size_model = size_initial\n",
    "size_list = [150000]\n",
    "src_key = s_key\n",
    "tgt_key = t_key\n",
    "print('Source domain',src_key,'Target Domain',tgt_key)\n",
    "results_random = pd.DataFrame()\n",
    "for size in size_list:\n",
    "    print('Training on target sample of size:',size)\n",
    "    tgt_train_df = dict_transfer_train_ids[tgt_key][src_key][:size]\n",
    "    tgt_train_y = dict_train_y[tgt_key][:size]\n",
    "    print(tgt_train_df.shape,tgt_train_y.shape)\n",
    "    results = continue_transfer_train(src_key,size_model,tgt_key,tgt_train_df,tgt_train_y)\n",
    "    results['size'] = size\n",
    "    results_random = pd.concat([results_random,results])\n",
    "\n",
    "results_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vid\n",
      "aut\n"
     ]
    }
   ],
   "source": [
    "print(s_key)\n",
    "print(t_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target aut source vid\n",
      "/newvolume/project_new/runs/vid/aut/size_500000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/project_new/runs/vid/aut/size_500000/checkpoints/vidaut_model\n",
      "0.910522\n",
      "vid aut AUC 92.90%\n",
      "source vid target aut accuracy 91.05%\n",
      "\n",
      "(500000,) (500000,) (500000,)\n",
      "max, min uncertainty absolute 1.0 1.60932540894e-06\n",
      "max, min length 150 0\n",
      "max, min certainty*length inf 1.60932540894e-06\n",
      "For range 0.00 to 0.10, average certainty = 0.28, average length = 57.65, average certainty per length = 1.22\n",
      "For range 0.10 to 0.20, average certainty = 0.69, average length = 54.06, average certainty per length = inf\n",
      "For range 0.20 to 0.30, average certainty = 0.87, average length = 51.82, average certainty per length = 5.06\n",
      "For range 0.30 to 0.40, average certainty = 0.95, average length = 49.97, average certainty per length = 5.09\n",
      "For range 0.40 to 0.50, average certainty = 0.98, average length = 49.29, average certainty per length = 5.09\n",
      "For range 0.50 to 0.60, average certainty = 0.99, average length = 49.11, average certainty per length = 5.07\n",
      "For range 0.60 to 0.70, average certainty = 1.00, average length = 49.56, average certainty per length = 4.82\n",
      "For range 0.70 to 0.80, average certainty = 1.00, average length = 50.59, average certainty per length = 4.45\n",
      "For range 0.80 to 0.90, average certainty = 1.00, average length = 52.62, average certainty per length = 4.23\n",
      "For range 0.90 to 1.00, average certainty = 1.00, average length = 58.89, average certainty per length = 3.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "#Calculating certainty for target train set review\n",
    "\n",
    "src_key = s_key #source domain\n",
    "tgt_key = t_key #target domain\n",
    "size = size_initial #This is source model train set data size to read from the right file.\n",
    "\n",
    "#calculate absolute difference of positive and negative class probability for target train set using source model built on combined vocab.\n",
    "\n",
    "u_train_target_abs = predict_transfer_probability(src_key, size, tgt_key)\n",
    "train_target_len = np.count_nonzero(dict_transfer_train_ids[tgt_key][src_key],axis =1)\n",
    "c_div_len_target = u_train_target_abs*150/train_target_len #calculates certainty per word id in the review\n",
    "# file_name = \"src_\" + src_key + \"_tar_\" + tar_key + \"_\" + \"train\" + str(size)\n",
    "# u_train_target_abs = np.load(file_name)\n",
    "print(u_train_target_abs.shape,train_target_len.shape,c_div_len_target.shape)\n",
    "print('max, min uncertainty absolute',np.max(u_train_target_abs),np.min(u_train_target_abs))\n",
    "print('max, min length',np.max(train_target_len),np.min(train_target_len))\n",
    "print('max, min certainty*length',np.max(c_div_len_target),np.min(c_div_len_target))\n",
    "\n",
    "#See if certainty is correlated with length\n",
    "span = 0.1\n",
    "sort_ids = np.argsort(u_train_target_abs)\n",
    "u_train_target_sorted = u_train_target_abs[sort_ids]\n",
    "train_target_len_sorted = train_target_len[sort_ids]\n",
    "c_div_len_sorted = c_div_len_target[sort_ids]\n",
    "range_l = int(span*len(u_train_target_sorted))\n",
    "\n",
    "for i in range(np.int(1/span)):\n",
    "    print(\"For range %0.2f to %0.2f, average certainty = %0.2f, average length = %0.2f, average certainty per length = %0.2f\"\n",
    "          %(i*span,(i+1)*span,np.average(u_train_target_sorted[i*range_l:(i+1)*range_l]),np.average(train_target_len_sorted[i*range_l:(i+1)*range_l]),\n",
    "           np.average(c_div_len_sorted[i*range_l:(i+1)*range_l])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe23f8f8518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "import pylab\n",
    "\n",
    "matplotlib.pyplot.scatter(u_train_target_abs,train_target_len)\n",
    "\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target labels pre sort [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      " Target labels post sort [1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1]\n",
      "\n",
      " Certainty sorted \n",
      " First 20 [  1.60932541e-06   1.73747540e-05   3.64184380e-05   7.59661198e-05\n",
      "   8.64863396e-05   9.34600830e-05   9.70661640e-05   9.89139080e-05\n",
      "   1.23113394e-04   1.34795904e-04   1.39355659e-04   1.49756670e-04\n",
      "   1.59054995e-04   1.70171261e-04   1.72197819e-04   1.78039074e-04\n",
      "   1.81734562e-04   1.98572874e-04   2.14219093e-04   2.32756138e-04] \n",
      " Last 20 [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.]\n",
      "\n",
      "Training on least certain first\n",
      "Training on target sample of size: 150000 with average certainty 0.614\n",
      "(150000, 150) (150000,)\n",
      "/newvolume/project_new/runs/vid/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/project_new/runs/vid/aut/size_500000/checkpoints/vidaut_model\n",
      " Model loaded from: /newvolume/project_new/runs/vid/aut/size_500000/checkpoints/vidaut_model\n",
      "# batches = 1171\n",
      "Train epoch 0, average loss 0.470108, average accuracy 0.796922,\n",
      "\t\t aut Dev epoch 0, average loss 0.176,average accuracy 0.932,auc 0.954,f1_pos 0.961,f1_neg 0.728,f1_avg 0.845\n",
      "\t\t\t\t    Time taken for 0 epochs =  60.60604977607727\n",
      "Train epoch 2, average loss 0.301401, average accuracy 0.871291,\n",
      "\t\t aut Dev epoch 2, average loss 0.164,average accuracy 0.939,auc 0.960,f1_pos 0.964,f1_neg 0.771,f1_avg 0.868\n",
      "Train epoch 4, average loss 0.240886, average accuracy 0.899332,\n",
      "\t\t aut Dev epoch 4, average loss 0.169,average accuracy 0.939,auc 0.962,f1_pos 0.965,f1_neg 0.772,f1_avg 0.869\n",
      "\t\t\t\t    Time taken for 4 epochs =  277.46394085884094\n",
      "Train epoch 6, average loss 0.195257, average accuracy 0.921388,\n",
      "\t\t aut Dev epoch 6, average loss 0.174,average accuracy 0.940,auc 0.961,f1_pos 0.965,f1_neg 0.781,f1_avg 0.873\n",
      "Train epoch 8, average loss 0.16543, average accuracy 0.934284,\n",
      "\t\t aut Dev epoch 8, average loss 0.186,average accuracy 0.940,auc 0.959,f1_pos 0.965,f1_neg 0.777,f1_avg 0.871\n",
      "\t\t\t\t    Time taken for 8 epochs =  494.3634054660797\n",
      "Train epoch 10, average loss 0.137884, average accuracy 0.946854,\n",
      "\t\t aut Dev epoch 10, average loss 0.193,average accuracy 0.940,auc 0.961,f1_pos 0.965,f1_neg 0.783,f1_avg 0.874\n",
      "Train epoch 12, average loss 0.118729, average accuracy 0.954853,\n",
      "\t\t aut Dev epoch 12, average loss 0.202,average accuracy 0.939,auc 0.961,f1_pos 0.964,f1_neg 0.784,f1_avg 0.874\n",
      "\t\t\t\t    Time taken for 12 epochs =  711.2565491199493\n",
      "Train epoch 14, average loss 0.103863, average accuracy 0.961011,\n",
      "\t\t aut Dev epoch 14, average loss 0.216,average accuracy 0.940,auc 0.959,f1_pos 0.965,f1_neg 0.782,f1_avg 0.874\n",
      "ERROR ANALYSIS\n",
      "149888\n",
      "src_key vid tar_key aut\n",
      "True negatives\n",
      "Values in no_err_neg_probas 16037\n",
      "Correct neg probabilities > 0.9 12878\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.000540665409062\n",
      "Neg prob value 0.999459326267\n",
      "review length 50\n",
      "I'm all for the 2nd amendment. Loved what this said, but it NEVER DELIVERED! Its been months now since it said it shipped, and still nothing! I don't know what else to add to it, giving it even 1 star is too much seeing as i don't even have it. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.00411872193217\n",
      "Neg prob value 0.995881319046\n",
      "review length 43\n",
      "Good thing it was not the bulb that requires me to remove the battery to get to. I called the manufacturer and there is no warranty. I would have expected at least a year. Some manufacturers give their bulbs a 2 year warranty. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 9.86233953881e-06\n",
      "Neg prob value 0.999990105629\n",
      "review length 40\n",
      "I bought 3 as Christmas gifts for my family. What a total waste of money!  After 10 minutes of being plugged in, the stupid thing couldn't even have melted a light frost!  We threw all of them in the trash! \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 3.4694292026e-06\n",
      "Neg prob value 0.999996542931\n",
      "review length 80\n",
      "This polish is not recommended for glasses but I got it anyhow to try and get some scratches out of my prescription sunglasses. It did not work. However the manufacturer never claimed it would work on glasses, may even have said not to use it on glasses so this rating isn't fair for what the product was meant for. I'm going to use it on my car headlights and see if it cleans them up.But don't buy for glasses. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.0617946088314\n",
      "Neg prob value 0.938205420971\n",
      "review length 61\n",
      "I ran into a trailer hitch with my Honda, and decided that I needed a band-aid.. I was sorely disappointed when I realized that this sticker is waaay smaller than your average bumper sticker.  I seriously could have bought a box of real large band-aids and gotten the same effect (with band-aids to spare for human injuries) for the same price.. \n",
      "\n",
      "Correct neg probabilities between 0.8 and 0.9 1212\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.17786321044\n",
      "Neg prob value 0.822136819363\n",
      "review length 150\n",
      "I needed them for my back bumper because people do not know how to park in New York. They keep hitting the rear with those stupid giant SUV. If you don't know your distance between my car and yours get some glasses. I followed the instructions. Since there were four and I only need two I thought that if it doesn't work the first time I has spares that I can use and get other person to place them on the rear. What a mistake. I washed my car and did't use any wax, place the protectors on and a few hours later returned back to my car to see them on the ground. Okay maybe I'm not the right person for this easy task. I went to the Nissan dealership and had them apply them. It seemed to work until after I returned home and noticed that one of them fell off during the drive home (ten minutes), Geez!! also they were nothing like the picture. They had some silver along the rim and some kind of reflection paint on them. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.155005604029\n",
      "Neg prob value 0.844994366169\n",
      "review length 39\n",
      "The shirt that was sent didn't look at all like this one. The shirt displayed isn't the long sleeve &#34;Don't tread on me&#34;. The one received is gold with the snake on it! What a Christmas surprise! \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.120110422373\n",
      "Neg prob value 0.87988960743\n",
      "review length 37\n",
      "we were vaery dissapointed to recieve these floor mats and see the fairy was dressed in blue not purple like on the seat covers we ordered at the same time. did not like the blue color at all. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.194439992309\n",
      "Neg prob value 0.80555999279\n",
      "review length 25\n",
      "Too big for my car.I tried to evaluate from the dimensions, pictures, reviews and comments , but I could be sure until I received it. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.162530601025\n",
      "Neg prob value 0.837469398975\n",
      "review length 148\n",
      "I purchased a Torrin 1000 lb transmission jack to help with the a Toyota FJ40 transmission rebuild job, having no desire to try and rig my floor jack to do the job. The unit is ok, except where it comes to the part that is different from any other jack - the top that holds the trxmission.First, the instructions don't match the actual unit. It says you should attach the two chains to the chain bolts - well, there is only 1 chain, and the \"chain bolts\" don't even fit through the chain. There are no photos other than the one small b&w; photo on this instructions (that don't match). It is not possible to understand what they are telling you to do with the arms at the corners of the top plate without further instruction.The little fish hook screw that should hold the chain is only about 1/8\" in diameter and since it is about 4\" long, very flimsy. Secondly, the chain catch on the other side of the top plate - doesn't. If you get even 1/4\" of slack in the chain, it drops out of the catch (which is just a slot cut in a bracket welded to the top). So, at 1am last night, in the rain that was about to turn to downpour, with the trxmission 1/2 way out, I said to heck with it, and went to the barn to retrieve a strap.Up and down works pretty well. Tilt features work pretty well on the top plate. The handle to raise the unit comes out too easily, normally while you are trying to use it to pull the transmission across the floor, as that is the only thing that you can easily use to move the 200 lbs sitting on the jack. The knob to lower the unit opens the valve extremely quickly such that the load can drop much too fast. The unit is very tippy and must be carefully balanced when you have a transmission on it. Did I mention that securing the transmission on the jack is impossible with the supplied chain - oh, yea, I said that. It bears repeating.In all, it gets the job done, but it is not a unit I'd recommend anyone purchase for more than a single transmission. \n",
      "\n",
      "Correct neg probabilities between 0.7 and 0.8 768\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.230969384313\n",
      "Neg prob value 0.769030570984\n",
      "review length 80\n",
      "I needed them for my back bumper because people do not know how to park in New York. They keep hitting the rear with those stupid giant SUV. If you don't know your distance between my car and yours get some glasses. I followed the instructions. Since there were four and I only need two I thought that if it doesn't work the first time I has spares that I can use and get other person to place them on the rear. What a mistake. I washed my car and did't use any wax, place the protectors on and a few hours later returned back to my car to see them on the ground. Okay maybe I'm not the right person for this easy task. I went to the Nissan dealership and had them apply them. It seemed to work until after I returned home and noticed that one of them fell off during the drive home (ten minutes), Geez!! also they were nothing like the picture. They had some silver along the rim and some kind of reflection paint on them. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.236557766795\n",
      "Neg prob value 0.763442277908\n",
      "review length 69\n",
      "The shirt that was sent didn't look at all like this one. The shirt displayed isn't the long sleeve &#34;Don't tread on me&#34;. The one received is gold with the snake on it! What a Christmas surprise! \n",
      "\n",
      "Correct neg probabilities between 0.6 and 0.7 634\n",
      "Correct neg probabilities between 0.5 and 0.6 545\n",
      "Correct neg probabilities < 0.5 0\n",
      "\n",
      "\n",
      "False positives\n",
      "Values in neg_err_pos_probas 5737\n",
      "Pos probabilities > 0.9 3256\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.976211726665\n",
      "Neg prob value 0.0237882696092\n",
      "review length 18\n",
      "I should have gotten an invoice with the k40 k-30 c b antenna so I can a get a replactment. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.998190581799\n",
      "Neg prob value 0.0018093616236\n",
      "review length 99\n",
      "Usually batteries that have heads similar to this battery come with alternate connection options. This battery ships for connections via the front/top only.The Suzuki VS1400's battery case is set up for side-mount connections. It might be possible to install with an adapter, additional parts, or a bit of fudging things, but this battery is not an exact fit for the bike it's advertised for.Worse, Buy4Easy & T-Motorsports refused to accept responsibly for their mistake. Amazon had to intervene and cover the shipping, as the vendor demanded I pay for the return shipping, even though it was their listing error. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.929010510445\n",
      "Neg prob value 0.0709895342588\n",
      "review length 24\n",
      "the ASIN clearly lists the SKU as a the manufacturer part number for a 6 pack but amazon sent me 1, its going back. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.999791681767\n",
      "Neg prob value 0.000208352590562\n",
      "review length 121\n",
      "Installed this to interface the steering wheel controls on my 2006 Ford F-250 with my new Kenwood DNX9140 Nav Receiver.  Of the 4 steering wheel controls only the volume up and scan up work as they did witht hte original Ford radio.  Volume down mutes the volume instead of stepping it down and mode will only select the FM bands on the Kenwood receiver and none of the other mode options such as AM, DVD/CD, XM etc.  The manufacturer's technical support was very fasr but only verified that the interface device had not been tested with the Kenwood DNX9140.  It would be nice if the manufacturer would specify which specific units from the various manufacturers the interface had been tested with! \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.978858053684\n",
      "Neg prob value 0.0211419668049\n",
      "review length 59\n",
      "I BOUGHT THESE FOR MY 2003 SL500 TO HELP WITH BRAKE DUST PROBLEM AS I KEEP IT IMMACULANT AS IT ONLY HAS 28K MILES,BUT THESE THING INSTALLED WITH NEW ROTORS AND BROKE IN/ SEATED PERFECTLY ARE THE SUEALINGEST THINGS EVER. DO NOT BUY UNLESS YOU WANT CONSTANT SQUEALING. WILL BE BUYING FACTORY AGAIN AND WASTING A LOT OF MONEY. \n",
      "\n",
      "Pos probabilities between 0.8 and 0.9 841\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.808008909225\n",
      "Neg prob value 0.191991105676\n",
      "review length 149\n",
      "This armrest is ok, but I feel it could be better.  It's better than nothing, but it was a gift to my dad and I was a little disappointed.  I have 2 main problems with it: 1. It's just too soft.  That sounds weird to say, but it really is just too soft.  I wish they had used a higher-density foam inside.  2. I also wish it sat farther forward.  I'm about 5'10\" and my dad is about the same and when the seat is adjusted to a normal and comfortable position for our size, the pad is so far back between the seats that our elbows only make contact with the front edge of the armrest.  I'm considering cutting the bracket and rewelding it farther forward, about flush with the front of the top of the center console, so our elbows have something to sit on.  Honestly, for the money, I'd look at other armrests out there. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.858121871948\n",
      "Neg prob value 0.141878157854\n",
      "review length 150\n",
      "Product looks good and was easy to install. However, that's the end of my positive comments about this product...!I read the reviews about install nightmares.  Those had to have been written by somebody who has a hard time telling a screwdriver from a wrench. Removal and install took a total of 10 minutes.  It would have taken less, but the seal was not pre-attached to the light. But I'm getting ahead of myself...The product comes with the light.... That's it... Nothing else.. No purchase warranty or company contact information. No advertisement of additional products or even a sticker to advertise their company.  No installation instructions, No additional hardware, like screws or washers. So you better pray that your existing screws and washers are not rusted or stripped out.The product descriptions claims &#34;Super bright LED for superior visibility&#34;. I completely disagree with that claim. It's barley bright enough to see during the day, much less at night. My previous light (also aftermarket) was bright enough to blind you during the day. Not this one.Product quality of manufacture is sketchy. Mount holes were both closed with extra bleed over plastic from casting. Was nothing to pop the whole out, but does speak to quality of craftsmanship.  There were other places on the internal of the light that had the same bleed over from casting but had no affect on the install.Product comes with no manufactures warranty.Overall I would never order from this company (Spyder Auto) again.  I'm just praying it lasts longer than 12 months... \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.857077121735\n",
      "Neg prob value 0.142922878265\n",
      "review length 148\n",
      "UPDATE: I gave these things 4 stars a few months ago after the quick shipping and easy install. Unfortunately, today, the dang thing broke while opening the door. Nothing more than pulling on the latch in a typical fashion. I'm not the Hulk and I wasn't twerking on the thing like Miley. I simply wanted to remove myself from the passenger side of my daughter's car and snap, the plastic crumbled like a sheet pan of Blue Meth dropping from a cool steel countertop. Sadly, I can no longer recommend this product unless you have a budget for 5 or 6 of them to get you through the year. 2 stars for this product.ORIGINAL: This item was an exact fit for my daughter's 2001 Corolla. It was as good or better than OEM at a fraction of the cost. Shipped fast' arrived sooner than anticipated and was a snap to install. Word of caution, these are the handle receptacles only, make sure you keep the original screw or you'll be out trying to find one that matches your car's interior. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.850337147713\n",
      "Neg prob value 0.149662911892\n",
      "review length 86\n",
      "The only reason for the one star rating is that I ordered (and paid for) the 3 in spacers and received the 2 in. The product itself is a good quality spacer and was going to be used with 3 in lift springs. So instead of losing the shipping time I bought 4in springs from the brick and mortar shop in town. and used the 2in spacer that I was sent. Definitely dissipointed with not getting what I paid for, but thats the disadvantage of buying online. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.886303186417\n",
      "Neg prob value 0.113696783781\n",
      "review length 54\n",
      "I bought both the 14&#34; and 26 &#34;  stealth wiper blade and the 14&#34; blade broke immediately out of the package. The 26&#34; is working great. .I  like Michelin and Uniroyal( a Michelin Subsidiary).I was surprised how flimsy the attachment clip  was on the wiper. I cannot recommend  the 14 &#34; \n",
      "\n",
      "Pos probabilities between 0.7 and 0.8 612\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.716445982456\n",
      "Neg prob value 0.283554077148\n",
      "review length 19\n",
      "On arrival, both seat belts were malfunctional. Neither extended or retracted properly. POC. Returned them, haven't found a replacement yet... \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.703967511654\n",
      "Neg prob value 0.296032458544\n",
      "review length 54\n",
      "Save yourself the grief of using these pieces of trash and buy some other brand that actually do what their supposed to do without falling into pieces before you get the chance to use them. I would be ashamed to have my company name represented with junk like this and Carrand should be too. \n",
      "\n",
      "Pos probabilities between 0.6 and 0.7 526\n",
      "Pos probabilities between 0.5 and 0.6 502\n",
      "Pos probabilities < 0.5 0\n",
      "\n",
      "\n",
      "True positives\n",
      "Values in no_err_pos_probas 124908\n",
      "Correct positivr probabilities > 0.9 119802\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 1.0\n",
      "Neg prob value 3.83356635325e-08\n",
      "review length 20\n",
      "These LED bulbs are very bright white. Fit great easy to install and replace existing bulbs. I highly recommend them. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.99999833107\n",
      "Neg prob value 1.71767692336e-06\n",
      "review length 48\n",
      "This is a great item!  Anyone who drives my car laughs until they hit the gas.  Then they say it should be in yellow as a caution sign, because my car is fast.The lettering and logo could be a little cleaner, but is cool as it is! \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.999888062477\n",
      "Neg prob value 0.000111974870379\n",
      "review length 150\n",
      "Couldn't live without a Cruise Control in my 1997 Toyota Corolla which I just bought! Rostra offered left and right column mount controls as well as dash mount units. I would definitely suggest the column mount. I picked the right side mounted unit as I only had the windshield washer stalk on that side. Left side already had the stalks for the lights and wheel tilt and I thought it might be a little crowded. Turns out after installing it that it easily would have fit on the left (different part number). Depending on your preference or experience with other vehicles, either side would have worked fine. Dropping off the bottom of the steering column was easy on my vehicle and then it was simple to drill the hole to mount it through. You will want to be certain that you check for clearance on the column itself for the stub of the stalk and the wiring. You will also want to be sure that if your other column mounted items move, that they don't move into the location of the new cruise control switch. Otherwise, after finding a good location for installing it and then the hook-up to the actual cruise control unit was a breeze with just a simple pig tail to plug in. The &#34;touch&#34; to set the controls on this control switch is very &#34;soft.&#34; I sometimes brush by it when tuning the radio and disengage it. The only reason I gave this a 4 instead of 5 star is because this comes with a green LED light on it and it is so pale that during daylight hours it is next to impossible to tell if it is on or not. This really should be improved. I spend time looking away from the road and attempting to shield around the light to see if it is indeed on or not. There's no good reason for this to not be a more readily visible light. It shows fine after it is rather dark. After about 2000 miles of mixed driving I'm happy with the unit. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.999964952469\n",
      "Neg prob value 3.50169757439e-05\n",
      "review length 45\n",
      "Surprisingly for the cost, these actually do a great job of keeping the wind out of my eyes.  By no means are they &#34;High quality&#34;, but what a bargain!  Would highly recommend if you are looking for an inexpensive option while you ride. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 1.0\n",
      "Neg prob value 2.41340791796e-09\n",
      "review length 21\n",
      "very nice helmet. as good as fox or thor. perfect fit and light weight.and you just cant beat the price. \n",
      "\n",
      "Correct pos probabilities between 0.8 and 0.9 2437\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.812677204609\n",
      "Neg prob value 0.18732278049\n",
      "review length 31\n",
      "I used this to replace the plug on a fan that gets bumped around a lot. Heavy duty enough to take it but no larger than the original. Very securely made. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.843456447124\n",
      "Neg prob value 0.156543508172\n",
      "review length 8\n",
      "velcro is useless, but the shade is great \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.871121942997\n",
      "Neg prob value 0.128878027201\n",
      "review length 31\n",
      "Nice quality gloves.  However the Large size was tight on my hands, even tho I always take a large.  Seller quickly offered a return and I have ordered an XL pair. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.892143070698\n",
      "Neg prob value 0.107856981456\n",
      "review length 112\n",
      "So...I bought this to for use in our fifth wheel BUT now use it as a dog water bowl.  We take our dogs to the dog park every day and there is no water source.  I would bring water and a bowl and it would constantly get knocked over.  I started bring this thing instead as an interim solution and I will never go back to bowls!  It can't get tipped over!  I fill it up once and it's MORE THAN ENOUGH water for my dogs as well as other dogs at the park!  Even thought it's not used for it's intended purpose, it is still an excellent investment for us! \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.892473399639\n",
      "Neg prob value 0.107526622713\n",
      "review length 139\n",
      "After reading the reviews, I was skeptical.  The fabric is less than optimal, as are the straps to attempt to keep the seat cushions in place.  They do slide around a lot.  Now for the good news.  I purchased two of these, as my 10yo car lacks seat heaters.  I live in the Teton Valley where it snows in Sept and temperatures flux 50 on any given day.  The cushions heat up quick and maintain temperature well.  I have had these for three months now and keep waiting for them to fail.  Temps are now in the single digits, so these are holding up and working fine.  If they konk out, I will repost!  I am putting seat covers over the cushion to keep them in place.  They do slide all over when I get in & out of the car. \n",
      "\n",
      "Correct pos probabilities between 0.7 and 0.8 1183\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.723995685577\n",
      "Neg prob value 0.276004314423\n",
      "review length 22\n",
      "These worked so well it was a no-brainer to get the rear window lifts too.  They should sell them in a 2-pack. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.744262218475\n",
      "Neg prob value 0.255737781525\n",
      "review length 23\n",
      "They fit my 2006 F150.  Exactly what i thought they would be, standard black valve caps.  It's no secret what you're buying here. \n",
      "\n",
      "Correct pos probabilities between 0.6 and 0.7 838\n",
      "Correct pos probabilities between 0.5 and 0.6 648\n",
      "Correct pos probabilities < 0.5 0\n",
      "\n",
      "\n",
      "False negatives\n",
      "Values in pos_err_neg_probas 3206\n",
      "Neg probabilities > 0.9 1270\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0832599624991\n",
      "Neg prob value 0.916740059853\n",
      "review length 71\n",
      "WeatherTech! What more can you say? I had the run of he mill Bug Deflector. I got a dealand when I got it,I got what I paid for,can I say it \"cheap crap\" WeatherTech is quality.Not only can you see, you can feel it. I intend to buy more WeatherTech products soon.I know what i'm getting is \"GOOD OLD USA MADE\".............Need I say more? \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0534900799394\n",
      "Neg prob value 0.946509957314\n",
      "review length 53\n",
      "I am writing this review because I my light has just arrived. However the quality is spot on and it looks good, but when you add to cart... add 2. This is only one light, I assumed it was a pair. I was wrong. Ordered another one, will install when it gets here. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0345414765179\n",
      "Neg prob value 0.965458452702\n",
      "review length 45\n",
      "The lip that lies on the hood does not come in full contact so leaves and debris gather there..once on the highway they blow away but not a good feature. It even says in the manual it's not supposed to contact.  Looks great though. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0339049808681\n",
      "Neg prob value 0.966095030308\n",
      "review length 48\n",
      "SUPER BRIGHT. I have other LEDs in the interior light of my car but when I turn this light on it blows all the other ones away. Wouldn't recommend turning on when driving!! Brightest dome/interior light I've had and the product does not feel cheap either! \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.000223848779569\n",
      "Neg prob value 0.999776184559\n",
      "review length 61\n",
      "stoner 91036 trim shine was not what i wanted it didnot work and i wish i could get a refund if i can it not anygood at all i think you should stop selling it to customer now tell me how can i get a refund asap thank you for the chance to to give me truth about this product it dosnt work \n",
      "\n",
      "Neg probabilities between 0.8 and 0.9 491\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.184482455254\n",
      "Neg prob value 0.815517485142\n",
      "review length 45\n",
      "Well made rotors. It looks like no matter how much you pay you get rotors of good quality made in China. So do not spend extra, get these rotors and spend your money on good brake pads (I only use original pads from a dealer) \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.19211243093\n",
      "Neg prob value 0.807887494564\n",
      "review length 30\n",
      "Carlisle Turf Trac R/S Lawn & Garden Tire - 20X10-8 was very useful in getting the clutch addressed. Thanks but it took an ectremlely long time to receive the part.  Shame Shame \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.153174072504\n",
      "Neg prob value 0.846825957298\n",
      "review length 35\n",
      "Altough it took like 2 hours to equip for me, it is nice. I would recommand to get another needle cause' the needle from the package is kinda weak and it broke while I was sewing. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.129630252719\n",
      "Neg prob value 0.87036973238\n",
      "review length 148\n",
      "About a month before buying this Black and Decker unit I bought the Energizer 84020 power station for what I thought was a great Gold Box price. It wasn't. I also discovered that though I like the Energizer's design quite a bit, it's not as robust as I had hoped. And so, greedy for more power I bought the Black and Decker Electromate also.I bought both of these as emergency power sources for the next power outage. We had one this past year that lasted 8 days. No, even two of these dressed up car batteries won't help much for 8 days, but I figured that I can at least bring one to work with me every day for a recharge.I like this Black and Decker unit. It has a nice built in light for emergencies, jump start capablilities, a robust tire inflator, and plug ins for A/C and D/C. The D/C outlet should come in handy should I have to charge our Kindle Fire during an outage.We haven't had an outage yet, so I did a simulated outage to test this power source. It kept a floor lamp burning for about 8 hours, one with a fluorescent bulb but welcome light nonetheless. Before running the light test, I played with the Electromate for about an hour. It wouldn't run my electric stapler/nailer. It did power an electric drill, a television, a DVR player, a radio, and a coffee maker. Not all at the same time, of course.The inflator in the unit is excellent. The air pressure gauge is small but readable with its light and accurate. I checked it with a hand held pressure gauge.The Electromate is heavy but not prohibitively so; it's certainly not a strain to carry it about.Overall this is a nice unit, with easily understood controls. The jump start cables aren't attached to the unit, not a problem for me. Extra inflator nozzles are included but aren't attached to the unit as they are on the Energizer.One feature this unit has that is not on the Energizer and some others is the vehicle alternator voltage tester.When the weather gets nasty this winter, I plan on carrying the Black and Decker or the Energizer with me just in case.UPDATE - I've noticed that this unit doesn't hold a charge nearly as long as the Energizer. The charge on a sitting Black and Decker fades after a week or two. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.116381123662\n",
      "Neg prob value 0.883618831635\n",
      "review length 26\n",
      "this has a wide range of application than a sound deadener. one side is peel and stick. the material looks like asphalt but not that melting. \n",
      "\n",
      "Pos probabilities between 0.7 and 0.8 491\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.223284751177\n",
      "Neg prob value 0.776715278625\n",
      "review length 112\n",
      "I got this one after I'd returned a different one I got from Bean Garage, which was designed too low to rest an elbow on.  This one works, but I wish it had a more stable lid hinge (or something like a latch or pin) to keep the lid from sliding sideways when you move your arm on it.  It would also be better if it were designed so that you could lift it up to put your seatbelt on (maybe a second hinge about halfway down?).  When you go to put your seatbelt on, it's pretty cramped between one's hip and the storage box (and I'm not a real big person). \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.274650275707\n",
      "Neg prob value 0.725349724293\n",
      "review length 35\n",
      "The bull ring fit nicely. I had to trim the rail cap on the truck bed to allow the bull ring to fit against the bed. The rail cap trimmed easily with a razor knife. \n",
      "\n",
      "Pos probabilities between 0.6 and 0.7 445\n",
      "Pos probabilities between 0.5 and 0.6 509\n",
      "Pos probabilities < 0.5 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_neg</th>\n",
       "      <th>f1_pos</th>\n",
       "      <th>f1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.932436</td>\n",
       "      <td>0.953947</td>\n",
       "      <td>0.728258</td>\n",
       "      <td>0.961422</td>\n",
       "      <td>0.84484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.938514</td>\n",
       "      <td>0.960275</td>\n",
       "      <td>0.770849</td>\n",
       "      <td>0.964493</td>\n",
       "      <td>0.867671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.939181</td>\n",
       "      <td>0.961856</td>\n",
       "      <td>0.772282</td>\n",
       "      <td>0.964904</td>\n",
       "      <td>0.868593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.939662</td>\n",
       "      <td>0.961098</td>\n",
       "      <td>0.781155</td>\n",
       "      <td>0.965007</td>\n",
       "      <td>0.873081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.939975</td>\n",
       "      <td>0.959348</td>\n",
       "      <td>0.77702</td>\n",
       "      <td>0.96532</td>\n",
       "      <td>0.87117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.940015</td>\n",
       "      <td>0.960706</td>\n",
       "      <td>0.782653</td>\n",
       "      <td>0.965206</td>\n",
       "      <td>0.87393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.938848</td>\n",
       "      <td>0.960846</td>\n",
       "      <td>0.784177</td>\n",
       "      <td>0.964377</td>\n",
       "      <td>0.874277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.940335</td>\n",
       "      <td>0.95869</td>\n",
       "      <td>0.781968</td>\n",
       "      <td>0.965439</td>\n",
       "      <td>0.873704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      size       acc       auc    f1_neg    f1_pos    f1_avg\n",
       "0   150000  0.932436  0.953947  0.728258  0.961422   0.84484\n",
       "2   150000  0.938514  0.960275  0.770849  0.964493  0.867671\n",
       "4   150000  0.939181  0.961856  0.772282  0.964904  0.868593\n",
       "6   150000  0.939662  0.961098  0.781155  0.965007  0.873081\n",
       "8   150000  0.939975  0.959348   0.77702   0.96532   0.87117\n",
       "10  150000  0.940015  0.960706  0.782653  0.965206   0.87393\n",
       "12  150000  0.938848  0.960846  0.784177  0.964377  0.874277\n",
       "14  150000  0.940335   0.95869  0.781968  0.965439  0.873704"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Active transfer learning : Continue training with adding selected samples from target domain\n",
    "#In this cell, samples where we have the least absolute difference in predicted probability of positive and negative class are added first.\n",
    "\n",
    "num_epochs = 15\n",
    "size_model = size_initial\n",
    "src_key = s_key\n",
    "tgt_key = t_key\n",
    "\n",
    "#Create a sorted version of the certainty, and correspondingly sorted target train set ids, and labels.\n",
    "sort_ids = np.argsort(u_train_target_abs)\n",
    "certainty_sorted = u_train_target_abs[sort_ids]\n",
    "#print(sort_ids)\n",
    "df_target_ids_pre = dict_transfer_train_ids[tgt_key][src_key]\n",
    "df_target_labels_pre = dict_train_y[tgt_key]\n",
    "print('Target labels pre sort',df_target_labels_pre[-20:])\n",
    "print(type(df_target_labels_pre))\n",
    "#df_target_ids_pre = df_target_ids_pre.iloc([sort_ids])\n",
    "df_target_ids = df_target_ids_pre[sort_ids]\n",
    "df_target_labels = df_target_labels_pre[sort_ids]\n",
    "print('\\n Target labels post sort',df_target_labels[-20:])\n",
    "print('\\n Certainty sorted','\\n First 20',certainty_sorted[:20],'\\n Last 20',certainty_sorted[-20:])\n",
    "results_least_certain = pd.DataFrame()\n",
    "\n",
    "\n",
    "print('\\nTraining on least certain first')\n",
    "size_list = size_list\n",
    "for size in size_list:\n",
    "    avg_certainty = np.average(certainty_sorted[:size])\n",
    "    print('Training on target sample of size:',size,'with average certainty %0.3f'%avg_certainty)\n",
    "    tgt_train_df = df_target_ids[:size]\n",
    "    tgt_train_y = df_target_labels[:size]\n",
    "    avg_certainty = np.average(certainty_sorted[:size])\n",
    "    print(tgt_train_df.shape,tgt_train_y.shape)\n",
    "    results = continue_transfer_train(src_key,size_model,tgt_key,tgt_train_df,tgt_train_y)\n",
    "    results['size'] = size\n",
    "    results_least_certain = pd.concat([results_least_certain,results])\n",
    "    \n",
    "results_least_certain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on most certain first\n",
      "Training on target sample of size: 150000 with average certainty 0.999\n",
      "(150000, 150) (150000,)\n",
      "/newvolume/project_new/runs/vid/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/project_new/runs/vid/aut/size_500000/checkpoints/vidaut_model\n",
      " Model loaded from: /newvolume/project_new/runs/vid/aut/size_500000/checkpoints/vidaut_model\n",
      "# batches = 1171\n",
      "Train epoch 0, average loss 0.0467543, average accuracy 0.992868,\n",
      "\t\t aut Dev epoch 0, average loss 0.231,average accuracy 0.908,auc 0.929,f1_pos 0.946,f1_neg 0.666,f1_avg 0.806\n",
      "\t\t\t\t    Time taken for 0 epochs =  60.67856693267822\n",
      "Train epoch 2, average loss 0.0162435, average accuracy 0.99567,\n",
      "\t\t aut Dev epoch 2, average loss 0.225,average accuracy 0.915,auc 0.931,f1_pos 0.952,f1_neg 0.652,f1_avg 0.802\n",
      "Train epoch 4, average loss 0.00817159, average accuracy 0.997765,\n",
      "\t\t aut Dev epoch 4, average loss 0.220,average accuracy 0.919,auc 0.933,f1_pos 0.953,f1_neg 0.688,f1_avg 0.821\n",
      "\t\t\t\t    Time taken for 4 epochs =  277.7446391582489\n",
      "Train epoch 6, average loss 0.00524528, average accuracy 0.998606,\n",
      "\t\t aut Dev epoch 6, average loss 0.222,average accuracy 0.919,auc 0.934,f1_pos 0.953,f1_neg 0.693,f1_avg 0.823\n",
      "Train epoch 8, average loss 0.00387172, average accuracy 0.998959,\n",
      "\t\t aut Dev epoch 8, average loss 0.225,average accuracy 0.918,auc 0.935,f1_pos 0.953,f1_neg 0.700,f1_avg 0.826\n",
      "\t\t\t\t    Time taken for 8 epochs =  494.72181725502014\n",
      "Train epoch 10, average loss 0.00285688, average accuracy 0.999293,\n",
      "\t\t aut Dev epoch 10, average loss 0.228,average accuracy 0.917,auc 0.935,f1_pos 0.952,f1_neg 0.701,f1_avg 0.827\n",
      "Train epoch 12, average loss 0.00204838, average accuracy 0.999573,\n",
      "\t\t aut Dev epoch 12, average loss 0.230,average accuracy 0.920,auc 0.935,f1_pos 0.954,f1_neg 0.703,f1_avg 0.829\n",
      "\t\t\t\t    Time taken for 12 epochs =  711.7797455787659\n",
      "Train epoch 14, average loss 0.00159785, average accuracy 0.99974,\n",
      "\t\t aut Dev epoch 14, average loss 0.231,average accuracy 0.920,auc 0.936,f1_pos 0.954,f1_neg 0.705,f1_avg 0.829\n",
      "ERROR ANALYSIS\n",
      "149888\n",
      "src_key vid tar_key aut\n",
      "True negatives\n",
      "Values in no_err_neg_probas 14333\n",
      "Correct neg probabilities > 0.9 9797\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.00152370368596\n",
      "Neg prob value 0.998476326466\n",
      "review length 50\n",
      "I'm all for the 2nd amendment. Loved what this said, but it NEVER DELIVERED! Its been months now since it said it shipped, and still nothing! I don't know what else to add to it, giving it even 1 star is too much seeing as i don't even have it. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.0700381472707\n",
      "Neg prob value 0.929961800575\n",
      "review length 43\n",
      "Good thing it was not the bulb that requires me to remove the battery to get to. I called the manufacturer and there is no warranty. I would have expected at least a year. Some manufacturers give their bulbs a 2 year warranty. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 4.06901926908e-05\n",
      "Neg prob value 0.999959349632\n",
      "review length 40\n",
      "I bought 3 as Christmas gifts for my family. What a total waste of money!  After 10 minutes of being plugged in, the stupid thing couldn't even have melted a light frost!  We threw all of them in the trash! \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.00139048567507\n",
      "Neg prob value 0.998609542847\n",
      "review length 80\n",
      "This polish is not recommended for glasses but I got it anyhow to try and get some scratches out of my prescription sunglasses. It did not work. However the manufacturer never claimed it would work on glasses, may even have said not to use it on glasses so this rating isn't fair for what the product was meant for. I'm going to use it on my car headlights and see if it cleans them up.But don't buy for glasses. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.00520911300555\n",
      "Neg prob value 0.994790911674\n",
      "review length 70\n",
      "This is the first Dorman product that the product selector did not work for me.  The product that I received was for a 2 door coupe instead of the 4 door sedan.  Since i was not able to install it until today (weather permitting), I can not return it because its past some date.  Makes no sense. Wont be ordering car parts from amazon anymore, I'll just use Advance Auto. \n",
      "\n",
      "Correct neg probabilities between 0.8 and 0.9 1637\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.163295075297\n",
      "Neg prob value 0.836704969406\n",
      "review length 99\n",
      "Usually batteries that have heads similar to this battery come with alternate connection options. This battery ships for connections via the front/top only.The Suzuki VS1400's battery case is set up for side-mount connections. It might be possible to install with an adapter, additional parts, or a bit of fudging things, but this battery is not an exact fit for the bike it's advertised for.Worse, Buy4Easy & T-Motorsports refused to accept responsibly for their mistake. Amazon had to intervene and cover the shipping, as the vendor demanded I pay for the return shipping, even though it was their listing error. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.139447197318\n",
      "Neg prob value 0.860552728176\n",
      "review length 68\n",
      "Agree with other reviewer, the casting was crude and full of flash. The housing required major clean-up before install. Cap was the worst with burrs, gouges and sharp edges. Would not seal. O Ring is a joke. Had to resort to old cap for now, not sure if this cap can be fixed. Be warned, not all aluminum parts are the same. Will try Welsh Enterprises next time. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.103988751769\n",
      "Neg prob value 0.896011292934\n",
      "review length 130\n",
      "I received this i-tec 6 in 1 portable for Christmas.  It seemed well and I have used it a few times, since then, for some light duty work.  Last week, I had it on the AC charger and asked a friend to unplug it.  He unplugged the wall charger, but not the plug on the 6 in 1.  I came back a few days later and it was deader than a door nail.  I tried bring it back up, but it doesn't seem to take a charge at all.  The charging transformer gets warm and I can get three weak lights out of the LED work light.  Nothing on the voltmeter.There is no mention, in the destructions, about replacing the battery.  I can't believe it gave up the ghost that easily. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.103359401226\n",
      "Neg prob value 0.896640598774\n",
      "review length 25\n",
      "I drive a 2011 mustang and unfortunately this did not fit in the DC outlet. It wasn't long enough and the body was too wide. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.113455653191\n",
      "Neg prob value 0.886544406414\n",
      "review length 150\n",
      "I needed them for my back bumper because people do not know how to park in New York. They keep hitting the rear with those stupid giant SUV. If you don't know your distance between my car and yours get some glasses. I followed the instructions. Since there were four and I only need two I thought that if it doesn't work the first time I has spares that I can use and get other person to place them on the rear. What a mistake. I washed my car and did't use any wax, place the protectors on and a few hours later returned back to my car to see them on the ground. Okay maybe I'm not the right person for this easy task. I went to the Nissan dealership and had them apply them. It seemed to work until after I returned home and noticed that one of them fell off during the drive home (ten minutes), Geez!! also they were nothing like the picture. They had some silver along the rim and some kind of reflection paint on them. \n",
      "\n",
      "Correct neg probabilities between 0.7 and 0.8 1107\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.286579698324\n",
      "Neg prob value 0.713420331478\n",
      "review length 39\n",
      "Usually batteries that have heads similar to this battery come with alternate connection options. This battery ships for connections via the front/top only.The Suzuki VS1400's battery case is set up for side-mount connections. It might be possible to install with an adapter, additional parts, or a bit of fudging things, but this battery is not an exact fit for the bike it's advertised for.Worse, Buy4Easy & T-Motorsports refused to accept responsibly for their mistake. Amazon had to intervene and cover the shipping, as the vendor demanded I pay for the return shipping, even though it was their listing error. \n",
      "\n",
      "actual y 0.0 Pred y 0.0\n",
      "Pos prob value 0.269466102123\n",
      "Neg prob value 0.730533957481\n",
      "review length 32\n",
      "Agree with other reviewer, the casting was crude and full of flash. The housing required major clean-up before install. Cap was the worst with burrs, gouges and sharp edges. Would not seal. O Ring is a joke. Had to resort to old cap for now, not sure if this cap can be fixed. Be warned, not all aluminum parts are the same. Will try Welsh Enterprises next time. \n",
      "\n",
      "Correct neg probabilities between 0.6 and 0.7 971\n",
      "Correct neg probabilities between 0.5 and 0.6 821\n",
      "Correct neg probabilities < 0.5 0\n",
      "\n",
      "\n",
      "False positives\n",
      "Values in neg_err_pos_probas 7441\n",
      "Pos probabilities > 0.9 3446\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.978277921677\n",
      "Neg prob value 0.0217221211642\n",
      "review length 149\n",
      "This armrest is ok, but I feel it could be better.  It's better than nothing, but it was a gift to my dad and I was a little disappointed.  I have 2 main problems with it: 1. It's just too soft.  That sounds weird to say, but it really is just too soft.  I wish they had used a higher-density foam inside.  2. I also wish it sat farther forward.  I'm about 5'10\" and my dad is about the same and when the seat is adjusted to a normal and comfortable position for our size, the pad is so far back between the seats that our elbows only make contact with the front edge of the armrest.  I'm considering cutting the bracket and rewelding it farther forward, about flush with the front of the top of the center console, so our elbows have something to sit on.  Honestly, for the money, I'd look at other armrests out there. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.994436979294\n",
      "Neg prob value 0.0055629722774\n",
      "review length 18\n",
      "I should have gotten an invoice with the k40 k-30 c b antenna so I can a get a replactment. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.983480155468\n",
      "Neg prob value 0.0165198389441\n",
      "review length 59\n",
      "I BOUGHT THESE FOR MY 2003 SL500 TO HELP WITH BRAKE DUST PROBLEM AS I KEEP IT IMMACULANT AS IT ONLY HAS 28K MILES,BUT THESE THING INSTALLED WITH NEW ROTORS AND BROKE IN/ SEATED PERFECTLY ARE THE SUEALINGEST THINGS EVER. DO NOT BUY UNLESS YOU WANT CONSTANT SQUEALING. WILL BE BUYING FACTORY AGAIN AND WASTING A LOT OF MONEY. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.98112398386\n",
      "Neg prob value 0.0188760571182\n",
      "review length 54\n",
      "I own a custom shop the smithbilt bumper are not easy to install .It was a 2 hour job by professionals installers getting it to fit .Thank god we had three people a two post lift and a sledge hammer to get it over the rear frame .As far as looks go they look great . \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.954600274563\n",
      "Neg prob value 0.0453997589648\n",
      "review length 26\n",
      "great lighting, not as blue as they claim.Sylvania Ultra would be the better choice. will never purchase these againnot worth the money at allonly last 5 months \n",
      "\n",
      "Pos probabilities between 0.8 and 0.9 1278\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.886128127575\n",
      "Neg prob value 0.11387193948\n",
      "review length 149\n",
      "I had one of these in my workshed for a couple of years and finally decided to try it out. It didn't work, wouldn't get cold or warm. I figured it was just broken.  Found one at a yard sale for $20 so I got it as I was making a trip and wanted something cold to drink.  It didn't work either. Figured I got burnt for the 20. Found one on Amazon and decided to try it. I put ice cold soda in it from the fridge, put it in the car and started on a 300 mile trip. I made sure the unit was turned on and running. After about 150 miles, I decided I wanted to drink. Has anyone ever drank warm Mountain Dew??  The cans were cold when they went in but warm when I stopped to have one. Yes the unit was on the cold mode. Given a choice between buying a Yugo and one of these, I would go for the Yugo. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.815626382828\n",
      "Neg prob value 0.184373572469\n",
      "review length 63\n",
      "I'm not very impressed by this. The handle itself is nice but the thread-inserts are made of low quaility metal. They stripped out quickly and now my handle won't tighten down. I eventually plan on filling the metal, drilling out a hole and rethreading it for my specific application.Between this and a handle made for a specific application, I'd go for the later. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.858605444431\n",
      "Neg prob value 0.141394540668\n",
      "review length 43\n",
      "I thought this broom will be softer and make it easier to clean, but the brush is actually hard and not as flexible as I expected it to be. The straws started to drop out at the first time when I used it. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.843010246754\n",
      "Neg prob value 0.156989783049\n",
      "review length 29\n",
      "Looks like the rotor for the HITACHI distributor on an accord, but was made for a bigger distributor shaft - they should have a vin search function or something. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.877868533134\n",
      "Neg prob value 0.122131451964\n",
      "review length 38\n",
      "I BELIEVE IT'S A KNOCK OFF.  NOT BRANDED, NOT THE SAME AS THE ONE PICTURED, NO CENTERED BOLT IN FRONT OF SWITCH AND DOES NOT HAVE FLAMING RIVER DECAL.  PROBABLY MADE IN CHINA, A $5.00 SWITCH.  WASTED $35.00. \n",
      "\n",
      "Pos probabilities between 0.7 and 0.8 993\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.726529359818\n",
      "Neg prob value 0.273470699787\n",
      "review length 61\n",
      "I ran into a trailer hitch with my Honda, and decided that I needed a band-aid.. I was sorely disappointed when I realized that this sticker is waaay smaller than your average bumper sticker.  I seriously could have bought a box of real large band-aids and gotten the same effect (with band-aids to spare for human injuries) for the same price.. \n",
      "\n",
      "actual y 0.0 Pred y 1.0\n",
      "Pos prob value 0.754962742329\n",
      "Neg prob value 0.24503724277\n",
      "review length 24\n",
      "the ASIN clearly lists the SKU as a the manufacturer part number for a 6 pack but amazon sent me 1, its going back. \n",
      "\n",
      "Pos probabilities between 0.6 and 0.7 889\n",
      "Pos probabilities between 0.5 and 0.6 835\n",
      "Pos probabilities < 0.5 0\n",
      "\n",
      "\n",
      "True positives\n",
      "Values in no_err_pos_probas 123559\n",
      "Correct positivr probabilities > 0.9 112902\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.999974131584\n",
      "Neg prob value 2.58695527009e-05\n",
      "review length 20\n",
      "These LED bulbs are very bright white. Fit great easy to install and replace existing bulbs. I highly recommend them. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.999980330467\n",
      "Neg prob value 1.96707314899e-05\n",
      "review length 48\n",
      "This is a great item!  Anyone who drives my car laughs until they hit the gas.  Then they say it should be in yellow as a caution sign, because my car is fast.The lettering and logo could be a little cleaner, but is cool as it is! \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.998626470566\n",
      "Neg prob value 0.00137351534795\n",
      "review length 150\n",
      "Couldn't live without a Cruise Control in my 1997 Toyota Corolla which I just bought! Rostra offered left and right column mount controls as well as dash mount units. I would definitely suggest the column mount. I picked the right side mounted unit as I only had the windshield washer stalk on that side. Left side already had the stalks for the lights and wheel tilt and I thought it might be a little crowded. Turns out after installing it that it easily would have fit on the left (different part number). Depending on your preference or experience with other vehicles, either side would have worked fine. Dropping off the bottom of the steering column was easy on my vehicle and then it was simple to drill the hole to mount it through. You will want to be certain that you check for clearance on the column itself for the stub of the stalk and the wiring. You will also want to be sure that if your other column mounted items move, that they don't move into the location of the new cruise control switch. Otherwise, after finding a good location for installing it and then the hook-up to the actual cruise control unit was a breeze with just a simple pig tail to plug in. The &#34;touch&#34; to set the controls on this control switch is very &#34;soft.&#34; I sometimes brush by it when tuning the radio and disengage it. The only reason I gave this a 4 instead of 5 star is because this comes with a green LED light on it and it is so pale that during daylight hours it is next to impossible to tell if it is on or not. This really should be improved. I spend time looking away from the road and attempting to shield around the light to see if it is indeed on or not. There's no good reason for this to not be a more readily visible light. It shows fine after it is rather dark. After about 2000 miles of mixed driving I'm happy with the unit. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.996357858181\n",
      "Neg prob value 0.00364216440357\n",
      "review length 45\n",
      "Surprisingly for the cost, these actually do a great job of keeping the wind out of my eyes.  By no means are they &#34;High quality&#34;, but what a bargain!  Would highly recommend if you are looking for an inexpensive option while you ride. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.999975562096\n",
      "Neg prob value 2.44328548433e-05\n",
      "review length 21\n",
      "very nice helmet. as good as fox or thor. perfect fit and light weight.and you just cant beat the price. \n",
      "\n",
      "Correct pos probabilities between 0.8 and 0.9 5276\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.898370862007\n",
      "Neg prob value 0.101629190147\n",
      "review length 103\n",
      "The picture Amazon shows is not accurate.  The fan does not have the \"legs\" on the bottom extending toward the rear and the controls are on the side, not the front.  The picture is close otherwise.  I've read earlier reviews claiming the fan did not put out much of a \"breeze\".  Mine puts out a nice breeze, esp. in the high output setting.  I have not tried it with 8 D cell batteries, but it works fine with a power adapter hooked into a 110V outlet.  Without the \"legs\", it's not very stable and seems to be designed to hang from it's handle. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.850544810295\n",
      "Neg prob value 0.149455174804\n",
      "review length 30\n",
      "Easy to install.  Way more adjustable then the stock  mirrors.  They look good.  They are smaller mirror area then the stock mirrors, but I knew that when I ordered them. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.870471715927\n",
      "Neg prob value 0.129528239369\n",
      "review length 92\n",
      "My 1998 F150 (with 270k+ miles...)barely ran and I knew it needed at least one ignition coil.  I decided it would be most cost-effective to use this kit to replace all of them.It runs like a top now and I shouldn't have to replace any for a while...if ever.Never having done these before, it took me from 6:30am to 1:00 pm to complete, but I did not have to remove the fuel rail as some manuals will indicate.  I found an online post with more info. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.821591854095\n",
      "Neg prob value 0.178408160806\n",
      "review length 150\n",
      "This thing is worth every dollar, and is an even better value here on Amazon.Technical Detail#1: This went into my DODGE DAKOTA 2000, it has a 4.7 liter V8 and 4x4 with EcoBoost (all amounts given from here on out will be during the use of 4X2 not 4x4) This product has improved my fuel economy, slightly increased my horsepower, and greatly improved my vehicles response to acceleration.The Dakota has an estimated 19mpg Highway out of factory, mine has bigger, wider tires and a slight lift (meaning less MPG) After installing this product I now rock 21 MPG highway (at around 60 miles per hour) for a 14 year old truck, some new gen trucks have barely 22 or 23 mpg out of factory. Oddly enough, my City MPG improved more than my Highway, I now push 24-26 MPG city doing about 40 miles per hour on a level street. I clean the filter with K&N; Air Filter Cleaner and Degreaser every oil change to maintain it.Technical Detail #2: The trim for the heat shield is all 1 piece that you will have to cut to fit the sides, it has a metal center that grips the edge of the heat shield but if you have plastic cutting scissors or pliers you should have no problems.Technical Detail #3: it has detailed instructions and it can be installed by ameteurs (like me), though it will require you to use a factory hose that you must cut in half for re-use and you will use the factory screws for your vehicle's air resonator, and more specifically, the air filter box. Do yourself a favor, get a new hose (i got mine for $12 dollars at a local part store) or DONT cut it exactly in half. Give the half you will keep more length to work with, and if it is tough and old like mine, boiling it in a pot for a short period of time (15 secs or till soft) will make it easier to work with.All that said, I will buy this brand of air intake for all my vehicles. I looked hard and long and I personally could not find a better value for the money. \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.852141857147\n",
      "Neg prob value 0.147858098149\n",
      "review length 25\n",
      "EZ to program and the Nissan dealer wanted $140 for a new one.  I paid $18 and programmed it myself and I'm an old lady. \n",
      "\n",
      "Correct pos probabilities between 0.7 and 0.8 2537\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.786564052105\n",
      "Neg prob value 0.213435932994\n",
      "review length 71\n",
      "WeatherTech! What more can you say? I had the run of he mill Bug Deflector. I got a dealand when I got it,I got what I paid for,can I say it \"cheap crap\" WeatherTech is quality.Not only can you see, you can feel it. I intend to buy more WeatherTech products soon.I know what i'm getting is \"GOOD OLD USA MADE\".............Need I say more? \n",
      "\n",
      "actual y 1.0 Pred y 1.0\n",
      "Pos prob value 0.751511156559\n",
      "Neg prob value 0.248488798738\n",
      "review length 150\n",
      "Well made with fit and function as expected. Curved lens are close to my eyes, so if that bugs you, you might want to check out another style. Once moving the vents allow just enough air flow to prevent fogging but without tearing me up. I need prescription lens and my local place admitted they couldn't do it for what Bobster charges (~ $90). I sent them in to SoCa and the fun began. Bobster customer service is an answering machine. It takes at least three calls and/or e-mails to get a reply. Sometimes they call, sometimes an e-mail. Maybe.After several hoops and delays on their end, they were &#34;rushed&#34; ordered to their eye guys. After two months, I got them back the same day I receive a message telling me to mail them back because they had switched mine with another customer's. Back they go the next day and then they e-mail me stating their was no mix up just keep them. Little late, Bobster.Last weekend was cold and windy for our Toys for Tots ride and they functioned very well. Comfortable even with the strap under the helmet; no fogging even with a face mask on. I would buy them again and recommend them to a friend.Good product, but poor to confusing customer service at Bobster. \n",
      "\n",
      "Correct pos probabilities between 0.6 and 0.7 1596\n",
      "Correct pos probabilities between 0.5 and 0.6 1248\n",
      "Correct pos probabilities < 0.5 0\n",
      "\n",
      "\n",
      "False negatives\n",
      "Values in pos_err_neg_probas 4555\n",
      "Neg probabilities > 0.9 1194\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0096104638651\n",
      "Neg prob value 0.99038952589\n",
      "review length 45\n",
      "Well made rotors. It looks like no matter how much you pay you get rotors of good quality made in China. So do not spend extra, get these rotors and spend your money on good brake pads (I only use original pads from a dealer) \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.000156507070642\n",
      "Neg prob value 0.999843478203\n",
      "review length 61\n",
      "stoner 91036 trim shine was not what i wanted it didnot work and i wish i could get a refund if i can it not anygood at all i think you should stop selling it to customer now tell me how can i get a refund asap thank you for the chance to to give me truth about this product it dosnt work \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0446020029485\n",
      "Neg prob value 0.955397963524\n",
      "review length 21\n",
      "ITEM LOOKS GOOD IN THE TRUCK BUT WOULD NOT BUY FOR A SHOW TRUCK EXCELLENT FOR EVERYDAY DRIVER HOPE THIS HELPS \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0759790763259\n",
      "Neg prob value 0.924020946026\n",
      "review length 149\n",
      "The Ultra Breeze cover for the Fan-tastic vents is exactly as their marketing materials would describe it. It's a large plastic cover that fits over your Fan-tastic vent and prevents the rain from coming into your vent. Several other reviewers complained about this product for reasons that seem invalid. I hate writing reviews for Amazon, but because their reviews were so poor and not factual, I felt compelled to write a redemptive review.I almost didn't buy this product because the reviews were so bad. Then I read the reviews on Camping World's site and realized it was still the product that I wanted. I bought it, installed it myself, and am supremely happy with it. It lets my Fan-tastic vent open several inches further than the cheaper competitive products.Here are some of the complaints written by other reviewers - and my rebuttal to their complaints:1. This cover does not have a bug screen - CORRECT. Because the Fan-tastic fan has one built into it. You don't need two bug screens.2. The grill is held on by Christmas Tree clips - FALSE. It's held on by pins, which are installed under your fan edge (and should then be sealed to your roof). The pins are metal and sturdy and should not break. The nice thing about this installation method is that it is easy to remove your fan cover to clean your fan and roof and then put the cover back on (without tools). This is great, because then when you get pine needles and junk under there it will be WAY easier to clean.3. The plastic mesh part in the front is loose - FALSE. The plastic part fits into the grooves provided and has plastic screws on the side to hold it in place. It will keep a tree branch out of your fan, or a bird. It would not survive a person kicking it. But who would be kicking things on your roof?!?4. There is not enough clearance for the bracket between the screw and the Fan-Tastic fan body - FALSE. The pins that the cover fits onto slide L/R. When you install it, you slide them to fit your cover into the right place. It was REALLY easy to install. I am not a handy person, and I installed 2 of these by myself in under an hour. And I didn't have any problem with the #10 screws. They went right into the holes I pulled the #8s out of. No problem.In summary, this is a great product that does exactly what it says it will do... which is to give your Fan-tastic brand vent MORE CLEARANCE to open further. I wish it was a little bit cheaper, but hey, I wish that about most things I buy ;) \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.0459392629564\n",
      "Neg prob value 0.954060792923\n",
      "review length 130\n",
      "Look, I know you have been messing with those horrible vacuum hubs on that Kia Sportage, and try as you might, they still will not work!! Just order these! They are the easiest things I have ever put on. Comes with detailed instructions with pictures! All parts come with the pair! For my 2000, unbolt 6 bolts, throw the flat washer on the bolts away and keep just the lock washers. Pull old ones off and put new ones on! IT IS THAT SIMPLE! On earlier ones (95-97) you have to replace a washer and a spacer(all provided!). That is it!!! Took 15 minutes and now I will never wonder if they work again! Unplu vacuum solenoid under hood and tape connection up and drive!! YOU WILL NOT REGRET THIS!!!! \n",
      "\n",
      "Neg probabilities between 0.8 and 0.9 789\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.185628384352\n",
      "Neg prob value 0.814371645451\n",
      "review length 45\n",
      "The lip that lies on the hood does not come in full contact so leaves and debris gather there..once on the highway they blow away but not a good feature. It even says in the manual it's not supposed to contact.  Looks great though. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.140740439296\n",
      "Neg prob value 0.859259545803\n",
      "review length 31\n",
      "Nice quality gloves.  However the Large size was tight on my hands, even tho I always take a large.  Seller quickly offered a return and I have ordered an XL pair. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.132817730308\n",
      "Neg prob value 0.867182254791\n",
      "review length 53\n",
      "Light where extremely bright.  I will post pics later.Tip - If the light does not work remove it turn it 180 degrees and re install.  If it still does not work slightly bend the prongs( contacts) at the base of the lights out this will help them make contact in the base. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.11510437727\n",
      "Neg prob value 0.88489562273\n",
      "review length 130\n",
      "The instructions that come with the replacement transmitter are terrible; it's as if they were intentionally written to frustrate whomever decided to buy them.  After fiddling around with it, I finally figured it out. Here is an easier way to do it.1. Open any one of your car doors.2. Turn the key to the on position; do not turn the engine over.3. Press the Valet button (located on your cars alarm system, not the one on the remote) 7 times .The Led light on the valet switch will flash seven times indicating the auto learn function.Next, press and hold the same button an 8th time.4. While still holding the valet button, press the open/close door button on the remote.5. You are all done. \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.19737829268\n",
      "Neg prob value 0.802621722221\n",
      "review length 81\n",
      "I have had OEM, Monroe Reflex and now the Sensa-Tracs on my 99 Mountaineer (Explorer).  The Reflexes handled better, but the ride was way too rough.  These Sensa-Tracs are a lot softer and feel more like an upscale SUV.  There was hardly any body roll with the Reflex, there is body roll with these.  Overall, I like these better as a daily driver shock.  I have not towed anything with these yet so I don't know how well the helper springs work. \n",
      "\n",
      "Pos probabilities between 0.7 and 0.8 767\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.237658292055\n",
      "Neg prob value 0.762341678143\n",
      "review length 112\n",
      "I got this one after I'd returned a different one I got from Bean Garage, which was designed too low to rest an elbow on.  This one works, but I wish it had a more stable lid hinge (or something like a latch or pin) to keep the lid from sliding sideways when you move your arm on it.  It would also be better if it were designed so that you could lift it up to put your seatbelt on (maybe a second hinge about halfway down?).  When you go to put your seatbelt on, it's pretty cramped between one's hip and the storage box (and I'm not a real big person). \n",
      "\n",
      "actual y 1.0 Pred y 0.0\n",
      "Pos prob value 0.221302732825\n",
      "Neg prob value 0.778697252274\n",
      "review length 48\n",
      "SUPER BRIGHT. I have other LEDs in the interior light of my car but when I turn this light on it blows all the other ones away. Wouldn't recommend turning on when driving!! Brightest dome/interior light I've had and the product does not feel cheap either! \n",
      "\n",
      "Pos probabilities between 0.6 and 0.7 816\n",
      "Pos probabilities between 0.5 and 0.6 989\n",
      "Pos probabilities < 0.5 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_neg</th>\n",
       "      <th>f1_pos</th>\n",
       "      <th>f1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.907658</td>\n",
       "      <td>0.928653</td>\n",
       "      <td>0.666361</td>\n",
       "      <td>0.946413</td>\n",
       "      <td>0.806387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.915003</td>\n",
       "      <td>0.930685</td>\n",
       "      <td>0.651855</td>\n",
       "      <td>0.951592</td>\n",
       "      <td>0.801724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.918506</td>\n",
       "      <td>0.933352</td>\n",
       "      <td>0.687987</td>\n",
       "      <td>0.953132</td>\n",
       "      <td>0.82056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.918786</td>\n",
       "      <td>0.934429</td>\n",
       "      <td>0.692578</td>\n",
       "      <td>0.953213</td>\n",
       "      <td>0.822895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.918139</td>\n",
       "      <td>0.934723</td>\n",
       "      <td>0.699941</td>\n",
       "      <td>0.952604</td>\n",
       "      <td>0.826273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.916898</td>\n",
       "      <td>0.93495</td>\n",
       "      <td>0.701338</td>\n",
       "      <td>0.951734</td>\n",
       "      <td>0.826536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.92014</td>\n",
       "      <td>0.93515</td>\n",
       "      <td>0.703478</td>\n",
       "      <td>0.953856</td>\n",
       "      <td>0.828667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.919967</td>\n",
       "      <td>0.936073</td>\n",
       "      <td>0.704983</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.829343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      size       acc       auc    f1_neg    f1_pos    f1_avg\n",
       "0   150000  0.907658  0.928653  0.666361  0.946413  0.806387\n",
       "2   150000  0.915003  0.930685  0.651855  0.951592  0.801724\n",
       "4   150000  0.918506  0.933352  0.687987  0.953132   0.82056\n",
       "6   150000  0.918786  0.934429  0.692578  0.953213  0.822895\n",
       "8   150000  0.918139  0.934723  0.699941  0.952604  0.826273\n",
       "10  150000  0.916898   0.93495  0.701338  0.951734  0.826536\n",
       "12  150000   0.92014   0.93515  0.703478  0.953856  0.828667\n",
       "14  150000  0.919967  0.936073  0.704983  0.953704  0.829343"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this cell, samples where we have the most absolute difference in predicted probability of positive and negative class are added first.\n",
    "size_model = size_initial\n",
    "print('Training on most certain first')\n",
    "size_list = size_list\n",
    "results_most_certain = pd.DataFrame()\n",
    "for size in size_list:\n",
    "    avg_certainty = np.average(certainty_sorted[-size:])\n",
    "    print('Training on target sample of size:',size,'with average certainty %0.3f'%avg_certainty)\n",
    "    tgt_train_df = df_target_ids[-size:]\n",
    "    tgt_train_y = df_target_labels[-size:]\n",
    "    print(tgt_train_df.shape,tgt_train_y.shape)\n",
    "    results = continue_transfer_train(src_key,size_model,tgt_key,tgt_train_df,tgt_train_y)\n",
    "    results['size'] = size\n",
    "    results_most_certain = pd.concat([results_most_certain,results])\n",
    "    \n",
    "results_most_certain\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target labels pre sort [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      " Target labels post sort [1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      " Certainty sorted \n",
      " First 20 [  2.93254852e-05   3.64184380e-05   2.38120556e-05   1.46538019e-04\n",
      "   1.17838383e-04   1.61826611e-04   3.12894583e-04   4.05132771e-04\n",
      "   4.24623489e-04   5.79833984e-04   5.28812408e-04   2.94387341e-04\n",
      "   1.44541264e-04   7.98702240e-05   2.49713659e-04   1.56641006e-04\n",
      "   7.81148672e-04   8.16792250e-04   4.39465046e-04   1.23530626e-04] \n",
      " Last 20 [ 0.59301013  0.59301013  0.59301013  0.59301013  0.59301013  0.59301013\n",
      "  0.59301013  0.59301013  0.59301013  0.59301013  0.59301013  0.59301013\n",
      "  0.59301013  0.59301013  0.59301013  0.59301013  0.59301013  0.59301013\n",
      "  0.59301013  0.59301013]\n",
      "\n",
      "Training on least certain first\n",
      "Training on target sample of size: 20000 with average certainty per word id 0.387551747995 with average certainty 0.250\n",
      "(20000, 150) (20000,)\n",
      "/newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      " Model loaded from: /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      "# batches = 156\n",
      "Train epoch 0, average loss 0.898939, average accuracy 0.624249,\n",
      "\t\t aut Dev epoch 0, average loss 0.211,average accuracy 0.922,auc 0.934,f1_pos 0.955,f1_neg 0.700,f1_avg 0.828\n",
      "\t\t\t\t    Time taken for 0 epochs =  18.952993154525757\n",
      "Train epoch 2, average loss 0.378793, average accuracy 0.825771,\n",
      "\t\t aut Dev epoch 2, average loss 0.195,average accuracy 0.928,auc 0.939,f1_pos 0.959,f1_neg 0.717,f1_avg 0.838\n",
      "Train epoch 4, average loss 0.251939, average accuracy 0.903095,\n",
      "\t\t aut Dev epoch 4, average loss 0.191,average accuracy 0.929,auc 0.945,f1_pos 0.959,f1_neg 0.739,f1_avg 0.849\n",
      "\t\t\t\t    Time taken for 4 epochs =  69.0117347240448\n",
      "Train epoch 6, average loss 0.174188, average accuracy 0.94406,\n",
      "\t\t aut Dev epoch 6, average loss 0.195,average accuracy 0.930,auc 0.945,f1_pos 0.960,f1_neg 0.729,f1_avg 0.845\n",
      "Train epoch 8, average loss 0.131378, average accuracy 0.962891,\n",
      "\t\t aut Dev epoch 8, average loss 0.202,average accuracy 0.930,auc 0.943,f1_pos 0.960,f1_neg 0.731,f1_avg 0.845\n",
      "\t\t\t\t    Time taken for 8 epochs =  119.0948896408081\n",
      "Train epoch 10, average loss 0.100341, average accuracy 0.97506,\n",
      "\t\t aut Dev epoch 10, average loss 0.203,average accuracy 0.931,auc 0.944,f1_pos 0.960,f1_neg 0.739,f1_avg 0.850\n",
      "Train epoch 12, average loss 0.0765842, average accuracy 0.984575,\n",
      "\t\t aut Dev epoch 12, average loss 0.211,average accuracy 0.930,auc 0.945,f1_pos 0.960,f1_neg 0.730,f1_avg 0.845\n",
      "\t\t\t\t    Time taken for 12 epochs =  169.18201184272766\n",
      "Train epoch 14, average loss 0.0629851, average accuracy 0.988231,\n",
      "\t\t aut Dev epoch 14, average loss 0.218,average accuracy 0.931,auc 0.943,f1_pos 0.960,f1_neg 0.735,f1_avg 0.847\n",
      "Training on target sample of size: 50000 with average certainty per word id 0.720856812326 with average certainty 0.610\n",
      "(50000, 150) (50000,)\n",
      "/newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      " Model loaded from: /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      "# batches = 390\n",
      "Train epoch 0, average loss 0.6199, average accuracy 0.740625,\n",
      "\t\t aut Dev epoch 0, average loss 0.224,average accuracy 0.923,auc 0.935,f1_pos 0.956,f1_neg 0.706,f1_avg 0.831\n",
      "\t\t\t\t    Time taken for 0 epochs =  28.327017068862915\n",
      "Train epoch 2, average loss 0.315364, average accuracy 0.860056,\n",
      "\t\t aut Dev epoch 2, average loss 0.193,average accuracy 0.930,auc 0.944,f1_pos 0.960,f1_neg 0.738,f1_avg 0.849\n",
      "Train epoch 4, average loss 0.224216, average accuracy 0.908554,\n",
      "\t\t aut Dev epoch 4, average loss 0.187,average accuracy 0.931,auc 0.947,f1_pos 0.960,f1_neg 0.747,f1_avg 0.854\n",
      "\t\t\t\t    Time taken for 4 epochs =  117.12881422042847\n",
      "Train epoch 6, average loss 0.164743, average accuracy 0.938301,\n",
      "\t\t aut Dev epoch 6, average loss 0.197,average accuracy 0.928,auc 0.946,f1_pos 0.958,f1_neg 0.750,f1_avg 0.854\n",
      "Train epoch 8, average loss 0.125456, average accuracy 0.95619,\n",
      "\t\t aut Dev epoch 8, average loss 0.193,average accuracy 0.930,auc 0.944,f1_pos 0.960,f1_neg 0.741,f1_avg 0.850\n",
      "\t\t\t\t    Time taken for 8 epochs =  205.9057698249817\n",
      "Train epoch 10, average loss 0.0967235, average accuracy 0.969812,\n",
      "\t\t aut Dev epoch 10, average loss 0.201,average accuracy 0.930,auc 0.943,f1_pos 0.960,f1_neg 0.737,f1_avg 0.848\n",
      "Train epoch 12, average loss 0.0782035, average accuracy 0.977003,\n",
      "\t\t aut Dev epoch 12, average loss 0.204,average accuracy 0.931,auc 0.943,f1_pos 0.960,f1_neg 0.746,f1_avg 0.853\n",
      "\t\t\t\t    Time taken for 12 epochs =  294.79552125930786\n",
      "Train epoch 14, average loss 0.064434, average accuracy 0.982252,\n",
      "\t\t aut Dev epoch 14, average loss 0.214,average accuracy 0.930,auc 0.940,f1_pos 0.960,f1_neg 0.736,f1_avg 0.848\n",
      "Training on target sample of size: 100000 with average certainty per word id 0.999097881564 with average certainty 0.743\n",
      "(100000, 150) (100000,)\n",
      "/newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      " Model loaded from: /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      "# batches = 781\n",
      "Train epoch 0, average loss 0.450847, average accuracy 0.81195,\n",
      "\t\t aut Dev epoch 0, average loss 0.197,average accuracy 0.925,auc 0.943,f1_pos 0.957,f1_neg 0.693,f1_avg 0.825\n",
      "\t\t\t\t    Time taken for 0 epochs =  44.43828892707825\n",
      "Train epoch 2, average loss 0.254894, average accuracy 0.887824,\n",
      "\t\t aut Dev epoch 2, average loss 0.182,average accuracy 0.934,auc 0.951,f1_pos 0.962,f1_neg 0.754,f1_avg 0.858\n",
      "Train epoch 4, average loss 0.189969, average accuracy 0.921495,\n",
      "\t\t aut Dev epoch 4, average loss 0.182,average accuracy 0.933,auc 0.949,f1_pos 0.961,f1_neg 0.732,f1_avg 0.847\n",
      "\t\t\t\t    Time taken for 4 epochs =  197.71836733818054\n",
      "Train epoch 6, average loss 0.147094, average accuracy 0.941971,\n",
      "\t\t aut Dev epoch 6, average loss 0.181,average accuracy 0.934,auc 0.950,f1_pos 0.962,f1_neg 0.747,f1_avg 0.854\n",
      "Train epoch 8, average loss 0.114083, average accuracy 0.957256,\n",
      "\t\t aut Dev epoch 8, average loss 0.188,average accuracy 0.932,auc 0.952,f1_pos 0.961,f1_neg 0.764,f1_avg 0.862\n",
      "\t\t\t\t    Time taken for 8 epochs =  351.11082339286804\n",
      "Train epoch 10, average loss 0.0913507, average accuracy 0.966859,\n",
      "\t\t aut Dev epoch 10, average loss 0.194,average accuracy 0.934,auc 0.949,f1_pos 0.962,f1_neg 0.754,f1_avg 0.858\n",
      "Train epoch 12, average loss 0.0761126, average accuracy 0.974222,\n",
      "\t\t aut Dev epoch 12, average loss 0.199,average accuracy 0.933,auc 0.951,f1_pos 0.961,f1_neg 0.759,f1_avg 0.860\n",
      "\t\t\t\t    Time taken for 12 epochs =  504.37131428718567\n",
      "Train epoch 14, average loss 0.0627394, average accuracy 0.979203,\n",
      "\t\t aut Dev epoch 14, average loss 0.208,average accuracy 0.931,auc 0.951,f1_pos 0.960,f1_neg 0.761,f1_avg 0.860\n",
      "Training on target sample of size: 150000 with average certainty per word id 1.29461965807 with average certainty 0.797\n",
      "(150000, 150) (150000,)\n",
      "/newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      " Model loaded from: /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      "# batches = 1171\n",
      "Train epoch 0, average loss 0.37455, average accuracy 0.844944,\n",
      "\t\t aut Dev epoch 0, average loss 0.185,average accuracy 0.932,auc 0.949,f1_pos 0.961,f1_neg 0.743,f1_avg 0.852\n",
      "\t\t\t\t    Time taken for 0 epochs =  60.29289412498474\n",
      "Train epoch 2, average loss 0.224476, average accuracy 0.904822,\n",
      "\t\t aut Dev epoch 2, average loss 0.171,average accuracy 0.936,auc 0.956,f1_pos 0.963,f1_neg 0.761,f1_avg 0.862\n",
      "Train epoch 4, average loss 0.172962, average accuracy 0.92862,\n",
      "\t\t aut Dev epoch 4, average loss 0.179,average accuracy 0.935,auc 0.952,f1_pos 0.963,f1_neg 0.742,f1_avg 0.852\n",
      "\t\t\t\t    Time taken for 4 epochs =  277.04795932769775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 6, average loss 0.137506, average accuracy 0.944579,\n",
      "\t\t aut Dev epoch 6, average loss 0.174,average accuracy 0.937,auc 0.956,f1_pos 0.963,f1_neg 0.765,f1_avg 0.864\n",
      "Train epoch 8, average loss 0.111807, average accuracy 0.956221,\n",
      "\t\t aut Dev epoch 8, average loss 0.183,average accuracy 0.936,auc 0.953,f1_pos 0.963,f1_neg 0.759,f1_avg 0.861\n",
      "\t\t\t\t    Time taken for 8 epochs =  493.8971607685089\n",
      "Train epoch 10, average loss 0.0905362, average accuracy 0.966141,\n",
      "\t\t aut Dev epoch 10, average loss 0.187,average accuracy 0.937,auc 0.956,f1_pos 0.963,f1_neg 0.774,f1_avg 0.869\n",
      "Train epoch 12, average loss 0.0761937, average accuracy 0.972066,\n",
      "\t\t aut Dev epoch 12, average loss 0.195,average accuracy 0.936,auc 0.954,f1_pos 0.963,f1_neg 0.768,f1_avg 0.865\n",
      "\t\t\t\t    Time taken for 12 epochs =  710.7086460590363\n",
      "Train epoch 14, average loss 0.0646022, average accuracy 0.976903,\n",
      "\t\t aut Dev epoch 14, average loss 0.204,average accuracy 0.936,auc 0.953,f1_pos 0.963,f1_neg 0.768,f1_avg 0.865\n",
      "Training on target sample of size: 200000 with average certainty per word id 1.60249073767 with average certainty 0.828\n",
      "(200000, 150) (200000,)\n",
      "/newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      " Model loaded from: /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      "# batches = 1562\n",
      "Train epoch 0, average loss 0.331438, average accuracy 0.864377,\n",
      "\t\t aut Dev epoch 0, average loss 0.178,average accuracy 0.932,auc 0.953,f1_pos 0.961,f1_neg 0.729,f1_avg 0.845\n",
      "\t\t\t\t    Time taken for 0 epochs =  76.23231506347656\n",
      "Train epoch 2, average loss 0.207349, average accuracy 0.912652,\n",
      "\t\t aut Dev epoch 2, average loss 0.166,average accuracy 0.937,auc 0.958,f1_pos 0.964,f1_neg 0.758,f1_avg 0.861\n",
      "Train epoch 4, average loss 0.162504, average accuracy 0.933019,\n",
      "\t\t aut Dev epoch 4, average loss 0.167,average accuracy 0.938,auc 0.959,f1_pos 0.964,f1_neg 0.779,f1_avg 0.871\n",
      "\t\t\t\t    Time taken for 4 epochs =  356.4174427986145\n",
      "Train epoch 6, average loss 0.133371, average accuracy 0.946653,\n",
      "\t\t aut Dev epoch 6, average loss 0.171,average accuracy 0.938,auc 0.958,f1_pos 0.964,f1_neg 0.776,f1_avg 0.870\n",
      "Train epoch 8, average loss 0.108248, average accuracy 0.957661,\n",
      "\t\t aut Dev epoch 8, average loss 0.178,average accuracy 0.938,auc 0.957,f1_pos 0.964,f1_neg 0.775,f1_avg 0.870\n",
      "\t\t\t\t    Time taken for 8 epochs =  636.6096963882446\n",
      "Train epoch 10, average loss 0.091937, average accuracy 0.964334,\n",
      "\t\t aut Dev epoch 10, average loss 0.184,average accuracy 0.937,auc 0.957,f1_pos 0.964,f1_neg 0.775,f1_avg 0.869\n",
      "Train epoch 12, average loss 0.0777089, average accuracy 0.970566,\n",
      "\t\t aut Dev epoch 12, average loss 0.204,average accuracy 0.937,auc 0.952,f1_pos 0.964,f1_neg 0.759,f1_avg 0.862\n",
      "\t\t\t\t    Time taken for 12 epochs =  916.8472599983215\n",
      "Train epoch 14, average loss 0.0676991, average accuracy 0.975002,\n",
      "\t\t aut Dev epoch 14, average loss 0.205,average accuracy 0.938,auc 0.955,f1_pos 0.964,f1_neg 0.769,f1_avg 0.866\n",
      "Training on target sample of size: 300000 with average certainty per word id 2.28118252703 with average certainty 0.867\n",
      "(300000, 150) (300000,)\n",
      "/newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      " Model loaded from: /newvolume/W266Big/final_project/runs/hnk/aut/size_500000/checkpoints/hnkaut_model\n",
      "# batches = 2343\n",
      "Train epoch 0, average loss 0.274003, average accuracy 0.891302,\n",
      "\t\t aut Dev epoch 0, average loss 0.171,average accuracy 0.936,auc 0.958,f1_pos 0.963,f1_neg 0.765,f1_avg 0.864\n",
      "\t\t\t\t    Time taken for 0 epochs =  107.70446252822876\n",
      "Train epoch 2, average loss 0.178548, average accuracy 0.927363,\n",
      "\t\t aut Dev epoch 2, average loss 0.162,average accuracy 0.938,auc 0.962,f1_pos 0.964,f1_neg 0.758,f1_avg 0.861\n",
      "Train epoch 4, average loss 0.144343, average accuracy 0.942298,\n",
      "\t\t aut Dev epoch 4, average loss 0.160,average accuracy 0.941,auc 0.963,f1_pos 0.966,f1_neg 0.784,f1_avg 0.875\n",
      "\t\t\t\t    Time taken for 4 epochs =  513.5091598033905\n",
      "Train epoch 6, average loss 0.121157, average accuracy 0.951891,\n",
      "\t\t aut Dev epoch 6, average loss 0.170,average accuracy 0.940,auc 0.960,f1_pos 0.965,f1_neg 0.771,f1_avg 0.868\n",
      "Train epoch 8, average loss 0.102687, average accuracy 0.95979,\n",
      "\t\t aut Dev epoch 8, average loss 0.174,average accuracy 0.939,auc 0.962,f1_pos 0.964,f1_neg 0.786,f1_avg 0.875\n",
      "\t\t\t\t    Time taken for 8 epochs =  919.476071357727\n",
      "Train epoch 10, average loss 0.0879021, average accuracy 0.965862,\n",
      "\t\t aut Dev epoch 10, average loss 0.179,average accuracy 0.939,auc 0.961,f1_pos 0.964,f1_neg 0.786,f1_avg 0.875\n",
      "Train epoch 12, average loss 0.0767178, average accuracy 0.970791,\n",
      "\t\t aut Dev epoch 12, average loss 0.194,average accuracy 0.935,auc 0.961,f1_pos 0.962,f1_neg 0.781,f1_avg 0.872\n",
      "\t\t\t\t    Time taken for 12 epochs =  1325.5811002254486\n",
      "Train epoch 14, average loss 0.0663473, average accuracy 0.974939,\n",
      "\t\t aut Dev epoch 14, average loss 0.197,average accuracy 0.940,auc 0.960,f1_pos 0.965,f1_neg 0.782,f1_avg 0.874\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>acc</th>\n",
       "      <th>auc</th>\n",
       "      <th>f1_neg</th>\n",
       "      <th>f1_pos</th>\n",
       "      <th>f1_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.921988</td>\n",
       "      <td>0.933922</td>\n",
       "      <td>0.700341</td>\n",
       "      <td>0.955157</td>\n",
       "      <td>0.827749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.928173</td>\n",
       "      <td>0.938866</td>\n",
       "      <td>0.716952</td>\n",
       "      <td>0.958868</td>\n",
       "      <td>0.83791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.929321</td>\n",
       "      <td>0.945052</td>\n",
       "      <td>0.738871</td>\n",
       "      <td>0.959129</td>\n",
       "      <td>0.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.930188</td>\n",
       "      <td>0.945041</td>\n",
       "      <td>0.729207</td>\n",
       "      <td>0.959929</td>\n",
       "      <td>0.844568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.929914</td>\n",
       "      <td>0.942704</td>\n",
       "      <td>0.73091</td>\n",
       "      <td>0.95971</td>\n",
       "      <td>0.84531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.930662</td>\n",
       "      <td>0.944424</td>\n",
       "      <td>0.738981</td>\n",
       "      <td>0.960021</td>\n",
       "      <td>0.849501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.929821</td>\n",
       "      <td>0.944517</td>\n",
       "      <td>0.730054</td>\n",
       "      <td>0.959668</td>\n",
       "      <td>0.844861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20000</td>\n",
       "      <td>0.930715</td>\n",
       "      <td>0.942695</td>\n",
       "      <td>0.734772</td>\n",
       "      <td>0.960153</td>\n",
       "      <td>0.847462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.923056</td>\n",
       "      <td>0.934978</td>\n",
       "      <td>0.706158</td>\n",
       "      <td>0.955732</td>\n",
       "      <td>0.830945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.930388</td>\n",
       "      <td>0.94416</td>\n",
       "      <td>0.737628</td>\n",
       "      <td>0.95987</td>\n",
       "      <td>0.848749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.930995</td>\n",
       "      <td>0.946874</td>\n",
       "      <td>0.747393</td>\n",
       "      <td>0.96004</td>\n",
       "      <td>0.853716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.927526</td>\n",
       "      <td>0.945805</td>\n",
       "      <td>0.749856</td>\n",
       "      <td>0.957624</td>\n",
       "      <td>0.85374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.930048</td>\n",
       "      <td>0.944285</td>\n",
       "      <td>0.741322</td>\n",
       "      <td>0.959555</td>\n",
       "      <td>0.850439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.930468</td>\n",
       "      <td>0.942558</td>\n",
       "      <td>0.736552</td>\n",
       "      <td>0.959949</td>\n",
       "      <td>0.84825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.930708</td>\n",
       "      <td>0.943254</td>\n",
       "      <td>0.746126</td>\n",
       "      <td>0.959879</td>\n",
       "      <td>0.853002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50000</td>\n",
       "      <td>0.929848</td>\n",
       "      <td>0.94023</td>\n",
       "      <td>0.735558</td>\n",
       "      <td>0.95956</td>\n",
       "      <td>0.847559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.925258</td>\n",
       "      <td>0.942779</td>\n",
       "      <td>0.693463</td>\n",
       "      <td>0.95744</td>\n",
       "      <td>0.825452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.933837</td>\n",
       "      <td>0.950828</td>\n",
       "      <td>0.754073</td>\n",
       "      <td>0.961777</td>\n",
       "      <td>0.857925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.93267</td>\n",
       "      <td>0.948806</td>\n",
       "      <td>0.731952</td>\n",
       "      <td>0.961499</td>\n",
       "      <td>0.846726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.933664</td>\n",
       "      <td>0.950467</td>\n",
       "      <td>0.747133</td>\n",
       "      <td>0.961824</td>\n",
       "      <td>0.854479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.932349</td>\n",
       "      <td>0.952496</td>\n",
       "      <td>0.764372</td>\n",
       "      <td>0.960505</td>\n",
       "      <td>0.862439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.933737</td>\n",
       "      <td>0.949145</td>\n",
       "      <td>0.753768</td>\n",
       "      <td>0.961718</td>\n",
       "      <td>0.857743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.933163</td>\n",
       "      <td>0.950851</td>\n",
       "      <td>0.758905</td>\n",
       "      <td>0.961204</td>\n",
       "      <td>0.860054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100000</td>\n",
       "      <td>0.931355</td>\n",
       "      <td>0.950929</td>\n",
       "      <td>0.760949</td>\n",
       "      <td>0.959924</td>\n",
       "      <td>0.860436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.932296</td>\n",
       "      <td>0.94927</td>\n",
       "      <td>0.742541</td>\n",
       "      <td>0.961023</td>\n",
       "      <td>0.851782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.935879</td>\n",
       "      <td>0.95637</td>\n",
       "      <td>0.760772</td>\n",
       "      <td>0.962978</td>\n",
       "      <td>0.861875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.934998</td>\n",
       "      <td>0.95201</td>\n",
       "      <td>0.741586</td>\n",
       "      <td>0.962823</td>\n",
       "      <td>0.852204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.936653</td>\n",
       "      <td>0.955865</td>\n",
       "      <td>0.764946</td>\n",
       "      <td>0.963394</td>\n",
       "      <td>0.86417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.936146</td>\n",
       "      <td>0.953448</td>\n",
       "      <td>0.759287</td>\n",
       "      <td>0.963191</td>\n",
       "      <td>0.861239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.936546</td>\n",
       "      <td>0.955522</td>\n",
       "      <td>0.774402</td>\n",
       "      <td>0.963081</td>\n",
       "      <td>0.868741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.935512</td>\n",
       "      <td>0.954088</td>\n",
       "      <td>0.767957</td>\n",
       "      <td>0.962552</td>\n",
       "      <td>0.865254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150000</td>\n",
       "      <td>0.935512</td>\n",
       "      <td>0.953367</td>\n",
       "      <td>0.768346</td>\n",
       "      <td>0.962542</td>\n",
       "      <td>0.865444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.931609</td>\n",
       "      <td>0.95287</td>\n",
       "      <td>0.728716</td>\n",
       "      <td>0.960872</td>\n",
       "      <td>0.844794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.937153</td>\n",
       "      <td>0.958491</td>\n",
       "      <td>0.757616</td>\n",
       "      <td>0.963896</td>\n",
       "      <td>0.860756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.938094</td>\n",
       "      <td>0.958954</td>\n",
       "      <td>0.778729</td>\n",
       "      <td>0.964013</td>\n",
       "      <td>0.871371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.937687</td>\n",
       "      <td>0.958274</td>\n",
       "      <td>0.775621</td>\n",
       "      <td>0.963819</td>\n",
       "      <td>0.86972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.937553</td>\n",
       "      <td>0.956538</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>0.963735</td>\n",
       "      <td>0.869567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.93738</td>\n",
       "      <td>0.957352</td>\n",
       "      <td>0.774678</td>\n",
       "      <td>0.963637</td>\n",
       "      <td>0.869158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.93698</td>\n",
       "      <td>0.952055</td>\n",
       "      <td>0.759264</td>\n",
       "      <td>0.963744</td>\n",
       "      <td>0.861504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200000</td>\n",
       "      <td>0.937593</td>\n",
       "      <td>0.954598</td>\n",
       "      <td>0.768694</td>\n",
       "      <td>0.963931</td>\n",
       "      <td>0.866313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.935905</td>\n",
       "      <td>0.958013</td>\n",
       "      <td>0.764575</td>\n",
       "      <td>0.962903</td>\n",
       "      <td>0.863739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.938007</td>\n",
       "      <td>0.961913</td>\n",
       "      <td>0.757642</td>\n",
       "      <td>0.964458</td>\n",
       "      <td>0.86105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.940796</td>\n",
       "      <td>0.962591</td>\n",
       "      <td>0.783972</td>\n",
       "      <td>0.965697</td>\n",
       "      <td>0.874835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.940015</td>\n",
       "      <td>0.960082</td>\n",
       "      <td>0.770573</td>\n",
       "      <td>0.965497</td>\n",
       "      <td>0.868035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.938521</td>\n",
       "      <td>0.962227</td>\n",
       "      <td>0.785852</td>\n",
       "      <td>0.964108</td>\n",
       "      <td>0.87498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.939101</td>\n",
       "      <td>0.961487</td>\n",
       "      <td>0.78633</td>\n",
       "      <td>0.96449</td>\n",
       "      <td>0.87541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.935025</td>\n",
       "      <td>0.961292</td>\n",
       "      <td>0.781377</td>\n",
       "      <td>0.961842</td>\n",
       "      <td>0.87161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>300000</td>\n",
       "      <td>0.939942</td>\n",
       "      <td>0.959528</td>\n",
       "      <td>0.781865</td>\n",
       "      <td>0.965177</td>\n",
       "      <td>0.873521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      size       acc       auc    f1_neg    f1_pos    f1_avg\n",
       "0    20000  0.921988  0.933922  0.700341  0.955157  0.827749\n",
       "2    20000  0.928173  0.938866  0.716952  0.958868   0.83791\n",
       "4    20000  0.929321  0.945052  0.738871  0.959129     0.849\n",
       "6    20000  0.930188  0.945041  0.729207  0.959929  0.844568\n",
       "8    20000  0.929914  0.942704   0.73091   0.95971   0.84531\n",
       "10   20000  0.930662  0.944424  0.738981  0.960021  0.849501\n",
       "12   20000  0.929821  0.944517  0.730054  0.959668  0.844861\n",
       "14   20000  0.930715  0.942695  0.734772  0.960153  0.847462\n",
       "0    50000  0.923056  0.934978  0.706158  0.955732  0.830945\n",
       "2    50000  0.930388   0.94416  0.737628   0.95987  0.848749\n",
       "4    50000  0.930995  0.946874  0.747393   0.96004  0.853716\n",
       "6    50000  0.927526  0.945805  0.749856  0.957624   0.85374\n",
       "8    50000  0.930048  0.944285  0.741322  0.959555  0.850439\n",
       "10   50000  0.930468  0.942558  0.736552  0.959949   0.84825\n",
       "12   50000  0.930708  0.943254  0.746126  0.959879  0.853002\n",
       "14   50000  0.929848   0.94023  0.735558   0.95956  0.847559\n",
       "0   100000  0.925258  0.942779  0.693463   0.95744  0.825452\n",
       "2   100000  0.933837  0.950828  0.754073  0.961777  0.857925\n",
       "4   100000   0.93267  0.948806  0.731952  0.961499  0.846726\n",
       "6   100000  0.933664  0.950467  0.747133  0.961824  0.854479\n",
       "8   100000  0.932349  0.952496  0.764372  0.960505  0.862439\n",
       "10  100000  0.933737  0.949145  0.753768  0.961718  0.857743\n",
       "12  100000  0.933163  0.950851  0.758905  0.961204  0.860054\n",
       "14  100000  0.931355  0.950929  0.760949  0.959924  0.860436\n",
       "0   150000  0.932296   0.94927  0.742541  0.961023  0.851782\n",
       "2   150000  0.935879   0.95637  0.760772  0.962978  0.861875\n",
       "4   150000  0.934998   0.95201  0.741586  0.962823  0.852204\n",
       "6   150000  0.936653  0.955865  0.764946  0.963394   0.86417\n",
       "8   150000  0.936146  0.953448  0.759287  0.963191  0.861239\n",
       "10  150000  0.936546  0.955522  0.774402  0.963081  0.868741\n",
       "12  150000  0.935512  0.954088  0.767957  0.962552  0.865254\n",
       "14  150000  0.935512  0.953367  0.768346  0.962542  0.865444\n",
       "0   200000  0.931609   0.95287  0.728716  0.960872  0.844794\n",
       "2   200000  0.937153  0.958491  0.757616  0.963896  0.860756\n",
       "4   200000  0.938094  0.958954  0.778729  0.964013  0.871371\n",
       "6   200000  0.937687  0.958274  0.775621  0.963819   0.86972\n",
       "8   200000  0.937553  0.956538    0.7754  0.963735  0.869567\n",
       "10  200000   0.93738  0.957352  0.774678  0.963637  0.869158\n",
       "12  200000   0.93698  0.952055  0.759264  0.963744  0.861504\n",
       "14  200000  0.937593  0.954598  0.768694  0.963931  0.866313\n",
       "0   300000  0.935905  0.958013  0.764575  0.962903  0.863739\n",
       "2   300000  0.938007  0.961913  0.757642  0.964458   0.86105\n",
       "4   300000  0.940796  0.962591  0.783972  0.965697  0.874835\n",
       "6   300000  0.940015  0.960082  0.770573  0.965497  0.868035\n",
       "8   300000  0.938521  0.962227  0.785852  0.964108   0.87498\n",
       "10  300000  0.939101  0.961487   0.78633   0.96449   0.87541\n",
       "12  300000  0.935025  0.961292  0.781377  0.961842   0.87161\n",
       "14  300000  0.939942  0.959528  0.781865  0.965177  0.873521"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this cell, we add samples with the lowest certainty per word id first.\n",
    "size_model = size_initial\n",
    "src_key = s_key\n",
    "tgt_key = t_key\n",
    "\n",
    "#Create a sorted version of the certainty, and correspondingly sorted target train set ids, and labels.\n",
    "sort_ids = np.argsort(c_div_len_target)\n",
    "c_div_len_sorted = c_div_len_target[sort_ids]\n",
    "certainty_sorted = u_train_target_abs[sort_ids]\n",
    "#print(sort_ids)\n",
    "df_target_ids_pre = dict_transfer_train_ids[tgt_key][src_key]\n",
    "df_target_labels_pre = dict_train_y[tgt_key]\n",
    "print('Target labels pre sort',df_target_labels_pre[-20:])\n",
    "print(type(df_target_labels_pre))\n",
    "#df_target_ids_pre = df_target_ids_pre.iloc([sort_ids])\n",
    "df_target_ids = df_target_ids_pre[sort_ids]\n",
    "df_target_labels = df_target_labels_pre[sort_ids]\n",
    "print('\\n Target labels post sort',df_target_labels[-20:])\n",
    "print('\\n Certainty sorted','\\n First 20',certainty_sorted[:20],'\\n Last 20',certainty_sorted[-20:])\n",
    "\n",
    "results_cperlen = pd.DataFrame()\n",
    "\n",
    "\n",
    "print('\\nTraining on least certain first')\n",
    "size_list = size_list\n",
    "for size in size_list:\n",
    "    avg_certainty = np.average(certainty_sorted[:size])\n",
    "    avg_c_div_len = np.average(c_div_len_sorted[:size])\n",
    "    print('Training on target sample of size:',size,'with average certainty per word id',avg_c_div_len,'with average certainty %0.3f'%avg_certainty)\n",
    "    tgt_train_df = df_target_ids[:size]\n",
    "    tgt_train_y = df_target_labels[:size]\n",
    "    avg_certainty = np.average(certainty_sorted[:size])\n",
    "    print(tgt_train_df.shape,tgt_train_y.shape)\n",
    "    results = continue_transfer_train(src_key,size_model,tgt_key,tgt_train_df,tgt_train_y)\n",
    "    results['size'] = size\n",
    "    results_cperlen = pd.concat([results_cperlen,results])\n",
    "    \n",
    "results_cperlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old runs of CNN for 1 domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train results \n",
    "\n",
    "toys(10000)\n",
    "completed cnn creation\n",
    "# batches = 78\n",
    "Train epoch 0, average loss 0.410732, average accuracy 0.84986,\n",
    "\t\tDev epoch 0, average loss 0.384446, average accuracy 0.857337,\n",
    "\t\tDev epoch 0, auc 0.761469, new accuracy 0.857337, right accuracy 0.857337,\n",
    "\t\t\t\t    Time taken for 0 epochs =  85.97571444511414\n",
    "Train epoch 3, average loss 0.278746, average accuracy 0.882512,\n",
    "Train epoch 6, average loss 0.190353, average accuracy 0.92508,\n",
    "\t\tDev epoch 6, average loss 0.267068, average accuracy 0.892323,\n",
    "\t\tDev epoch 6, auc 0.884168, new accuracy 0.892323, right accuracy 0.892323,\n",
    "Train epoch 9, average loss 0.120366, average accuracy 0.963241,\n",
    "Train epoch 12, average loss 0.0750643, average accuracy 0.983874,\n",
    "\t\tDev epoch 12, average loss 0.250142, average accuracy 0.90591,\n",
    "\t\tDev epoch 12, auc 0.898789, new accuracy 0.90591, right accuracy 0.90591,\n",
    "\t\t\t\t    Time taken for 12 epochs =  1057.8321163654327\n",
    "Train epoch 15, average loss 0.0461413, average accuracy 0.995292,\n",
    "Saved model toys /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    "\n",
    "Continue train \n",
    "toys(2000)\n",
    "# batches = 15\n",
    "Train epoch 0, average loss 0.0328055, average accuracy 0.998958,\n",
    "\t\tDev epoch 0, average loss 0.253235, average accuracy 0.900391,\n",
    "\t\tDev epoch 0, auc 0.902401, new accuracy 0.900391, right accuracy 0.900391,\n",
    "\t\t\t\t    Time taken for 0 epochs =  15.97208309173584\n",
    "Train epoch 3, average loss 0.0209501, average accuracy 1,\n",
    "Train epoch 6, average loss 0.0157637, average accuracy 1,\n",
    "\t\tDev epoch 6, average loss 0.246794, average accuracy 0.910156,\n",
    "\t\tDev epoch 6, auc 0.904086, new accuracy 0.910156, right accuracy 0.910156,\n",
    "Train epoch 9, average loss 0.0133948, average accuracy 0.999479,\n",
    "Train epoch 12, average loss 0.0110164, average accuracy 1,\n",
    "\t\tDev epoch 12, average loss 0.268596, average accuracy 0.900391,\n",
    "\t\tDev epoch 12, auc 0.903477, new accuracy 0.900391, right accuracy 0.900391,\n",
    "\t\t\t\t    Time taken for 12 epochs =  203.32078433036804\n",
    "Train epoch 15, average loss 0.00984509, average accuracy 1,\n",
    "\n",
    "\n",
    "Continue train \n",
    "toys(4000)\n",
    "/home/reachanamikasinha/project/testruns/toys/checkpoints\n",
    " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
    "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    " Model loaded from: /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    "# batches = 31\n",
    "Train epoch 0, average loss 0.0316445, average accuracy 0.997732,\n",
    "\t\tDev epoch 0, average loss 0.238046, average accuracy 0.911458,\n",
    "\t\tDev epoch 0, auc 0.915253, new accuracy 0.911458, right accuracy 0.911458,\n",
    "\t\t\t\t    Time taken for 0 epochs =  33.795907497406006\n",
    "Train epoch 3, average loss 0.021633, average accuracy 0.999748,\n",
    "Train epoch 6, average loss 0.0158767, average accuracy 0.999496,\n",
    "\t\tDev epoch 6, average loss 0.272812, average accuracy 0.903646,\n",
    "\t\tDev epoch 6, auc 0.915281, new accuracy 0.903646, right accuracy 0.903646,\n",
    "Train epoch 9, average loss 0.0130432, average accuracy 0.999748,\n",
    "Train epoch 12, average loss 0.0109889, average accuracy 1,\n",
    "\t\tDev epoch 12, average loss 0.275246, average accuracy 0.908854,\n",
    "\t\tDev epoch 12, auc 0.91456, new accuracy 0.908854, right accuracy 0.908854,\n",
    "\t\t\t\t    Time taken for 12 epochs =  418.0527718067169\n",
    "Train epoch 15, average loss 0.0100661, average accuracy 0.999748,\n",
    "Train epoch 18, average loss 0.00892393, average accuracy 1,\n",
    "\t\tDev epoch 18, average loss 0.276631, average accuracy 0.907986,\n",
    "\t\tDev epoch 18, auc 0.915397, new accuracy 0.907986, right accuracy 0.907986,\n",
    "Train epoch 21, average loss 0.00748347, average accuracy 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of transfer learning testing\n",
    "\n",
    "Comparison\n",
    "target vid\n",
    "/home/reachanamikasinha/project/runs/toys/checkpoints\n",
    "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/checkpoints/toys01_model\n",
    "0.814630681818\n",
    "toys vid AUC 81.24%\n",
    "toys vid accuracy 81.46%\n",
    "\n",
    "Transfer accuracy source toys, target vid(source trained on 10,000 and \n",
    "auc 0.893548, new accuracy 0.898438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving output of model saved run to be able to compare dev accuracy\n",
    "\n",
    "# toys\n",
    "\n",
    "completed cnn creation\n",
    "#batches = 1562\n",
    "Train epoch 0, average loss 0.281697, average accuracy 0.886964,\n",
    "\t\tDev epoch 0, average loss 0.244123, average accuracy 0.908854,\n",
    "\t\t\t\t    Time taken for 0 epochs =  37.60847544670105\n",
    "Train epoch 3, average loss 0.142607, average accuracy 0.944032,\n",
    "Train epoch 6, average loss 0.0771984, average accuracy 0.971111,\n",
    "\t\tDev epoch 6, average loss 0.234148, average accuracy 0.925581,\n",
    "Train epoch 9, average loss 0.0413356, average accuracy 0.985665,\n",
    "Train epoch 12, average loss 0.0262069, average accuracy 0.991207,\n",
    "\t\tDev epoch 12, average loss 0.259911, average accuracy 0.931858,\n",
    "\t\t\t\t    Time taken for 12 epochs =  458.23175573349\n",
    "Train epoch 15, average loss 0.0191853, average accuracy 0.994008,\n",
    "Train epoch 18, average loss 0.0141925, average accuracy 0.995569,\n",
    "\t\tDev epoch 18, average loss 0.297451, average accuracy 0.932759,\n",
    "Train epoch 21, average loss 0.0111091, average accuracy 0.996669,\n",
    "Train epoch 24, average loss 0.00874389, average accuracy 0.997289,\n",
    "\t\tDev epoch 24, average loss 0.37409, average accuracy 0.930889,\n",
    "\t\t\t\t    Time taken for 24 epochs =  879.0003838539124\n",
    "Train epoch 27, average loss 0.00864632, average accuracy 0.997309,\n",
    "Train epoch 30, average loss 0.00784798, average accuracy 0.997459,\n",
    "\t\tDev epoch 30, average loss 0.358449, average accuracy 0.932192,\n",
    "Train epoch 33, average loss 0.00659365, average accuracy 0.997819,\n",
    "Train epoch 36, average loss 0.00578367, average accuracy 0.998349,\n",
    "\t\tDev epoch 36, average loss 0.373084, average accuracy 0.931958,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1299.6440062522888\n",
    "Train epoch 39, average loss 0.0064537, average accuracy 0.998089,\n",
    "Train epoch 42, average loss 0.00580202, average accuracy 0.998259,\n",
    "\t\tDev epoch 42, average loss 0.391404, average accuracy 0.933393,\n",
    "Train epoch 45, average loss 0.00514404, average accuracy 0.99845,\n",
    "Train epoch 48, average loss 0.00371194, average accuracy 0.99892,\n",
    "\t\tDev epoch 48, average loss 0.469131, average accuracy 0.931457,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1720.3404235839844\n",
    "Train epoch 51, average loss 0.0041625, average accuracy 0.99878,\n",
    "Train epoch 54, average loss 0.00460097, average accuracy 0.99856,\n",
    "\t\tDev epoch 54, average loss 0.412365, average accuracy 0.934195,\n",
    "Train epoch 57, average loss 0.00364978, average accuracy 0.99895,\n",
    "Saved model toys /home/ubuntu/project/runs/cnn/checkpoints/toys_model\n",
    "\n",
    "# vid\n",
    "completed cnn creation\n",
    "#batches = 1562\n",
    "Train epoch 0, average loss 0.364666, average accuracy 0.84355,\n",
    "\t\tDev epoch 0, average loss 0.319497, average accuracy 0.86071,\n",
    "\t\t\t\t    Time taken for 0 epochs =  37.799813985824585\n",
    "Train epoch 3, average loss 0.205366, average accuracy 0.917043,\n",
    "Train epoch 6, average loss 0.121604, average accuracy 0.952835,\n",
    "\t\tDev epoch 6, average loss 0.270975, average accuracy 0.899272,\n",
    "Train epoch 9, average loss 0.07385, average accuracy 0.972371,\n",
    "Train epoch 12, average loss 0.0478919, average accuracy 0.982955,\n",
    "\t\tDev epoch 12, average loss 0.35926, average accuracy 0.8959,\n",
    "\t\t\t\t    Time taken for 12 epochs =  459.7377371788025\n",
    "Train epoch 15, average loss 0.0352903, average accuracy 0.987636,\n",
    "Train epoch 18, average loss 0.0294105, average accuracy 0.989997,\n",
    "\t\tDev epoch 18, average loss 0.453399, average accuracy 0.899439,\n",
    "Train epoch 21, average loss 0.0230395, average accuracy 0.992167,\n",
    "Train epoch 24, average loss 0.021058, average accuracy 0.992928,\n",
    "\t\tDev epoch 24, average loss 0.537794, average accuracy 0.899773,\n",
    "\t\t\t\t    Time taken for 24 epochs =  881.6988339424133\n",
    "Train epoch 27, average loss 0.0175359, average accuracy 0.994248,\n",
    "Train epoch 30, average loss 0.0139743, average accuracy 0.995549,\n",
    "\t\tDev epoch 30, average loss 0.571489, average accuracy 0.899673,\n",
    "Train epoch 33, average loss 0.0134842, average accuracy 0.995739,\n",
    "Train epoch 36, average loss 0.0106022, average accuracy 0.996829,\n",
    "\t\tDev epoch 36, average loss 0.609015, average accuracy 0.900407,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1303.694475889206\n",
    "Train epoch 39, average loss 0.0102687, average accuracy 0.996589,\n",
    "Train epoch 42, average loss 0.00967124, average accuracy 0.997099,\n",
    "\t\tDev epoch 42, average loss 0.639956, average accuracy 0.900174,\n",
    "Train epoch 45, average loss 0.00792713, average accuracy 0.997689,\n",
    "Train epoch 48, average loss 0.00804649, average accuracy 0.997559,\n",
    "\t\tDev epoch 48, average loss 0.696189, average accuracy 0.899272,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1725.6383044719696\n",
    "Train epoch 51, average loss 0.00929602, average accuracy 0.997259,\n",
    "Train epoch 54, average loss 0.0067116, average accuracy 0.998039,\n",
    "\t\tDev epoch 54, average loss 0.606182, average accuracy 0.900007,\n",
    "Train epoch 57, average loss 0.00869572, average accuracy 0.997289,\n",
    "Saved model vid /home/ubuntu/project/runs/cnn/checkpoints/vid_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For 100,000 rows\n",
    "## Output of predict on source domain toys\n",
    "Target toys\n",
    "\n",
    "\n",
    "## Output of predict on source domainvid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEEPING TRACK OF RESULTS FROM DIFFERENT RUNS\n",
    "#### Number samples = 10000, Number batches = 156, Without pre-trained embeddings, no dropout\n",
    "Train epoch 0, loss 0.357085, average loss 0.440941, acc 0.84375, average acc 0.845152,\n",
    "\tDev epoch 0, loss 0.494501, average loss 0.394619, acc 0.78125, average acc 0.854959,\n",
    "\t\tTime taken for 0 epochs =  36.18872332572937\n",
    "Train epoch 2, loss 0.25934, average loss 0.335786, acc 0.875, average acc 0.862079,\n",
    "Train epoch 4, loss 0.214457, average loss 0.263726, acc 0.875, average acc 0.891827,\n",
    "Train epoch 6, loss 0.161232, average loss 0.194851, acc 0.890625, average acc 0.92528,\n",
    "Train epoch 8, loss 0.0968393, average loss 0.132971, acc 0.984375, average acc 0.958133,\n",
    "Train epoch 10, loss 0.0598382, average loss 0.0935878, acc 1, average acc 0.978766,\n",
    "\tDev epoch 10, loss 0.318434, average loss 0.279314, acc 0.875, average acc 0.891304,\n",
    "\t\tTime taken for 10 epochs =  366.54717350006104\n",
    "Train epoch 12, loss 0.0432213, average loss 0.089715, acc 1, average acc 0.979768,\n",
    "Train epoch 14, loss 0.0724975, average loss 0.299487, acc 1, average acc 0.957933,\n",
    "Train epoch 16, loss 0.0388074, average loss 0.0520482, acc 1, average acc 0.991987,\n",
    "Train epoch 18, loss 0.0239645, average loss 0.0351604, acc 1, average acc 0.997196,\n",
    "Train epoch 20, loss 0.0157139, average loss 0.0272624, acc 1, average acc 0.996595,\n",
    "\tDev epoch 20, loss 0.304895, average loss 0.283551, acc 0.921875, average acc 0.902853,\n",
    "\t\tTime taken for 20 epochs =  697.680163860321\n",
    "Train epoch 22, loss 0.0131277, average loss 0.017179, acc 1, average acc 0.999299,\n",
    "Train epoch 24, loss 0.0104588, average loss 0.0121853, acc 1, average acc 0.9999,\n",
    "Train epoch 26, loss 0.0072446, average loss 0.00969622, acc 1, average acc 0.9999,\n",
    "Train epoch 28, loss 0.00628954, average loss 0.00805781, acc 1, average acc 0.9999,\n",
    "Train epoch 30, loss 0.00585206, average loss 0.00689346, acc 1, average acc 0.9999,\n",
    "\tDev epoch 30, loss 0.37002, average loss 0.327092, acc 0.875, average acc 0.902514,\n",
    "\t\tTime taken for 30 epochs =  1029.1201057434082\n",
    "Train epoch 32, loss 0.00559655, average loss 0.00594074, acc 1, average acc 0.9999,\n",
    "Train epoch 34, loss 0.0053231, average loss 0.00518374, acc 1, average acc 1,\n",
    "Train epoch 36, loss 0.00504235, average loss 0.00461033, acc 1, average acc 1,\n",
    "Train epoch 38, loss 0.00477842, average loss 0.00416377, acc 1, average acc 1,\n",
    "Train epoch 40, loss 0.00451324, average loss 0.00380536, acc 1, average acc 1,\n",
    "\tDev epoch 40, loss 0.451459, average loss 0.382435, acc 0.859375, average acc 0.899796,\n",
    "\t\tTime taken for 40 epochs =  1360.2103555202484\n",
    "        \n",
    "        \n",
    "#### Number samples = 10000, Number batches = 156, With pre-trained embeddings(Trainable = False), dropout = 0.8\n",
    "Train epoch 0, average loss 0.819034, average acc 0.802784,\n",
    "\tDev epoch 0, average loss 0.415172, average acc 0.853601,\n",
    "\t\tTime taken for 0 epochs =  35.559093713760376\n",
    "Train epoch 2, average loss 0.403757, average acc 0.841046,\n",
    "Train epoch 4, average loss 0.340479, average acc 0.860777,\n",
    "\tDev epoch 5, average loss 0.329067, average acc 0.867188,\n",
    "Train epoch 6, average loss 0.289147, average acc 0.882312,\n",
    "Train epoch 8, average loss 0.237817, average acc 0.904948,\n",
    "Train epoch 10, average loss 0.194272, average acc 0.923978,\n",
    "\tDev epoch 10, average loss 0.330927, average acc 0.876698,\n",
    "\t\tTime taken for 10 epochs =  363.92726016044617\n",
    "Train epoch 12, average loss 0.149883, average acc 0.940405,\n",
    "Train epoch 14, average loss 0.128152, average acc 0.951322,\n",
    "\tDev epoch 15, average loss 0.349508, average acc 0.877717,\n",
    "Train epoch 16, average loss 0.101319, average acc 0.961639,\n",
    "Train epoch 18, average loss 0.079585, average acc 0.970052,\n",
    "Train epoch 20, average loss 0.0705579, average acc 0.97516,\n",
    "\tDev epoch 20, average loss 0.364253, average acc 0.878057,\n",
    "\t\tTime taken for 20 epochs =  692.3398864269257\n",
    "Train epoch 22, average loss 0.0631964, average acc 0.978466,\n",
    "Train epoch 24, average loss 0.0484077, average acc 0.984876,\n",
    "\tDev epoch 25, average loss 0.435054, average acc 0.877717,\n",
    "Train epoch 26, average loss 0.0433892, average acc 0.985377,\n",
    "Train epoch 28, average loss 0.0368327, average acc 0.988381,\n",
    "Train epoch 30, average loss 0.0308169, average acc 0.990385,\n",
    "\tDev epoch 30, average loss 0.570798, average acc 0.875679,\n",
    "\t\tTime taken for 30 epochs =  1052.273297548294\n",
    "Train epoch 32, average loss 0.0291807, average acc 0.991086,\n",
    "Train epoch 34, average loss 0.0271599, average acc 0.991987,\n",
    "\tDev epoch 35, average loss 0.661539, average acc 0.87534,\n",
    "Train epoch 36, average loss 0.029594, average acc 0.991486,\n",
    "Train epoch 38, average loss 0.0236557, average acc 0.99359,\n",
    "Train epoch 40, average loss 0.018746, average acc 0.995292,\n",
    "\tDev epoch 40, average loss 0.506544, average acc 0.878397,\n",
    "\t\tTime taken for 40 epochs =  1466.0729427337646\n",
    "        \n",
    "        \n",
    "#### Changes. Changed convolutional layer weights to xavier initialization. Added random see = 42 to train-test split. Dropped learning rate initial to 0.007\n",
    "\n",
    "Train epoch 0, average loss 0.399899, average accuracy 0.858273,\n",
    "\tDev epoch 0, average loss 0.374428, average accuracy 0.857337,\n",
    "\t\tTime taken for 0 epochs =  34.83389401435852\n",
    "Train epoch 2, average loss 0.326706, average accuracy 0.869391,\n",
    "Train epoch 4, average loss 0.269826, average accuracy 0.891126,\n",
    "\tDev epoch 5, average loss 0.303238, average accuracy 0.88519,\n",
    "Train epoch 6, average loss 0.219368, average accuracy 0.911659,\n",
    "Train epoch 8, average loss 0.171234, average accuracy 0.935296,\n",
    "Train epoch 10, average loss 0.13296, average accuracy 0.953325,\n",
    "\tDev epoch 10, average loss 0.293018, average accuracy 0.887568,\n",
    "\t\tTime taken for 10 epochs =  370.47870922088623\n",
    "Train epoch 12, average loss 0.100562, average accuracy 0.967348,\n",
    "Train epoch 14, average loss 0.0793127, average accuracy 0.977063,\n",
    "\tDev epoch 15, average loss 0.320119, average accuracy 0.886209,\n",
    "Train epoch 16, average loss 0.0582729, average accuracy 0.988482,\n",
    "Train epoch 18, average loss 0.0456755, average accuracy 0.990385,\n",
    "Train epoch 20, average loss 0.0405185, average accuracy 0.992788,\n",
    "\tDev epoch 20, average loss 0.321604, average accuracy 0.886889,\n",
    "\t\tTime taken for 20 epochs =  705.0958936214447\n",
    "Train epoch 22, average loss 0.0351258, average accuracy 0.993089,\n",
    "Train epoch 24, average loss 0.0270392, average accuracy 0.996194,\n",
    "\tDev epoch 25, average loss 0.399808, average accuracy 0.884171,\n",
    "Train epoch 26, average loss 0.0262923, average accuracy 0.995994,\n",
    "Train epoch 28, average loss 0.0242657, average accuracy 0.995994,\n",
    "Train epoch 30, average loss 0.0208821, average accuracy 0.996394,\n",
    "\tDev epoch 30, average loss 0.413923, average accuracy 0.886889,\n",
    "\t\tTime taken for 30 epochs =  1039.4113600254059\n",
    "Train epoch 32, average loss 0.017492, average accuracy 0.997696,\n",
    "Train epoch 34, average loss 0.0146527, average accuracy 0.998097,\n",
    "\tDev epoch 35, average loss 0.386267, average accuracy 0.884851,\n",
    "Train epoch 36, average loss 0.0168233, average accuracy 0.997396,\n",
    "Train epoch 38, average loss 0.0142984, average accuracy 0.997796,\n",
    "Train epoch 40, average loss 0.0110543, average accuracy 0.998998,\n",
    "\tDev epoch 40, average loss 0.478341, average accuracy 0.884171,\n",
    "\t\tTime taken for 40 epochs =  1374.086744070053\n",
    "Train epoch 42, average loss 0.012298, average accuracy 0.998397,\n",
    "Train epoch 44, average loss 0.0116889, average accuracy 0.998197,\n",
    "\tDev epoch 45, average loss 0.448394, average accuracy 0.88587,\n",
    "Train epoch 46, average loss 0.0107089, average accuracy 0.998197,\n",
    "Train epoch 48, average loss 0.00953887, average accuracy 0.998898,\n",
    "Train epoch 50, average loss 0.0097256, average accuracy 0.998898,\n",
    "\tDev epoch 50, average loss 0.424627, average accuracy 0.886209,\n",
    "\t\tTime taken for 50 epochs =  1708.131004333496\n",
    "Train epoch 52, average loss 0.00792942, average accuracy 0.999099,\n",
    "Train epoch 54, average loss 0.00777054, average accuracy 0.999099,\n",
    "\tDev epoch 55, average loss 0.434766, average accuracy 0.887228,\n",
    "Train epoch 56, average loss 0.00812112, average accuracy 0.999099,\n",
    "Train epoch 58, average loss 0.00817043, average accuracy 0.998798,\n",
    "Train epoch 60, average loss 0.00776972, average accuracy 0.998498,\n",
    "\tDev epoch 60, average loss 0.447535, average accuracy 0.886889,\n",
    "\t\tTime taken for 60 epochs =  2042.1303217411041\n",
    "Train epoch 62, average loss 0.00759579, average accuracy 0.998998,\n",
    "Train epoch 64, average loss 0.00697335, average accuracy 0.998798,\n",
    "\tDev epoch 65, average loss 0.514295, average accuracy 0.88519,\n",
    "Train epoch 66, average loss 0.00579109, average accuracy 0.999199,\n",
    "Train epoch 68, average loss 0.00583337, average accuracy 0.999499,\n",
    "\n",
    "#### Changes. set trainable = True in glove embeddings. Changed learning rate back to 0.01 initial.\n",
    "# batches = 156\n",
    "Train epoch 0, average loss 0.399899, average accuracy 0.858273,\n",
    "\tDev epoch 0, average loss 0.374428, average accuracy 0.857337,\n",
    "\t\tTime taken for 0 epochs =  34.83389401435852\n",
    "Train epoch 2, average loss 0.326706, average accuracy 0.869391,\n",
    "Train epoch 4, average loss 0.269826, average accuracy 0.891126,\n",
    "\tDev epoch 5, average loss 0.303238, average accuracy 0.88519,\n",
    "Train epoch 6, average loss 0.219368, average accuracy 0.911659,\n",
    "Train epoch 8, average loss 0.171234, average accuracy 0.935296,\n",
    "Train epoch 10, average loss 0.13296, average accuracy 0.953325,\n",
    "\tDev epoch 10, average loss 0.293018, average accuracy 0.887568,\n",
    "\t\tTime taken for 10 epochs =  370.47870922088623\n",
    "Train epoch 12, average loss 0.100562, average accuracy 0.967348,\n",
    "Train epoch 14, average loss 0.0793127, average accuracy 0.977063,\n",
    "\tDev epoch 15, average loss 0.320119, average accuracy 0.886209,\n",
    "Train epoch 16, average loss 0.0582729, average accuracy 0.988482,\n",
    "Train epoch 18, average loss 0.0456755, average accuracy 0.990385,\n",
    "Train epoch 20, average loss 0.0405185, average accuracy 0.992788,\n",
    "\tDev epoch 20, average loss 0.321604, average accuracy 0.886889,\n",
    "\t\tTime taken for 20 epochs =  705.0958936214447\n",
    "Train epoch 22, average loss 0.0351258, average accuracy 0.993089,\n",
    "Train epoch 24, average loss 0.0270392, average accuracy 0.996194,\n",
    "\tDev epoch 25, average loss 0.399808, average accuracy 0.884171,\n",
    "Train epoch 26, average loss 0.0262923, average accuracy 0.995994,\n",
    "Train epoch 28, average loss 0.0242657, average accuracy 0.995994,\n",
    "Train epoch 30, average loss 0.0208821, average accuracy 0.996394,\n",
    "\tDev epoch 30, average loss 0.413923, average accuracy 0.886889,\n",
    "\t\tTime taken for 30 epochs =  1039.4113600254059\n",
    "Train epoch 32, average loss 0.017492, average accuracy 0.997696,\n",
    "Train epoch 34, average loss 0.0146527, average accuracy 0.998097,\n",
    "\tDev epoch 35, average loss 0.386267, average accuracy 0.884851,\n",
    "Train epoch 36, average loss 0.0168233, average accuracy 0.997396,\n",
    "Train epoch 38, average loss 0.0142984, average accuracy 0.997796,\n",
    "Train epoch 40, average loss 0.0110543, average accuracy 0.998998,\n",
    "\tDev epoch 40, average loss 0.478341, average accuracy 0.884171,\n",
    "\t\tTime taken for 40 epochs =  1374.086744070053\n",
    "Train epoch 42, average loss 0.012298, average accuracy 0.998397,\n",
    "Train epoch 44, average loss 0.0116889, average accuracy 0.998197,\n",
    "\tDev epoch 45, average loss 0.448394, average accuracy 0.88587,\n",
    "Train epoch 46, average loss 0.0107089, average accuracy 0.998197,\n",
    "Train epoch 48, average loss 0.00953887, average accuracy 0.998898,\n",
    "Train epoch 50, average loss 0.0097256, average accuracy 0.998898,\n",
    "\tDev epoch 50, average loss 0.424627, average accuracy 0.886209,\n",
    "\t\tTime taken for 50 epochs =  1708.131004333496\n",
    "Train epoch 52, average loss 0.00792942, average accuracy 0.999099,\n",
    "Train epoch 54, average loss 0.00777054, average accuracy 0.999099,\n",
    "\tDev epoch 55, average loss 0.434766, average accuracy 0.887228,\n",
    "Train epoch 56, average loss 0.00812112, average accuracy 0.999099,\n",
    "Train epoch 58, average loss 0.00817043, average accuracy 0.998798,\n",
    "Train epoch 60, average loss 0.00776972, average accuracy 0.998498,\n",
    "\tDev epoch 60, average loss 0.447535, average accuracy 0.886889,\n",
    "\t\tTime taken for 60 epochs =  2042.1303217411041\n",
    "Train epoch 62, average loss 0.00759579, average accuracy 0.998998,\n",
    "Train epoch 64, average loss 0.00697335, average accuracy 0.998798,\n",
    "\tDev epoch 65, average loss 0.514295, average accuracy 0.88519,\n",
    "Train epoch 66, average loss 0.00579109, average accuracy 0.999199,\n",
    "Train epoch 68, average loss 0.00583337, average accuracy 0.999499,\n",
    "\n",
    "\n",
    "#### Changes : increased sample size to 20000. increased filter number to 256 per filter size. Both together slowed it down 4 times. Ran 150 epochs.\n",
    "\n",
    "Result - get to accuracy of about 90.5% on dev set. First saw it in about 80 epochs.\n",
    "\n",
    "number of batches = 312\n",
    "Train epoch 0, average loss 0.399409, average accuracy 0.851763,\n",
    "\tDev epoch 0, average loss 0.390935, average accuracy 0.849798,\n",
    "\t\tTime taken for 0 epochs =  126.17049622535706\n",
    "Train epoch 2, average loss 0.310082, average accuracy 0.877955,\n",
    "Train epoch 4, average loss 0.240321, average accuracy 0.905298,\n",
    "\tDev epoch 5, average loss 0.309063, average accuracy 0.879872,\n",
    "Train epoch 6, average loss 0.183422, average accuracy 0.929137,\n",
    "Train epoch 8, average loss 0.132305, average accuracy 0.950871,\n",
    "Train epoch 10, average loss 0.091186, average accuracy 0.96855,\n",
    "\tDev epoch 10, average loss 0.359275, average accuracy 0.887769,\n",
    "\t\tTime taken for 10 epochs =  1301.3605210781097\n",
    "Train epoch 12, average loss 0.0655771, average accuracy 0.978966,\n",
    "Train epoch 14, average loss 0.0487824, average accuracy 0.986579,\n",
    "\tDev epoch 15, average loss 0.31907, average accuracy 0.901546,\n",
    "Train epoch 16, average loss 0.0353836, average accuracy 0.991136,\n",
    "Train epoch 18, average loss 0.0272109, average accuracy 0.993389,\n",
    "Train epoch 20, average loss 0.0209386, average accuracy 0.995843,\n",
    "\tDev epoch 20, average loss 0.473887, average accuracy 0.888609,\n",
    "\t\tTime taken for 20 epochs =  2475.8890883922577\n",
    "Train epoch 22, average loss 0.0169513, average accuracy 0.996444,\n",
    "Train epoch 24, average loss 0.0144857, average accuracy 0.997045,\n",
    "\tDev epoch 25, average loss 0.52256, average accuracy 0.886929,\n",
    "Train epoch 26, average loss 0.0115876, average accuracy 0.998147,\n",
    "Train epoch 28, average loss 0.00961356, average accuracy 0.998347,\n",
    "Train epoch 30, average loss 0.00915859, average accuracy 0.998297,\n",
    "\tDev epoch 30, average loss 0.458756, average accuracy 0.895665,\n",
    "\t\tTime taken for 30 epochs =  3649.31303191185\n",
    "Train epoch 32, average loss 0.00898325, average accuracy 0.998498,\n",
    "Train epoch 34, average loss 0.00841926, average accuracy 0.998448,\n",
    "\tDev epoch 35, average loss 0.457308, average accuracy 0.897681,\n",
    "Train epoch 36, average loss 0.00646681, average accuracy 0.999149,\n",
    "Train epoch 38, average loss 0.00662382, average accuracy 0.998598,\n",
    "Train epoch 40, average loss 0.00595506, average accuracy 0.999149,\n",
    "\tDev epoch 40, average loss 0.378182, average accuracy 0.901546,\n",
    "\t\tTime taken for 40 epochs =  4823.336989402771\n",
    "Train epoch 42, average loss 0.00593351, average accuracy 0.999149,\n",
    "Train epoch 44, average loss 0.00464219, average accuracy 0.999249,\n",
    "\tDev epoch 45, average loss 0.431081, average accuracy 0.90121,\n",
    "Train epoch 46, average loss 0.00444085, average accuracy 0.999499,\n",
    "Train epoch 48, average loss 0.00461485, average accuracy 0.999349,\n",
    "Train epoch 50, average loss 0.00466378, average accuracy 0.999199,\n",
    "\tDev epoch 50, average loss 0.380632, average accuracy 0.90289,\n",
    "\t\tTime taken for 50 epochs =  5997.558866024017\n",
    "Train epoch 52, average loss 0.00401276, average accuracy 0.999299,\n",
    "Train epoch 54, average loss 0.00360064, average accuracy 0.999549,\n",
    "\tDev epoch 55, average loss 0.472261, average accuracy 0.900706,\n",
    "Train epoch 56, average loss 0.00390259, average accuracy 0.999449,\n",
    "Train epoch 58, average loss 0.00343323, average accuracy 0.999499,\n",
    "Train epoch 60, average loss 0.00328182, average accuracy 0.999549,\n",
    "\tDev epoch 60, average loss 0.405813, average accuracy 0.901714,\n",
    "\t\tTime taken for 60 epochs =  7171.330280542374\n",
    "Train epoch 62, average loss 0.00357674, average accuracy 0.999499,\n",
    "Train epoch 64, average loss 0.00316356, average accuracy 0.999449,\n",
    "\tDev epoch 65, average loss 0.484432, average accuracy 0.899698,\n",
    "Train epoch 66, average loss 0.00242786, average accuracy 0.99975,\n",
    "Train epoch 68, average loss 0.0029979, average accuracy 0.999399,\n",
    "Train epoch 70, average loss 0.00219736, average accuracy 0.999599,\n",
    "\tDev epoch 70, average loss 0.522188, average accuracy 0.89953,\n",
    "\t\tTime taken for 70 epochs =  8345.40755033493\n",
    "Train epoch 72, average loss 0.00263028, average accuracy 0.999599,\n",
    "Train epoch 74, average loss 0.00262097, average accuracy 0.999599,\n",
    "\tDev epoch 75, average loss 0.501381, average accuracy 0.90037,\n",
    "Train epoch 76, average loss 0.00184087, average accuracy 0.99975,\n",
    "Train epoch 78, average loss 0.00261343, average accuracy 0.999399,\n",
    "Train epoch 80, average loss 0.00210662, average accuracy 0.9997,\n",
    "\tDev epoch 80, average loss 0.437249, average accuracy 0.90457,\n",
    "\t\tTime taken for 80 epochs =  9519.476462364197\n",
    "Train epoch 82, average loss 0.00218873, average accuracy 0.999599,\n",
    "Train epoch 84, average loss 0.00204953, average accuracy 0.9997,\n",
    "\tDev epoch 85, average loss 0.440843, average accuracy 0.90457,\n",
    "Train epoch 86, average loss 0.00178851, average accuracy 0.99975,\n",
    "Train epoch 88, average loss 0.00177724, average accuracy 0.999599,\n",
    "Train epoch 90, average loss 0.0018953, average accuracy 0.999599,\n",
    "\tDev epoch 90, average loss 0.465653, average accuracy 0.905074,\n",
    "\t\tTime taken for 90 epochs =  10693.68774318695\n",
    "Train epoch 92, average loss 0.00145395, average accuracy 0.9998,\n",
    "Train epoch 94, average loss 0.00158429, average accuracy 0.999649,\n",
    "\tDev epoch 95, average loss 0.472749, average accuracy 0.90541,\n",
    "Train epoch 96, average loss 0.00238694, average accuracy 0.999499,\n",
    "Train epoch 98, average loss 0.00175795, average accuracy 0.9997,\n",
    "Train epoch 100, average loss 0.00153946, average accuracy 0.9998,\n",
    "\tDev epoch 100, average loss 0.440989, average accuracy 0.906754,\n",
    "\t\tTime taken for 100 epochs =  11867.341850280762\n",
    "Train epoch 102, average loss 0.00144569, average accuracy 0.9998,\n",
    "Train epoch 104, average loss 0.00135836, average accuracy 0.99975,\n",
    "\tDev epoch 105, average loss 0.450465, average accuracy 0.90457,\n",
    "Train epoch 106, average loss 0.00134604, average accuracy 0.99985,\n",
    "Train epoch 108, average loss 0.00198454, average accuracy 0.999549,\n",
    "Train epoch 110, average loss 0.0016289, average accuracy 0.99975,\n",
    "\tDev epoch 110, average loss 0.43462, average accuracy 0.905242,\n",
    "\t\tTime taken for 110 epochs =  13040.53886771202\n",
    "Train epoch 112, average loss 0.00116808, average accuracy 0.99975,\n",
    "Train epoch 114, average loss 0.00163431, average accuracy 0.999649,\n",
    "\tDev epoch 115, average loss 0.544521, average accuracy 0.901714,\n",
    "Train epoch 116, average loss 0.00107182, average accuracy 0.9999,\n",
    "Train epoch 118, average loss 0.00118616, average accuracy 0.99985,\n",
    "Train epoch 120, average loss 0.00116875, average accuracy 0.9998,\n",
    "\tDev epoch 120, average loss 0.456846, average accuracy 0.906082,\n",
    "\t\tTime taken for 120 epochs =  14214.848599672318\n",
    "Train epoch 122, average loss 0.00144058, average accuracy 0.999649,\n",
    "Train epoch 124, average loss 0.00139588, average accuracy 0.999649,\n",
    "\tDev epoch 125, average loss 0.456731, average accuracy 0.90709,\n",
    "Train epoch 126, average loss 0.00129419, average accuracy 0.99975,\n",
    "Train epoch 128, average loss 0.000993939, average accuracy 0.9998,\n",
    "Train epoch 130, average loss 0.00110859, average accuracy 0.99975,\n",
    "\tDev epoch 130, average loss 0.440627, average accuracy 0.905242,\n",
    "\t\tTime taken for 130 epochs =  15387.949309825897\n",
    "Train epoch 132, average loss 0.000869354, average accuracy 0.9998,\n",
    "Train epoch 134, average loss 0.0010678, average accuracy 0.99975,\n",
    "\tDev epoch 135, average loss 0.662642, average accuracy 0.895497,\n",
    "Train epoch 136, average loss 0.00121623, average accuracy 0.99985,\n",
    "Train epoch 138, average loss 0.00106557, average accuracy 0.9998,\n",
    "Train epoch 140, average loss 0.0012005, average accuracy 0.9997,\n",
    "\tDev epoch 140, average loss 0.48323, average accuracy 0.905578,\n",
    "\t\tTime taken for 140 epochs =  16560.915422201157\n",
    "Train epoch 142, average loss 0.00142349, average accuracy 0.999649,\n",
    "Train epoch 144, average loss 0.00100832, average accuracy 0.9998,\n",
    "\tDev epoch 145, average loss 0.469864, average accuracy 0.906082,\n",
    "Train epoch 146, average loss 0.000867766, average accuracy 0.99985,\n",
    "Train epoch 148, average loss 0.000895252, average accuracy 0.9998,\n",
    "Train epoch 150, average loss 0.00128643, average accuracy 0.99975,\n",
    "\tDev epoch 150, average loss 0.504558, average accuracy 0.904738,\n",
    "\t\tTime taken for 150 epochs =  17732.689709424973\n",
    "Train epoch 152, average loss 0.00106975, average accuracy 0.99975,\n",
    "Train epoch 154, average loss 0.000923771, average accuracy 0.9998,\n",
    "\tDev epoch 155, average loss 0.46343, average accuracy 0.904066,\n",
    "    \n",
    "    \n",
    "#### Home and Kitchen, 100000 reviews. 200 epochs, min-documents = 0, embedding size = 100, embeddings trainable = True.\n",
    "\n",
    "completed cnn creation\n",
    "Number batches = 1562\n",
    "Train epoch 0, average loss 0.304591, average accuracy 0.87457,\n",
    "\tDev epoch 0, average loss 0.237596, average accuracy 0.904013,\n",
    "\t\tTime taken for 0 epochs =  36.92328929901123\n",
    "Train epoch 1, average loss 0.223893, average accuracy 0.910071,\n",
    "Train epoch 2, average loss 0.183924, average accuracy 0.927187,\n",
    "Train epoch 3, average loss 0.150811, average accuracy 0.941301,\n",
    "Train epoch 4, average loss 0.123611, average accuracy 0.952775,\n",
    "Train epoch 5, average loss 0.101681, average accuracy 0.961138,\n",
    "\tDev epoch 5, average loss 0.215108, average accuracy 0.917234,\n",
    "Train epoch 6, average loss 0.0824821, average accuracy 0.9694,\n",
    "Train epoch 7, average loss 0.0682877, average accuracy 0.975272,\n",
    "Train epoch 8, average loss 0.0581098, average accuracy 0.978353,\n",
    "Train epoch 9, average loss 0.0481609, average accuracy 0.982815,\n",
    "Train epoch 10, average loss 0.0426059, average accuracy 0.984625,\n",
    "\tDev epoch 10, average loss 0.258906, average accuracy 0.918336,\n",
    "\t\tTime taken for 10 epochs =  381.81759333610535\n",
    "Train epoch 11, average loss 0.0369901, average accuracy 0.986686,\n",
    "Train epoch 12, average loss 0.0348868, average accuracy 0.986946,\n",
    "Train epoch 13, average loss 0.0291087, average accuracy 0.989937,\n",
    "Train epoch 14, average loss 0.0271577, average accuracy 0.990807,\n",
    "Train epoch 15, average loss 0.0248202, average accuracy 0.991487,\n",
    "\tDev epoch 15, average loss 0.317684, average accuracy 0.925114,\n",
    "Train epoch 16, average loss 0.0232966, average accuracy 0.992037,\n",
    "Train epoch 17, average loss 0.0218823, average accuracy 0.992578,\n",
    "Train epoch 18, average loss 0.0189234, average accuracy 0.993878,\n",
    "Train epoch 19, average loss 0.0175714, average accuracy 0.993978,\n",
    "Train epoch 20, average loss 0.0175559, average accuracy 0.994108,\n",
    "\tDev epoch 20, average loss 0.322537, average accuracy 0.924746,\n",
    "\t\tTime taken for 20 epochs =  727.1127679347992\n",
    "Train epoch 21, average loss 0.0159045, average accuracy 0.994638,\n",
    "Train epoch 22, average loss 0.0141158, average accuracy 0.995409,\n",
    "Train epoch 23, average loss 0.0130852, average accuracy 0.995809,\n",
    "Train epoch 24, average loss 0.0131137, average accuracy 0.995709,\n",
    "Train epoch 25, average loss 0.0123618, average accuracy 0.995899,\n",
    "\tDev epoch 25, average loss 0.403685, average accuracy 0.924646,\n",
    "Train epoch 26, average loss 0.0112836, average accuracy 0.996369,\n",
    "Train epoch 27, average loss 0.0111362, average accuracy 0.996449,\n",
    "Train epoch 28, average loss 0.00865663, average accuracy 0.997299,\n",
    "Train epoch 29, average loss 0.00935199, average accuracy 0.996959,\n",
    "Train epoch 30, average loss 0.00935717, average accuracy 0.996699,\n",
    "\tDev epoch 30, average loss 0.367405, average accuracy 0.924579,\n",
    "\t\tTime taken for 30 epochs =  1072.1994183063507\n",
    "Train epoch 31, average loss 0.00909225, average accuracy 0.997149,\n",
    "Train epoch 32, average loss 0.00754316, average accuracy 0.997819,\n",
    "Train epoch 33, average loss 0.0079119, average accuracy 0.997379,\n",
    "Train epoch 34, average loss 0.00705012, average accuracy 0.997719,\n",
    "Train epoch 35, average loss 0.00730037, average accuracy 0.997829,\n",
    "\tDev epoch 35, average loss 0.370859, average accuracy 0.926749,\n",
    "Train epoch 36, average loss 0.00705196, average accuracy 0.997799,\n",
    "Train epoch 37, average loss 0.00669642, average accuracy 0.998039,\n",
    "Train epoch 38, average loss 0.00627199, average accuracy 0.998009,\n",
    "Train epoch 39, average loss 0.00528379, average accuracy 0.998309,\n",
    "Train epoch 40, average loss 0.00593793, average accuracy 0.998229,\n",
    "\tDev epoch 40, average loss 0.383177, average accuracy 0.927618,\n",
    "\t\tTime taken for 40 epochs =  1416.9269080162048\n",
    "Train epoch 41, average loss 0.00519099, average accuracy 0.998289,\n",
    "Train epoch 42, average loss 0.00574083, average accuracy 0.998259,\n",
    "Train epoch 43, average loss 0.00573397, average accuracy 0.998269,\n",
    "Train epoch 44, average loss 0.00478373, average accuracy 0.99844,\n",
    "Train epoch 45, average loss 0.00476654, average accuracy 0.99864,\n",
    "\tDev epoch 45, average loss 0.401872, average accuracy 0.927651,\n",
    "Train epoch 46, average loss 0.00546603, average accuracy 0.998079,\n",
    "Train epoch 47, average loss 0.00552262, average accuracy 0.998299,\n",
    "Train epoch 48, average loss 0.0044039, average accuracy 0.9986,\n",
    "Train epoch 49, average loss 0.00444202, average accuracy 0.99858,\n",
    "Train epoch 50, average loss 0.00553655, average accuracy 0.998239,\n",
    "\tDev epoch 50, average loss 0.410369, average accuracy 0.926182,\n",
    "\t\tTime taken for 50 epochs =  1761.8077738285065\n",
    "Train epoch 51, average loss 0.00446012, average accuracy 0.99862,\n",
    "Train epoch 52, average loss 0.00399219, average accuracy 0.9988,\n",
    "Train epoch 53, average loss 0.00413133, average accuracy 0.99876,\n",
    "Train epoch 54, average loss 0.00465115, average accuracy 0.99857,\n",
    "Train epoch 55, average loss 0.00390704, average accuracy 0.99883,\n",
    "\tDev epoch 55, average loss 0.523764, average accuracy 0.924212,\n",
    "Train epoch 56, average loss 0.00380121, average accuracy 0.99894,\n",
    "Train epoch 57, average loss 0.0041114, average accuracy 0.99863,\n",
    "Train epoch 58, average loss 0.00400388, average accuracy 0.99874,\n",
    "Train epoch 59, average loss 0.00436616, average accuracy 0.99868,\n",
    "Train epoch 60, average loss 0.00410136, average accuracy 0.99879,\n",
    "\tDev epoch 60, average loss 0.465152, average accuracy 0.926115,\n",
    "\t\tTime taken for 60 epochs =  2106.424416542053\n",
    "Train epoch 61, average loss 0.00331209, average accuracy 0.99906,\n",
    "Train epoch 62, average loss 0.00366131, average accuracy 0.99892,\n",
    "Train epoch 63, average loss 0.00425752, average accuracy 0.99884,\n",
    "Train epoch 64, average loss 0.00341706, average accuracy 0.99892,\n",
    "Train epoch 65, average loss 0.00352085, average accuracy 0.99884,\n",
    "\tDev epoch 65, average loss 0.443976, average accuracy 0.919404,\n",
    "Train epoch 66, average loss 0.00392861, average accuracy 0.99874,\n",
    "Train epoch 67, average loss 0.00390108, average accuracy 0.99879,\n",
    "Train epoch 68, average loss 0.00344137, average accuracy 0.99892,\n",
    "Train epoch 69, average loss 0.00282472, average accuracy 0.99918,\n",
    "Train epoch 70, average loss 0.00305406, average accuracy 0.99906,\n",
    "\tDev epoch 70, average loss 0.497239, average accuracy 0.926783,\n",
    "\t\tTime taken for 70 epochs =  2451.336544752121\n",
    "Train epoch 71, average loss 0.00335245, average accuracy 0.99887,\n",
    "Train epoch 72, average loss 0.00321827, average accuracy 0.99912,\n",
    "Train epoch 73, average loss 0.0030572, average accuracy 0.99906,\n",
    "Train epoch 74, average loss 0.00308252, average accuracy 0.99905,\n",
    "Train epoch 75, average loss 0.00312562, average accuracy 0.99909,\n",
    "\tDev epoch 75, average loss 0.454947, average accuracy 0.927818,\n",
    "Train epoch 76, average loss 0.00308124, average accuracy 0.99912,\n",
    "Train epoch 77, average loss 0.00282977, average accuracy 0.9991,\n",
    "Train epoch 78, average loss 0.00250926, average accuracy 0.99929,\n",
    "Train epoch 79, average loss 0.00278992, average accuracy 0.99917,\n",
    "Train epoch 80, average loss 0.00226384, average accuracy 0.99932,\n",
    "\tDev epoch 80, average loss 0.524143, average accuracy 0.925314,\n",
    "\t\tTime taken for 80 epochs =  2796.155880212784\n",
    "Train epoch 81, average loss 0.00289398, average accuracy 0.99907,\n",
    "Train epoch 82, average loss 0.00267882, average accuracy 0.99915,\n",
    "Train epoch 83, average loss 0.00251234, average accuracy 0.99917,\n",
    "Train epoch 84, average loss 0.00212931, average accuracy 0.99929,\n",
    "Train epoch 85, average loss 0.00204477, average accuracy 0.99936,\n",
    "\tDev epoch 85, average loss 0.533493, average accuracy 0.926382,\n",
    "Train epoch 86, average loss 0.00236821, average accuracy 0.99926,\n",
    "Train epoch 87, average loss 0.00237569, average accuracy 0.99921,\n",
    "Train epoch 88, average loss 0.00245643, average accuracy 0.99923,\n",
    "Train epoch 89, average loss 0.00226378, average accuracy 0.99937,\n",
    "Train epoch 90, average loss 0.00233121, average accuracy 0.99925,\n",
    "\tDev epoch 90, average loss 0.58426, average accuracy 0.924112,\n",
    "\t\tTime taken for 90 epochs =  3140.9629440307617\n",
    "Train epoch 91, average loss 0.00239393, average accuracy 0.99934,\n",
    "Train epoch 92, average loss 0.00168514, average accuracy 0.99947,\n",
    "Train epoch 93, average loss 0.00218618, average accuracy 0.99934,\n",
    "Train epoch 94, average loss 0.00201177, average accuracy 0.99939,\n",
    "Train epoch 95, average loss 0.00265143, average accuracy 0.99903,\n",
    "\n",
    "\tDev epoch 95, average loss 0.522781, average accuracy 0.927284,\n",
    "Train epoch 96, average loss 0.00200737, average accuracy 0.99945,\n",
    "Train epoch 97, average loss 0.00167531, average accuracy 0.99956,\n",
    "Train epoch 98, average loss 0.00197691, average accuracy 0.99943,\n",
    "Train epoch 99, average loss 0.00189404, average accuracy 0.99943,\n",
    "Train epoch 100, average loss 0.00217102, average accuracy 0.99935,\n",
    "\tDev epoch 100, average loss 0.509596, average accuracy 0.927417,\n",
    "\t\tTime taken for 100 epochs =  3485.5287368297577\n",
    "Train epoch 101, average loss 0.00214391, average accuracy 0.99932,\n",
    "Train epoch 102, average loss 0.00166687, average accuracy 0.99949,\n",
    "Train epoch 103, average loss 0.00193214, average accuracy 0.99941,\n",
    "Train epoch 104, average loss 0.00230417, average accuracy 0.99928,\n",
    "Train epoch 105, average loss 0.00262318, average accuracy 0.99917,\n",
    "\tDev epoch 105, average loss 0.580702, average accuracy 0.924379,\n",
    "Train epoch 106, average loss 0.00217951, average accuracy 0.99936,\n",
    "Train epoch 107, average loss 0.00196965, average accuracy 0.9994,\n",
    "Train epoch 108, average loss 0.00169252, average accuracy 0.99945,\n",
    "Train epoch 109, average loss 0.00184972, average accuracy 0.99937,\n",
    "Train epoch 110, average loss 0.00203432, average accuracy 0.99938,\n",
    "\tDev epoch 110, average loss 0.580674, average accuracy 0.925214,\n",
    "\t\tTime taken for 110 epochs =  3830.0551614761353\n",
    "Train epoch 111, average loss 0.0018848, average accuracy 0.99941,\n",
    "Train epoch 112, average loss 0.001956, average accuracy 0.99934,\n",
    "Train epoch 113, average loss 0.00164951, average accuracy 0.99948,\n",
    "Train epoch 114, average loss 0.00189435, average accuracy 0.99941,\n",
    "Train epoch 115, average loss 0.00179166, average accuracy 0.99939,\n",
    "\tDev epoch 115, average loss 0.541616, average accuracy 0.926282,\n",
    "Train epoch 116, average loss 0.0018286, average accuracy 0.99944,\n",
    "Train epoch 117, average loss 0.00176589, average accuracy 0.99943,\n",
    "Train epoch 118, average loss 0.00170349, average accuracy 0.99947,\n",
    "Train epoch 119, average loss 0.00222146, average accuracy 0.99928,\n",
    "Train epoch 120, average loss 0.00133639, average accuracy 0.99959,\n",
    "\tDev epoch 120, average loss 0.587092, average accuracy 0.926482,\n",
    "\t\tTime taken for 120 epochs =  4174.599865913391\n",
    "Train epoch 121, average loss 0.00179675, average accuracy 0.99939,\n",
    "Train epoch 122, average loss 0.00170493, average accuracy 0.99948,\n",
    "Train epoch 123, average loss 0.00196213, average accuracy 0.99939,\n",
    "Train epoch 124, average loss 0.00148926, average accuracy 0.99953,\n",
    "Train epoch 125, average loss 0.00146297, average accuracy 0.99951,\n",
    "\tDev epoch 125, average loss 0.555345, average accuracy 0.926516,\n",
    "Train epoch 126, average loss 0.00200696, average accuracy 0.99936,\n",
    "Train epoch 127, average loss 0.00152734, average accuracy 0.99952,\n",
    "Train epoch 128, average loss 0.00232658, average accuracy 0.9993,\n",
    "Train epoch 129, average loss 0.00197241, average accuracy 0.99939,\n",
    "Train epoch 130, average loss 0.00210397, average accuracy 0.99924,\n",
    "\tDev epoch 130, average loss 0.670168, average accuracy 0.92291,\n",
    "\t\tTime taken for 130 epochs =  4519.105709552765\n",
    "Train epoch 131, average loss 0.00184804, average accuracy 0.99943,\n",
    "Train epoch 132, average loss 0.00178672, average accuracy 0.99941,\n",
    "Train epoch 133, average loss 0.00175419, average accuracy 0.9994,\n",
    "Train epoch 134, average loss 0.0019224, average accuracy 0.99934,\n",
    "Train epoch 135, average loss 0.00222945, average accuracy 0.99931,\n",
    "\tDev epoch 135, average loss 0.657096, average accuracy 0.923377,\n",
    "Train epoch 136, average loss 0.00175209, average accuracy 0.99934,\n",
    "Train epoch 137, average loss 0.00214912, average accuracy 0.99925,\n",
    "Train epoch 138, average loss 0.00162636, average accuracy 0.99941,\n",
    "Train epoch 139, average loss 0.00180691, average accuracy 0.99946,\n",
    "Train epoch 140, average loss 0.00118666, average accuracy 0.99959,\n",
    "\tDev epoch 140, average loss 0.575778, average accuracy 0.926015,\n",
    "\t\tTime taken for 140 epochs =  4863.487067222595\n",
    "        \n",
    "\n",
    "#### Same as above, with 60 epochs.\n",
    "\n",
    "completed cnn creation\n",
    "Number batches = 1562\n",
    "Train epoch 0, average loss 0.30547, average accuracy 0.87393,\n",
    "\tDev epoch 0, average loss 0.237115, average accuracy 0.904915,\n",
    "\t\tTime taken for 0 epochs =  36.875508308410645\n",
    "Train epoch 3, average loss 0.152626, average accuracy 0.940681,\n",
    "Train epoch 6, average loss 0.0826888, average accuracy 0.96937,\n",
    "\tDev epoch 6, average loss 0.237618, average accuracy 0.921307,\n",
    "Train epoch 9, average loss 0.0498599, average accuracy 0.982274,\n",
    "Train epoch 12, average loss 0.0340667, average accuracy 0.987906,\n",
    "\tDev epoch 12, average loss 0.290549, average accuracy 0.922075,\n",
    "\t\tTime taken for 12 epochs =  449.51883339881897\n",
    "Train epoch 15, average loss 0.0254497, average accuracy 0.991217,\n",
    "Train epoch 18, average loss 0.0187158, average accuracy 0.993648,\n",
    "\tDev epoch 18, average loss 0.306756, average accuracy 0.923945,\n",
    "Train epoch 21, average loss 0.0152883, average accuracy 0.995038,\n",
    "Train epoch 24, average loss 0.0124834, average accuracy 0.995999,\n",
    "\tDev epoch 24, average loss 0.329978, average accuracy 0.926716,\n",
    "\t\tTime taken for 24 epochs =  862.4385898113251\n",
    "Train epoch 27, average loss 0.0112456, average accuracy 0.996409,\n",
    "Train epoch 30, average loss 0.00972575, average accuracy 0.996939,\n",
    "\tDev epoch 30, average loss 0.361886, average accuracy 0.92695,\n",
    "Train epoch 33, average loss 0.00892223, average accuracy 0.997189,\n",
    "Train epoch 36, average loss 0.00712946, average accuracy 0.997789,\n",
    "\tDev epoch 36, average loss 0.381359, average accuracy 0.92685,\n",
    "\t\tTime taken for 36 epochs =  1275.2902917861938\n",
    "Train epoch 39, average loss 0.00628213, average accuracy 0.998139,\n",
    "Train epoch 42, average loss 0.00596454, average accuracy 0.998159,\n",
    "\tDev epoch 42, average loss 0.47203, average accuracy 0.924446,\n",
    "Train epoch 45, average loss 0.00500223, average accuracy 0.9985,\n",
    "Train epoch 48, average loss 0.00469857, average accuracy 0.99864,\n",
    "\tDev epoch 48, average loss 0.401148, average accuracy 0.926749,\n",
    "\t\tTime taken for 48 epochs =  1688.264419078827\n",
    "Train epoch 51, average loss 0.00402232, average accuracy 0.99886,\n",
    "Train epoch 54, average loss 0.00421826, average accuracy 0.99865,\n",
    "\tDev epoch 54, average loss 0.416046, average accuracy 0.928385,\n",
    "Train epoch 57, average loss 0.00368313, average accuracy 0.99893,\n",
    "Train epoch 60, average loss 0.00335348, average accuracy 0.99901,\n",
    "\tDev epoch 60, average loss 0.438822, average accuracy 0.927918,\n",
    "\t\tTime taken for 60 epochs =  2101.384346008301\n",
    "\n",
    "#### With the embeddings set to trainable = false.\n",
    "completed cnn creation\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.304837, average accuracy 0.87436,\n",
    "\t\tDev epoch 0, average loss 0.239171, average accuracy 0.905649,\n",
    "\t\t\t\t    Time taken for 0 epochs =  33.351370334625244\n",
    "Train epoch 3, average loss 0.159725, average accuracy 0.93797,\n",
    "Train epoch 6, average loss 0.0951332, average accuracy 0.963478,\n",
    "\t\tDev epoch 6, average loss 0.222475, average accuracy 0.916299,\n",
    "Train epoch 9, average loss 0.0597436, average accuracy 0.978053,\n",
    "Train epoch 12, average loss 0.0414887, average accuracy 0.985045,\n",
    "\t\tDev epoch 12, average loss 0.258621, average accuracy 0.921708,\n",
    "\t\t\t\t    Time taken for 12 epochs =  405.9028522968292\n",
    "Train epoch 15, average loss 0.03174, average accuracy 0.988536,\n",
    "Train epoch 18, average loss 0.0268379, average accuracy 0.990747,\n",
    "\t\tDev epoch 18, average loss 0.308509, average accuracy 0.922242,\n",
    "Train epoch 21, average loss 0.0237158, average accuracy 0.991737,\n",
    "Train epoch 24, average loss 0.0182942, average accuracy 0.993528,\n",
    "\t\tDev epoch 24, average loss 0.330288, average accuracy 0.922643,\n",
    "\t\t\t\t    Time taken for 24 epochs =  778.5845618247986\n",
    "Train epoch 27, average loss 0.0170312, average accuracy 0.994198,\n",
    "Train epoch 30, average loss 0.0140965, average accuracy 0.994978,\n",
    "\t\tDev epoch 30, average loss 0.389294, average accuracy 0.922476,\n",
    "Train epoch 33, average loss 0.0119405, average accuracy 0.996029,\n",
    "Train epoch 36, average loss 0.0102918, average accuracy 0.996929,\n",
    "\t\tDev epoch 36, average loss 0.390921, average accuracy 0.922943,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1150.900290966034\n",
    "Train epoch 39, average loss 0.00885309, average accuracy 0.997279,\n",
    "Train epoch 42, average loss 0.0098332, average accuracy 0.996869,\n",
    "\t\tDev epoch 42, average loss 0.408098, average accuracy 0.922476,\n",
    "Train epoch 45, average loss 0.00801163, average accuracy 0.997559,\n",
    "Train epoch 48, average loss 0.0075703, average accuracy 0.997559,\n",
    "\t\tDev epoch 48, average loss 0.405061, average accuracy 0.920339,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1523.076869726181\n",
    "Train epoch 51, average loss 0.00815841, average accuracy 0.997429,\n",
    "Train epoch 54, average loss 0.00750207, average accuracy 0.997439,\n",
    "\t\tDev epoch 54, average loss 0.411605, average accuracy 0.922109,\n",
    "Train epoch 57, average loss 0.00687906, average accuracy 0.997739,\n",
    "Train epoch 60, average loss 0.00724441, average accuracy 0.997749,\n",
    "\t\tDev epoch 60, average loss 0.513384, average accuracy 0.920039,\n",
    "\t\t\t\t    Time taken for 60 epochs =  1895.2125108242035\n",
    "                    \n",
    " #### embedding dim = 50. trainable = tur\n",
    " Writing to /home/ubuntu/W266Project/final_project/runs/cnn\n",
    "\n",
    "completed cnn creation\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.344391, average accuracy 0.857604,\n",
    "\t\tDev epoch 0, average loss 0.289542, average accuracy 0.875501,\n",
    "\t\t\t\t    Time taken for 0 epochs =  32.877453088760376\n",
    "Train epoch 3, average loss 0.205036, average accuracy 0.915813,\n",
    "Train epoch 6, average loss 0.143437, average accuracy 0.943792,\n",
    "\t\tDev epoch 6, average loss 0.245589, average accuracy 0.913261,\n",
    "Train epoch 9, average loss 0.10241, average accuracy 0.960037,\n",
    "Train epoch 12, average loss 0.0766644, average accuracy 0.970851,\n",
    "\t\tDev epoch 12, average loss 0.256715, average accuracy 0.918636,\n",
    "\t\t\t\t    Time taken for 12 epochs =  357.13569045066833\n",
    "Train epoch 15, average loss 0.0596088, average accuracy 0.978213,\n",
    "Train epoch 18, average loss 0.0488154, average accuracy 0.982314,\n",
    "\t\tDev epoch 18, average loss 0.298308, average accuracy 0.917835,\n",
    "Train epoch 21, average loss 0.0426194, average accuracy 0.984715,\n",
    "Train epoch 24, average loss 0.0340587, average accuracy 0.988276,\n",
    "\t\tDev epoch 24, average loss 0.324277, average accuracy 0.920339,\n",
    "\t\t\t\t    Time taken for 24 epochs =  681.0641686916351\n",
    "Train epoch 27, average loss 0.0312987, average accuracy 0.989157,\n",
    "Train epoch 30, average loss 0.0283847, average accuracy 0.990167,\n",
    "\t\tDev epoch 30, average loss 0.374785, average accuracy 0.92261,\n",
    "Train epoch 33, average loss 0.0240973, average accuracy 0.991917,\n",
    "Train epoch 36, average loss 0.0231575, average accuracy 0.992087,\n",
    "\t\tDev epoch 36, average loss 0.372793, average accuracy 0.922175,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1004.7895133495331\n",
    "Train epoch 39, average loss 0.0181793, average accuracy 0.993778,\n",
    "Train epoch 42, average loss 0.0158069, average accuracy 0.994878,\n",
    "\t\tDev epoch 42, average loss 0.394058, average accuracy 0.917535,\n",
    "Train epoch 45, average loss 0.0152652, average accuracy 0.994888,\n",
    "Train epoch 48, average loss 0.0122295, average accuracy 0.995979,\n",
    "\t\tDev epoch 48, average loss 0.415253, average accuracy 0.917601,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1328.6486732959747\n",
    "Train epoch 51, average loss 0.0120298, average accuracy 0.996009,\n",
    "Train epoch 54, average loss 0.0118266, average accuracy 0.996119,\n",
    "\t\tDev epoch 54, average loss 0.413061, average accuracy 0.922443,\n",
    "        \n",
    "#### increased embedding size to 200\n",
    "\n",
    "Train epoch 0, average loss 0.286025, average accuracy 0.882512,\n",
    "\t\tDev epoch 0, average loss 0.238953, average accuracy 0.901142,\n",
    "\t\t\t\t    Time taken for 0 epochs =  110.49707412719727\n",
    "Train epoch 3, average loss 0.115594, average accuracy 0.956656,\n",
    "Train epoch 6, average loss 0.0497285, average accuracy 0.982724,\n",
    "\t\tDev epoch 6, average loss 0.225743, average accuracy 0.929988,\n",
    "Train epoch 9, average loss 0.0273755, average accuracy 0.990967,\n",
    "Train epoch 12, average loss 0.0186573, average accuracy 0.993748,\n",
    "\t\tDev epoch 12, average loss 0.281195, average accuracy 0.931424,\n",
    "\t\t\t\t    Time taken for 12 epochs =  803.1010024547577\n",
    "Train epoch 15, average loss 0.0134854, average accuracy 0.995819,\n",
    "Train epoch 18, average loss 0.0100261, average accuracy 0.997069,\n",
    "\t\tDev epoch 18, average loss 0.315512, average accuracy 0.930188,\n",
    "Train epoch 21, average loss 0.00782281, average accuracy 0.997709,\n",
    "Train epoch 24, average loss 0.00664231, average accuracy 0.998049,\n",
    "\t\tDev epoch 24, average loss 0.325848, average accuracy 0.930288,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1495.02197265625\n",
    "Train epoch 27, average loss 0.0047185, average accuracy 0.9988,\n",
    "Train epoch 30, average loss 0.00426745, average accuracy 0.99881,\n",
    "\t\tDev epoch 30, average loss 0.402758, average accuracy 0.92892,\n",
    "Train epoch 33, average loss 0.00427015, average accuracy 0.99867,\n",
    "Train epoch 36, average loss 0.00313919, average accuracy 0.99917,\n",
    "\t\tDev epoch 36, average loss 0.364247, average accuracy 0.931858,\n",
    "\t\t\t\t    Time taken for 36 epochs =  2186.8154296875\n",
    "Train epoch 39, average loss 0.00310154, average accuracy 0.99915,\n",
    "Train epoch 42, average loss 0.00245393, average accuracy 0.99941,\n",
    "\t\tDev epoch 42, average loss 0.430721, average accuracy 0.930255,\n",
    "        \n",
    "#### decreased embedding size to 100, and reduced number of filters to 128 per filter size.\n",
    "\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.324282, average accuracy 0.864767,\n",
    "\t\tDev epoch 0, average loss 0.247642, average accuracy 0.896368,\n",
    "\t\t\t\t    Time taken for 0 epochs =  31.22071099281311\n",
    "Train epoch 3, average loss 0.170937, average accuracy 0.932518,\n",
    "Train epoch 6, average loss 0.111375, average accuracy 0.956436,\n",
    "\t\tDev epoch 6, average loss 0.215396, average accuracy 0.916533,\n",
    "Train epoch 9, average loss 0.0755983, average accuracy 0.971181,\n",
    "Train epoch 12, average loss 0.0536806, average accuracy 0.980444,\n",
    "\t\tDev epoch 12, average loss 0.272522, average accuracy 0.921541,\n",
    "\t\t\t\t    Time taken for 12 epochs =  300.37999510765076\n",
    "Train epoch 15, average loss 0.0438895, average accuracy 0.983705,\n",
    "Train epoch 18, average loss 0.0348257, average accuracy 0.987456,\n",
    "\t\tDev epoch 18, average loss 0.311513, average accuracy 0.922676,\n",
    "Train epoch 21, average loss 0.0279689, average accuracy 0.990087,\n",
    "Train epoch 24, average loss 0.025247, average accuracy 0.990957,\n",
    "\t\tDev epoch 24, average loss 0.341999, average accuracy 0.923811,\n",
    "\t\t\t\t    Time taken for 24 epochs =  569.6881365776062\n",
    "Train epoch 27, average loss 0.0203997, average accuracy 0.992968,\n",
    "Train epoch 30, average loss 0.0187122, average accuracy 0.993508,\n",
    "\t\tDev epoch 30, average loss 0.357159, average accuracy 0.923811,\n",
    "Train epoch 33, average loss 0.0158869, average accuracy 0.994488,\n",
    "Train epoch 36, average loss 0.0158628, average accuracy 0.994568,\n",
    "\t\tDev epoch 36, average loss 0.371735, average accuracy 0.92311,\n",
    "\t\t\t\t    Time taken for 36 epochs =  838.69158244133\n",
    "Train epoch 39, average loss 0.0133922, average accuracy 0.995659,\n",
    "Train epoch 42, average loss 0.0127672, average accuracy 0.995709,\n",
    "\t\tDev epoch 42, average loss 0.402635, average accuracy 0.92498,\n",
    "Train epoch 45, average loss 0.0119068, average accuracy 0.996049,\n",
    "Train epoch 48, average loss 0.0107565, average accuracy 0.996499,\n",
    "\t\tDev epoch 48, average loss 0.419966, average accuracy 0.92478,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1107.9437527656555\n",
    "Train epoch 51, average loss 0.0100834, average accuracy 0.996689,\n",
    "Train epoch 54, average loss 0.00994073, average accuracy 0.996889,\n",
    "\t\tDev epoch 54, average loss 0.41175, average accuracy 0.924245,\n",
    "Train epoch 57, average loss 0.00979135, average accuracy 0.996649,\n",
    "Train epoch 60, average loss 0.00821928, average accuracy 0.997469,\n",
    "\t\tDev epoch 60, average loss 0.469267, average accuracy 0.926115,\n",
    "\t\t\t\t    Time taken for 60 epochs =  1377.0541887283325\n",
    "                    \n",
    " #### increased number of filters to 512 per filter size.\n",
    " \n",
    " Train epoch 0, average loss 0.311692, average accuracy 0.870288,\n",
    "\t\tDev epoch 0, average loss 0.242969, average accuracy 0.896701,\n",
    "\t\t\t\t    Time taken for 0 epochs =  84.6077299118042\n",
    "Train epoch 3, average loss 0.142796, average accuracy 0.944482,\n",
    "Train epoch 6, average loss 0.0668666, average accuracy 0.975692,\n",
    "\t\tDev epoch 6, average loss 0.210706, average accuracy 0.924913,\n",
    "Train epoch 9, average loss 0.0342145, average accuracy 0.988336,\n",
    "Train epoch 12, average loss 0.0224968, average accuracy 0.992708,\n",
    "\t\tDev epoch 12, average loss 0.274663, average accuracy 0.926916,\n",
    "\t\t\t\t    Time taken for 12 epochs =  809.0257966518402\n",
    "Train epoch 15, average loss 0.0171242, average accuracy 0.994098,\n",
    "Train epoch 18, average loss 0.0118646, average accuracy 0.996329,\n",
    "\t\tDev epoch 18, average loss 0.302414, average accuracy 0.928218,\n",
    "Train epoch 21, average loss 0.0098251, average accuracy 0.996879,\n",
    "Train epoch 24, average loss 0.0067338, average accuracy 0.998029,\n",
    "\t\tDev epoch 24, average loss 0.331184, average accuracy 0.927985,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1532.959394454956\n",
    "Train epoch 27, average loss 0.00506558, average accuracy 0.99858,\n",
    "Train epoch 30, average loss 0.00396653, average accuracy 0.99893,\n",
    "\t\tDev epoch 30, average loss 0.343347, average accuracy 0.928352,\n",
    "Train epoch 33, average loss 0.00384556, average accuracy 0.99898,\n",
    "Train epoch 36, average loss 0.0030804, average accuracy 0.99932,\n",
    "\t\tDev epoch 36, average loss 0.373929, average accuracy 0.928552,\n",
    "\t\t\t\t    Time taken for 36 epochs =  2256.9254744052887\n",
    "Train epoch 39, average loss 0.00246744, average accuracy 0.99933,\n",
    "Train epoch 42, average loss 0.00211266, average accuracy 0.9996,\n",
    "\t\tDev epoch 42, average loss 0.361598, average accuracy 0.929053,\n",
    "Train epoch 45, average loss 0.00217611, average accuracy 0.99941,\n",
    "Train epoch 48, average loss 0.00197936, average accuracy 0.99955,\n",
    "\t\tDev epoch 48, average loss 0.391054, average accuracy 0.929754,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2980.909019947052\n",
    "Train epoch 51, average loss 0.00187027, average accuracy 0.99957,\n",
    "Train epoch 54, average loss 0.00191055, average accuracy 0.99958,\n",
    "\t\tDev epoch 54, average loss 0.378678, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00153142, average accuracy 0.99968,\n",
    "Train epoch 60, average loss 0.00134926, average accuracy 0.99974,\n",
    "\t\tDev epoch 60, average loss 0.405928, average accuracy 0.929854,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3704.6175475120544\n",
    "                    \n",
    "completed cnn creation\n",
    "#### added a filter for length 6. so filter sizes became (3,4,5,6). \n",
    "Train epoch 0, average loss 0.315134, average accuracy 0.868868,\n",
    "\t\tDev epoch 0, average loss 0.248855, average accuracy 0.89353,\n",
    "\t\t\t\t    Time taken for 0 epochs =  69.76286554336548\n",
    "Train epoch 3, average loss 0.149292, average accuracy 0.942482,\n",
    "Train epoch 6, average loss 0.072978, average accuracy 0.973211,\n",
    "\t\tDev epoch 6, average loss 0.220149, average accuracy 0.921908,\n",
    "Train epoch 9, average loss 0.0399216, average accuracy 0.985855,\n",
    "Train epoch 12, average loss 0.0274265, average accuracy 0.990367,\n",
    "\t\tDev epoch 12, average loss 0.321294, average accuracy 0.922342,\n",
    "\t\t\t\t    Time taken for 12 epochs =  666.9912831783295\n",
    "Train epoch 15, average loss 0.0199664, average accuracy 0.993398,\n",
    "Train epoch 18, average loss 0.0145588, average accuracy 0.995429,\n",
    "\t\tDev epoch 18, average loss 0.30958, average accuracy 0.921341,\n",
    "Train epoch 21, average loss 0.011635, average accuracy 0.996449,\n",
    "Train epoch 24, average loss 0.00927037, average accuracy 0.997119,\n",
    "\t\tDev epoch 24, average loss 0.352559, average accuracy 0.926449,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1264.2059795856476\n",
    "Train epoch 27, average loss 0.00758558, average accuracy 0.997789,\n",
    "Train epoch 30, average loss 0.0056179, average accuracy 0.998409,\n",
    "\t\tDev epoch 30, average loss 0.385125, average accuracy 0.926749,\n",
    "Train epoch 33, average loss 0.0049369, average accuracy 0.99856,\n",
    "Train epoch 36, average loss 0.00391196, average accuracy 0.99882,\n",
    "\t\tDev epoch 36, average loss 0.369655, average accuracy 0.926482,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1861.2735829353333\n",
    "Train epoch 39, average loss 0.00425983, average accuracy 0.99884,\n",
    "Train epoch 42, average loss 0.00378442, average accuracy 0.99898,\n",
    "\t\tDev epoch 42, average loss 0.386586, average accuracy 0.926883,\n",
    "Train epoch 45, average loss 0.00346757, average accuracy 0.99902,\n",
    "Train epoch 48, average loss 0.00338043, average accuracy 0.99902,\n",
    "\t\tDev epoch 48, average loss 0.395295, average accuracy 0.927784,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2458.481989145279\n",
    "Train epoch 51, average loss 0.00368297, average accuracy 0.99901,\n",
    "Train epoch 54, average loss 0.00253086, average accuracy 0.99935,\n",
    "\t\tDev epoch 54, average loss 0.410479, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00307247, average accuracy 0.99915,\n",
    "Train epoch 60, average loss 0.00267205, average accuracy 0.99934,\n",
    "\t\tDev epoch 60, average loss 0.449313, average accuracy 0.926215,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3055.6355838775635 \n",
    "                    \n",
    "                    \n",
    "#### With random shuffle added for batches.\n",
    "Train epoch 0, average loss 0.315134, average accuracy 0.868868,\n",
    "\t\tDev epoch 0, average loss 0.248855, average accuracy 0.89353,\n",
    "\t\t\t\t    Time taken for 0 epochs =  69.76286554336548\n",
    "Train epoch 3, average loss 0.149292, average accuracy 0.942482,\n",
    "Train epoch 6, average loss 0.072978, average accuracy 0.973211,\n",
    "\t\tDev epoch 6, average loss 0.220149, average accuracy 0.921908,\n",
    "Train epoch 9, average loss 0.0399216, average accuracy 0.985855,\n",
    "Train epoch 12, average loss 0.0274265, average accuracy 0.990367,\n",
    "\t\tDev epoch 12, average loss 0.321294, average accuracy 0.922342,\n",
    "\t\t\t\t    Time taken for 12 epochs =  666.9912831783295\n",
    "Train epoch 15, average loss 0.0199664, average accuracy 0.993398,\n",
    "Train epoch 18, average loss 0.0145588, average accuracy 0.995429,\n",
    "\t\tDev epoch 18, average loss 0.30958, average accuracy 0.921341,\n",
    "Train epoch 21, average loss 0.011635, average accuracy 0.996449,\n",
    "Train epoch 24, average loss 0.00927037, average accuracy 0.997119,\n",
    "\t\tDev epoch 24, average loss 0.352559, average accuracy 0.926449,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1264.2059795856476\n",
    "Train epoch 27, average loss 0.00758558, average accuracy 0.997789,\n",
    "Train epoch 30, average loss 0.0056179, average accuracy 0.998409,\n",
    "\t\tDev epoch 30, average loss 0.385125, average accuracy 0.926749,\n",
    "Train epoch 33, average loss 0.0049369, average accuracy 0.99856,\n",
    "Train epoch 36, average loss 0.00391196, average accuracy 0.99882,\n",
    "\t\tDev epoch 36, average loss 0.369655, average accuracy 0.926482,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1861.2735829353333\n",
    "Train epoch 39, average loss 0.00425983, average accuracy 0.99884,\n",
    "Train epoch 42, average loss 0.00378442, average accuracy 0.99898,\n",
    "\t\tDev epoch 42, average loss 0.386586, average accuracy 0.926883,\n",
    "Train epoch 45, average loss 0.00346757, average accuracy 0.99902,\n",
    "Train epoch 48, average loss 0.00338043, average accuracy 0.99902,\n",
    "\t\tDev epoch 48, average loss 0.395295, average accuracy 0.927784,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2458.481989145279\n",
    "Train epoch 51, average loss 0.00368297, average accuracy 0.99901,\n",
    "Train epoch 54, average loss 0.00253086, average accuracy 0.99935,\n",
    "\t\tDev epoch 54, average loss 0.410479, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00307247, average accuracy 0.99915,\n",
    "Train epoch 60, average loss 0.00267205, average accuracy 0.99934,\n",
    "\t\tDev epoch 60, average loss 0.449313, average accuracy 0.926215,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3055.6355838775635"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
