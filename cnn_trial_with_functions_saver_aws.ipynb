{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !sudo pip install -U nltk\n",
    "# !sudo pip install wget\n",
    "# !sudo pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3 |Anaconda, Inc.| (default, Oct 13 2017, 12:02:49) \n",
      "[GCC 7.2.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reachanamikasinha/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "# Install a few python packages using pip\n",
    "#from common import utils\n",
    "from w266_common import utils\n",
    "utils.require_package('nltk')\n",
    "utils.require_package(\"wget\")      # for fetching dataset\n",
    "#from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_curve, auc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/reachanamikasinha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard python helper libraries.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys, time\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "from scipy import stats, optimize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "#comment or uncomment based on anamika/ arunima\n",
    "# Helper libraries\n",
    "#from common import utils, vocabulary, glove_helper\n",
    "\n",
    "#from common import utils, vocabulary\n",
    "from w266_common import utils, vocabulary\n",
    "from w266_common import glove_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to read the amazon review data files\n",
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "#df = getDF('reviews_Toys_and_Games.json.gz') #old def function corresponding to the step bt step vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#Using pretrained GLove embeddings\n",
    "hands = glove_helper.Hands(ndim=100)  # 50, 100, 200, 300 dim are available\n",
    "hands.shape\n",
    "print(hands.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please note that i had to comment out the path. Please uncomment before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 111.47875785827637\n",
      "end getDF\n",
      "time taken to load data =  111.47909832000732\n"
     ]
    }
   ],
   "source": [
    "#df_toys = getDF('/newvolume/reviews_Toys_and_Games.json.gz')\n",
    "df_toys = getDF('reviews_Toys_and_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 71.46229863166809\n",
      "end getDF\n",
      "time taken to load data =  71.46305871009827\n"
     ]
    }
   ],
   "source": [
    "#df_vid = getDF('/newvolume/reviews_Video_Games.json.gz')\n",
    "df_vid = getDF('reviews_Video_Games.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 67.347341299057\n",
      "end getDF\n",
      "time taken to load data =  67.34758734703064\n"
     ]
    }
   ],
   "source": [
    "#df_aut = getDF('/newvolume/reviews_Automotive.json.gz')\n",
    "df_aut = getDF('reviews_Automotive.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 208.94064497947693\n",
      "end getDF\n",
      "time taken to load data =  208.9409375190735\n"
     ]
    }
   ],
   "source": [
    "df_hnk = getDF('reviews_Home_and_Kitchen.json.gz')\n",
    "#df_hnk = getDF('/newvolume/reviews_Home_and_Kitchen.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_hnk.to_pickle('./df.hnk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Home and Kitchen reviews examples\n",
      "\n",
      "A210NOCSTBT4OD\n",
      "Have you ever thought about how you met your best friend? Was it normal, or was it wacky - like how Elias met Shohei? Pulling a boa constrictor snake named Mathilda out of your backpack can make a remarkable first impression! This book is about three best friends Elias, Honoria, and Shohei, who are united against \"That Which Is The Peshtigo School\". Their goal is to make it through the annual school science fair, but things don't always go as planned.Elias is part of a family made up of science fanatics who would do anything to win a science fair. Elias isn't exactly what you'd call the ambitious type, especially when it comes to science fairs. So he becomes like Galileo and \"retests\" one of his sibling's past projects. Honoria loves to be ambitious, especially when it comes to being a legal counsel extraordinaire. But when she faces a bigger challenge than beating Goliath Reed or getting a piranha to become vegetarian, she doesn't know if she can make it. Shohei is an all around slacker who tries to mooch off Elias instead of creating something on his own. His adoptive parents are constantly encouraging him to start \"hearing\" his ancestors. His mom has even turned Shohei's room into what looks like a walk-in Japanese museum exhibit!This book is laugh out loud hilarious and the more you read, the more exciting and unexpected it gets. I love the title on this book because it really made me laugh and want to read the book. I also like how people so different from one another can be such close friends. There is not much excitement in the beginning, but it builds up very quickly. So if you like that type of story, then this is the book for you.\n",
      "A28ILV4TOG8BH2\n",
      "The butter dish is serving us well, and keeping the butter fresh and healthy. Couldn't be happier with it, and the color is a pleasing green.\n"
     ]
    }
   ],
   "source": [
    "#Looking at a few examples of review text\n",
    "# print('Toys reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_toys['reviewerID'].iloc[i])\n",
    "#     print(df_toys['reviewText'].iloc[i])\n",
    "\n",
    "# print('\\n Video games reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_vid['reviewerID'].iloc[i])\n",
    "#     print(df_vid['reviewText'].iloc[i])\n",
    "    \n",
    "# print('\\n Automobile reviews examples\\n')\n",
    "# for i in range(1):\n",
    "#     print(df_aut['reviewerID'].iloc[i])\n",
    "#     print(df_aut['reviewText'].iloc[i])\n",
    "    \n",
    "print('\\n Home and Kitchen reviews examples\\n')\n",
    "for i in range(2):\n",
    "    print(df_hnk['reviewerID'].iloc[i])\n",
    "    print(df_hnk['reviewText'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy reviews train, dev and test set dataframe shape: (1351662, 9) (450554, 9) (450555, 9)\n",
      "Video games reviews train, dev and test set dataframe shape: (794851, 9) (264951, 9) (264951, 9)\n",
      "Auto reviews train, dev and test set dataframe shape: (824260, 9) (274754, 9) (274754, 9)\n",
      "Home and Kitchen reviews train, dev and test set dataframe shape: (2552355, 9) (850785, 9) (850786, 9)\n"
     ]
    }
   ],
   "source": [
    "# Create train,dev,test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_toys,devtest = train_test_split(df_toys, test_size=0.4, random_state=42)\n",
    "dev_toys,test_toys = train_test_split(devtest,test_size = 0.5, random_state=42)\n",
    "print('Toy reviews train, dev and test set dataframe shape:',train_toys.shape,dev_toys.shape,test_toys.shape)\n",
    "\n",
    "#For Video games reviews\n",
    "train_vid,devtest = train_test_split(df_vid, test_size=0.4)\n",
    "dev_vid,test_vid = train_test_split(devtest,test_size = 0.5)\n",
    "print('Video games reviews train, dev and test set dataframe shape:',train_vid.shape,dev_vid.shape,test_vid.shape)\n",
    "\n",
    "#For Auto reviews\n",
    "train_aut,devtest = train_test_split(df_aut, test_size=0.4)\n",
    "dev_aut,test_aut = train_test_split(devtest,test_size = 0.5)\n",
    "print('Auto reviews train, dev and test set dataframe shape:',train_aut.shape,dev_aut.shape,test_aut.shape)\n",
    "\n",
    "#For Home and Kitchen reviews\n",
    "train_hnk,devtest = train_test_split(df_hnk, test_size=0.4)\n",
    "dev_hnk,test_hnk = train_test_split(devtest,test_size = 0.5)\n",
    "print('Home and Kitchen reviews train, dev and test set dataframe shape:',train_hnk.shape,dev_hnk.shape,test_hnk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to create a smaller sized train and dev data set. Enables testing accuracy for different sizes.\n",
    "#Also binarizes the labels. Ratings of 1,2 and to 0; Ratings of 4,5 to 1.\n",
    "\n",
    "def set_df_size(size,data_train,data_dev):\n",
    "    size_train = size\n",
    "    len_max_train = data_train[data_train.overall!=3].shape[0] #max possible length of train data set taking out the 3 ratings.\n",
    "    #print(\"Number of reviews with ratings != 3 in train set\",len_max_train)\n",
    "    temp_size_train = min(len_max_train,size_train)\n",
    "\n",
    "    len_max_dev = data_dev[data_dev.overall!=3].shape[0]\n",
    "    #print(\"Number of reviews with ratings != 3 in dev set\",len_max_dev)\n",
    "    temp_size_dev = min(len_max_dev,int(0.3*temp_size_train)) #making the dev set about 0.3 times the train set.\n",
    "\n",
    "    temp_train_data = data_train[data_train.overall != 3][:temp_size_train]\n",
    "    #print('Size of train data',temp_train_data.shape)\n",
    "    #print(temp_train_data.groupby('overall').count())\n",
    "    #print(temp_train_toys[:5])\n",
    "\n",
    "    temp_dev_data = data_dev[data_dev.overall!=3][:temp_size_dev]\n",
    "    #print('Size of dev data',temp_dev_data.shape)\n",
    "    #print(temp_dev_data.groupby('overall').count())\n",
    "    #print(temp_dev_data[:2])\n",
    "    \n",
    "    #Binarize ratings\n",
    "    temp_train_y = np.zeros(temp_size_train)\n",
    "    temp_train_y[temp_train_data.overall > 3] = 1\n",
    "    temp_dev_y = np.zeros(temp_size_dev)\n",
    "    temp_dev_y[temp_dev_data.overall>3] = 1\n",
    "    #print('binarized y shape',temp_train_y.shape,temp_dev_y.shape)\n",
    "    #print(temp_dev_y[:20],data_dev.overall[:20])\n",
    "    return temp_train_data,temp_dev_data,temp_train_y,temp_dev_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] #list of keys that refer to each dataframe. Adding a new dataframe would require updating this list\n",
    "dict_train_df = {} #Dict to store train input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_dev_df = {} #Dict to store dev input data frame for each domain, can be accessed by using domain name as key\n",
    "dict_train_y = {} #Dict to store binarized train data label for each domain\n",
    "dict_dev_y = {} #Dict to store binarized dev data label for each domain\n",
    "#print(len(dict_train_df))\n",
    "\n",
    "def create_sized_data(size = 100000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "    key = list_df[0]\n",
    "    #print('Toys reviews\\n')\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "    #print('\\n Video games reviews\\n')\n",
    "    key = list_df[1]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_vid,dev_vid)\n",
    "    #print('\\n Auto reviews\\n')\n",
    "    key = list_df[2]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_aut,dev_aut)\n",
    "    #print('\\n Home and Kitchen reviews\\n')\n",
    "    key = list_df[3]\n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_hnk,dev_hnk)\n",
    "    \n",
    "create_sized_data(10000)\n",
    "#print(len(dict_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_length = 150\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_processor = tflearn.data_utils.VocabularyProcessor(max_length, min_frequency=0)\n",
    "#Note : Above function was used instead of the below, which is deprecated. \n",
    "# vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_length)\n",
    "\n",
    "def process_inputs(key, vocab_processor):\n",
    "    \n",
    "    # For simplicity, we call our features x and our outputs y\n",
    "    start_vectorize = time.time()\n",
    "    x_train = dict_train_df[key].reviewText\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_df[key].reviewText\n",
    "    y_dev = dict_dev_y[key]\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    # Train the vocab_processor from the training set\n",
    "    x_train = vocab_processor.fit_transform(x_train)\n",
    "    # Transform our test set with the vocabulary processor\n",
    "    x_dev = vocab_processor.transform(x_dev)\n",
    "\n",
    "    # We need these to be np.arrays instead of generators\n",
    "    x_train = np.array(list(x_train))\n",
    "    print(x_train.shape)\n",
    "    x_dev = np.array(list(x_dev))\n",
    "    y_train = np.array(y_train).astype(int)\n",
    "    y_dev = np.array(y_dev).astype(int)\n",
    "    \n",
    "#     y_train = tf.expand_dims(y_train,1)\n",
    "#     y_dev = tf.expand_dims(y_dev,1)\n",
    "    print('y train shape',y_train.shape)\n",
    "\n",
    "    V = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % V)\n",
    "    end_vectorize = time.time()\n",
    "    print('Time taken to vectorize %d size dataframe'%x_train.shape[0],end_vectorize-start_vectorize)\n",
    "\n",
    "    # Return the transformed data and the number of words\n",
    "    return x_train, y_train, x_dev, y_dev, V\n",
    "\n",
    "# x_train, y_train, x_dev, y_dev, V = process_inputs('hnk',vocab_processor)\n",
    "\n",
    "# #Print a few examples for viewing\n",
    "# print('sample review',dict_train_df['hnk']['reviewText'].iloc[3],'\\n')\n",
    "# print('corresponding ids\\n',x_train[3])\n",
    "# print('sample review',dict_dev_df['hnk']['reviewText'].iloc[3],'\\n')\n",
    "# print('corresponding ids\\n',x_dev[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 28336\n",
      "Time taken to vectorize 10000 size dataframe 1.8979973793029785\n",
      "Number words in training corpus for toys 28336\n",
      "toys dataset id shapes (10000, 150) (3000, 150)\n",
      "vid\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 28336\n",
      "Time taken to vectorize 10000 size dataframe 2.9902913570404053\n",
      "Number words in training corpus for vid 28336\n",
      "vid dataset id shapes (10000, 150) (3000, 150)\n",
      "aut\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 28336\n",
      "Time taken to vectorize 10000 size dataframe 1.8524789810180664\n",
      "Number words in training corpus for aut 28336\n",
      "aut dataset id shapes (10000, 150) (3000, 150)\n",
      "hnk\n",
      "(10000,)\n",
      "(10000, 150)\n",
      "y train shape (10000,)\n",
      "Total words: 28336\n",
      "Time taken to vectorize 10000 size dataframe 2.217888832092285\n",
      "Number words in training corpus for hnk 28336\n",
      "hnk dataset id shapes (10000, 150) (3000, 150)\n",
      "sample review It's just the model I was hoping for and more. It's challenging and has given me something more to learn in developing my model building skills. Revell as always makes good models to build! \n",
      "\n",
      "corresponding ids\n",
      " [58  9 48 59 33 50 60 39  4 61 58 62  4 63 64 65 66 61 67 68 69 70 51 59 71\n",
      " 72 73 74 75 76 77 78 67 79  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting reviews to ids for all domains and add padding.\n",
    "\n",
    "#dict_vectorizers = {} #Dict to store the padded tokens  on each domain\n",
    "dict_train_ids = {} #Dict to store train data reviews as sparse matrix of word ids\n",
    "dict_dev_ids = {} #Dict to store dev data reviews as sparse matrix of word ids\n",
    "dict_cnn = {} #Dict to store cnn model developed on each domain. Assumes input features are developed using the corresponding count_vectorizer\n",
    "dict_dev_ypred = {} #Dict to store dev predictions\n",
    "dict_vocab_len = {} #Store vocab length of each domain\n",
    "for key in list_df:\n",
    "    \n",
    "    #Converting ratings to tokenized word id counts as a sparse matrix using count_vectorizer\n",
    "    #dict_vectorizers[key] = CountVectorizer()\n",
    "    print(key)\n",
    "    dict_train_ids[key], dict_train_y[key],dict_dev_ids[key], dict_dev_ypred[key], dict_vocab_len[key] = process_inputs(key,vocab_processor)\n",
    "    \n",
    "#     dict_train_ids[key] = dict_vectorizers[key].fit_transform(dict_train_df[key].reviewText)\n",
    "#     dict_dev_ids[key] = dict_vectorizers[key].transform(dict_dev_df[key].reviewText)\n",
    "    print(\"Number words in training corpus for\",key,(dict_vocab_len[key]))\n",
    "    print(key,'dataset id shapes',dict_train_ids[key].shape, dict_dev_ids[key].shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     #Building a Naive Bayes model to predict the ratings\n",
    "#     dict_nb[key] = MultinomialNB()\n",
    "#     dict_nb[key].fit(dict_train_ids[key],dict_train_y[key])\n",
    "#     dict_dev_ypred[key] = dict_nb[key].predict(dict_dev_ids[key])\n",
    "#     acc = accuracy_score(dict_dev_y[key], dict_dev_ypred[key])\n",
    "#     print(\"Accuracy on\",key,\"dev set for binary prediction with toys naive bayes model: {:.02%}\".format(acc))\n",
    "#     print('Corresponding classification report\\n',classification_report(dict_dev_y[key], dict_dev_ypred[key]))\n",
    "\n",
    "#Print a few examples for viewing\n",
    "print('sample review',dict_train_df['toys'].reviewText.iloc[3],'\\n')\n",
    "print('corresponding ids\\n',dict_train_ids['toys'][3],'\\n')\n",
    "# print('sample review',dict_dev_ids['toys']['reviewText'].iloc[3],'\\n')\n",
    "# print('corresponding ids\\n',x_dev[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(y_train.shape)\n",
    "# print(y_dev.shape)\n",
    "# print(np.mean(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__( self, sequence_length, num_classes, vocab_size, learning_rate, momentum, embedding_size, \n",
    "                 gl_embed, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            #self.W = tf.get_variable(\"W_in\",[vocab_size, embedding_size],initializer =tf.random_uniform_initializer(0,1)) #from wildML\n",
    "            self.W=tf.get_variable(name=\"embedding_\",shape=gl_embed.shape,\n",
    "                                       initializer=tf.constant_initializer(gl_embed),trainable=True)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            #print('embedded_chars',self.embedded_chars.get_shape())\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            #print('embedded_chars_expanded',self.embedded_chars_expanded.get_shape())\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                #W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                Wname = \"w_%d\"%filter_size\n",
    "                W = tf.get_variable(Wname, shape = filter_shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "                b = tf.Variable(tf.constant(0.0, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d( self.embedded_chars_expanded, W, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "\n",
    "                # Apply nonlinearity\n",
    "                conv+= b\n",
    "                h = tf.nn.relu(conv, name=\"relu\")\n",
    "                #print('h',h.get_shape())\n",
    "\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(h, ksize=[1, sequence_length - filter_size + 1, 1, 1], strides=[1, 1, 1, 1],\n",
    "                    padding='VALID', name=\"pool\")\n",
    "                #print('pooled',pooled.get_shape())\n",
    "                pooled_outputs.append(pooled)\n",
    "                #print('pooled_outputs',type(pooled_outputs))\n",
    "                #print('pooled_outputs as array',type(np.array(pooled_outputs)),np.array(pooled_outputs).shape)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        #print('h_pool',self.h_pool.get_shape())\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        #print('h_pool_flat',self.h_pool_flat.get_shape())\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\"W\", shape=[num_filters_total, num_classes],initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            #print('self.scores',self.scores.get_shape())\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            self.pred_proba = tf.nn.softmax(self.scores, name=\"pred_proba\")\n",
    "            #print('self.predictions',self.predictions.get_shape())\n",
    "            \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "            #self.loss = tf.losses.mean_squared_error(self.input_y, self.scores)\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(tf.cast(self.predictions,tf.int32), self.input_y)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "#             correct_pred = tf.equal(tf.cast(tf.round(self.scores), tf.int32), self.input_y)\n",
    "#             self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "        # AUC\n",
    "#         with tf.name_scope(\"auc\"):\n",
    "#             false_pos_rate, true_pos_rate, _ = roc_curve(self.input_y, self.pred_proba[:,1])\n",
    "#             self.auc = auc(false_pos_rate, true_pos_rate)\n",
    "            \n",
    "            \n",
    "        with tf.name_scope('train'):\n",
    "            #self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "            self.optimizer = tf.train.MomentumOptimizer(learning_rate = learning_rate,momentum=momentum,use_nesterov=True).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(ids, labels, batch_size=100):\n",
    "            #ids is input, X_train\n",
    "            #need to fix this to shuffle between epochs\n",
    "    \n",
    "            n_batches = len(ids)//batch_size\n",
    "            ids, labels = ids[:n_batches*batch_size], labels[:n_batches*batch_size]\n",
    "            #if Trainable:\n",
    "            shuffle = np.random.permutation(np.arange(n_batches*batch_size))\n",
    "            ids, labels = ids[shuffle], labels[shuffle]\n",
    "\n",
    "    \n",
    "            for ii in range(0, len(ids), batch_size):\n",
    "                yield ids[ii:ii+batch_size], labels[ii:ii+batch_size]\n",
    "\n",
    "# for ii, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size), 1):\n",
    "#     print(ii,type(x),x.shape,type(np.array(y)),y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "#embed_dim = 50 #use when not using pre-trained embeddings\n",
    "embed_dim = hands.shape[1]\n",
    "filter_sizes= [3,4,5]\n",
    "num_filters = 256\n",
    "l2_reg_lambda = 0\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "keep_prob = 0.8\n",
    "evaluate_train = 3 # of epochs at which to print test accuracy\n",
    "evaluate_dev = 6 # of epochs at which to estimate and print dev accuracy\n",
    "time_print = 12 # of epochs at which to print time taken\n",
    "num_classes = 2\n",
    "num_epochs = 20\n",
    "#num_checkpoints = 2\n",
    "#batch_size = 64\n",
    "batch_size=128\n",
    "#batch_size = 32\n",
    "\n",
    "# out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", \"cnn\"))\n",
    "# print(\"Model saving  to {}\\n\".format(out_dir))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Actual training loop:\n",
    "\n",
    "def train_cnn(key, size=5000):\n",
    "    \n",
    "    \n",
    "    x_train = dict_train_ids[key]\n",
    "    y_train = dict_train_y[key]\n",
    "    x_dev = dict_dev_ids[key]\n",
    "    y_dev = dict_dev_ypred[key]\n",
    "    V = dict_vocab_len[key]\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            #for saving model \n",
    "            #saver = tf.train.Saver()\n",
    "        \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print('completed cnn creation')\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            size_folder =  \"size_\" + str(size) \n",
    "            out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "            #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key))\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            model_name = key \n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, model_name  + \"_model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables())\n",
    "            \n",
    "            # Write vocabulary\n",
    "            ## vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    #avg_auc = total_auc/(batch_size*(len(x_train)//batch_size))\n",
    "                   # print(\"Train epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size), 1):\n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                    \n",
    "                    \n",
    "                    right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                   #Calculate Accuracy\n",
    "                    new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "                     \n",
    "                    \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                    print(\"\\t\\tDev epoch {}, auc {:g}, new accuracy {:g}, right accuracy {:g},\".format(e,  roc_auc, new_acc, right_acc))\n",
    "                    #print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n",
    "                    \n",
    "        # Save model weights for future use.\n",
    "        \n",
    "        \n",
    "            #save_path = saver.save(sess, checkpoint_prefix, global_step=20,write_meta_graph=False)\n",
    "            save_path = saver.save(sess, checkpoint_prefix)\n",
    "            print(\"Saved model\", model_name, save_path)\n",
    "            \n",
    "            #calculate predictions and prediction probability    \n",
    "#             feed_dict={cnn.input_x:x_dev, cnn.input_y: y_dev, cnn.dropout_keep_prob: 1.0}\n",
    "#             y_pred, y_pred_proba = sess.run([cnn.predictions, cnn.pred_proba],feed_dict)\n",
    "            #print(y_pred, y_pred_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please make sure you change the size below so that the source model is saved with that name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_df = ['toys','vid','aut','hnk'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys 10000\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.409325, average accuracy 0.853165,\n",
      "\t\tDev epoch 0, average loss 0.384542, average accuracy 0.857337,\n",
      "\t\tDev epoch 0, auc 0.770855, new accuracy 0.857337, right accuracy 0.857337,\n",
      "\t\t\t\t    Time taken for 0 epochs =  86.57898116111755\n",
      "Train epoch 3, average loss 0.282849, average accuracy 0.881911,\n",
      "Train epoch 6, average loss 0.194367, average accuracy 0.92518,\n",
      "\t\tDev epoch 6, average loss 0.263489, average accuracy 0.900476,\n",
      "\t\tDev epoch 6, auc 0.883425, new accuracy 0.900476, right accuracy 0.900476,\n",
      "Train epoch 9, average loss 0.127527, average accuracy 0.958133,\n",
      "Train epoch 12, average loss 0.0778273, average accuracy 0.982572,\n",
      "\t\tDev epoch 12, average loss 0.273302, average accuracy 0.897418,\n",
      "\t\tDev epoch 12, auc 0.89801, new accuracy 0.897418, right accuracy 0.897418,\n",
      "\t\t\t\t    Time taken for 12 epochs =  1056.6949923038483\n",
      "Train epoch 15, average loss 0.0484964, average accuracy 0.992989,\n",
      "Train epoch 18, average loss 0.0334822, average accuracy 0.996995,\n",
      "\t\tDev epoch 18, average loss 0.287304, average accuracy 0.902174,\n",
      "\t\tDev epoch 18, auc 0.902544, new accuracy 0.902174, right accuracy 0.902174,\n",
      "Saved model toys /home/reachanamikasinha/project/runs/toys/size_5000/checkpoints/toys_model\n",
      "vid 10000\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.478341, average accuracy 0.804087,\n",
      "\t\tDev epoch 0, average loss 0.468162, average accuracy 0.799592,\n",
      "\t\tDev epoch 0, auc 0.730918, new accuracy 0.799592, right accuracy 0.799592,\n",
      "\t\t\t\t    Time taken for 0 epochs =  87.78782224655151\n",
      "Train epoch 3, average loss 0.343996, average accuracy 0.847356,\n",
      "Train epoch 6, average loss 0.243173, average accuracy 0.906951,\n",
      "\t\tDev epoch 6, average loss 0.346404, average accuracy 0.848166,\n",
      "\t\tDev epoch 6, auc 0.861482, new accuracy 0.848166, right accuracy 0.848166,\n",
      "Train epoch 9, average loss 0.157544, average accuracy 0.951723,\n",
      "Train epoch 12, average loss 0.0963278, average accuracy 0.978666,\n",
      "\t\tDev epoch 12, average loss 0.330964, average accuracy 0.866508,\n",
      "\t\tDev epoch 12, auc 0.879219, new accuracy 0.866508, right accuracy 0.866508,\n",
      "\t\t\t\t    Time taken for 12 epochs =  1057.959323644638\n",
      "Train epoch 15, average loss 0.0586111, average accuracy 0.992488,\n",
      "Train epoch 18, average loss 0.0398154, average accuracy 0.996595,\n",
      "\t\tDev epoch 18, average loss 0.343644, average accuracy 0.863451,\n",
      "\t\tDev epoch 18, auc 0.881282, new accuracy 0.863451, right accuracy 0.863451,\n",
      "Saved model vid /home/reachanamikasinha/project/runs/vid/size_5000/checkpoints/vid_model\n",
      "aut 10000\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.41739, average accuracy 0.854067,\n",
      "\t\tDev epoch 0, average loss 0.392438, average accuracy 0.854959,\n",
      "\t\tDev epoch 0, auc 0.740013, new accuracy 0.854959, right accuracy 0.854959,\n",
      "\t\t\t\t    Time taken for 0 epochs =  88.25112724304199\n",
      "Train epoch 3, average loss 0.306672, average accuracy 0.869692,\n",
      "Train epoch 6, average loss 0.215792, average accuracy 0.916967,\n",
      "\t\tDev epoch 6, average loss 0.285091, average accuracy 0.877038,\n",
      "\t\tDev epoch 6, auc 0.873754, new accuracy 0.877038, right accuracy 0.877038,\n",
      "Train epoch 9, average loss 0.144401, average accuracy 0.952023,\n",
      "Train epoch 12, average loss 0.0912355, average accuracy 0.976562,\n",
      "\t\tDev epoch 12, average loss 0.265567, average accuracy 0.895041,\n",
      "\t\tDev epoch 12, auc 0.892603, new accuracy 0.895041, right accuracy 0.895041,\n",
      "\t\t\t\t    Time taken for 12 epochs =  1064.982051372528\n",
      "Train epoch 15, average loss 0.0537227, average accuracy 0.992688,\n",
      "Train epoch 18, average loss 0.0353411, average accuracy 0.996895,\n",
      "\t\tDev epoch 18, average loss 0.300798, average accuracy 0.887228,\n",
      "\t\tDev epoch 18, auc 0.896191, new accuracy 0.887228, right accuracy 0.887228,\n",
      "Saved model aut /home/reachanamikasinha/project/runs/aut/size_5000/checkpoints/aut_model\n",
      "hnk 10000\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.443339, average accuracy 0.831931,\n",
      "\t\tDev epoch 0, average loss 0.404906, average accuracy 0.837976,\n",
      "\t\tDev epoch 0, auc 0.781479, new accuracy 0.837976, right accuracy 0.837976,\n",
      "\t\t\t\t    Time taken for 0 epochs =  87.85708141326904\n",
      "Train epoch 3, average loss 0.301934, average accuracy 0.872296,\n",
      "Train epoch 6, average loss 0.200336, average accuracy 0.92508,\n",
      "\t\tDev epoch 6, average loss 0.267273, average accuracy 0.888247,\n",
      "\t\tDev epoch 6, auc 0.906474, new accuracy 0.888247, right accuracy 0.888247,\n",
      "Train epoch 9, average loss 0.126772, average accuracy 0.96254,\n",
      "Train epoch 12, average loss 0.0799644, average accuracy 0.980669,\n",
      "\t\tDev epoch 12, average loss 0.245084, average accuracy 0.900815,\n",
      "\t\tDev epoch 12, auc 0.918791, new accuracy 0.900815, right accuracy 0.900815,\n",
      "\t\t\t\t    Time taken for 12 epochs =  1063.4684715270996\n",
      "Train epoch 15, average loss 0.0478768, average accuracy 0.992488,\n",
      "Train epoch 18, average loss 0.0339114, average accuracy 0.996695,\n",
      "\t\tDev epoch 18, average loss 0.264597, average accuracy 0.898438,\n",
      "\t\tDev epoch 18, auc 0.921218, new accuracy 0.898438, right accuracy 0.898438,\n",
      "Saved model hnk /home/reachanamikasinha/project/runs/hnk/size_5000/checkpoints/hnk_model\n"
     ]
    }
   ],
   "source": [
    "#Pass the size to save the model name with size in different folders\n",
    "\n",
    "size = 10000\n",
    "for key in list_df:\n",
    "    print(key, size=10000)\n",
    "    train_cnn(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below cell is just for testing. Will be deleted later. Function in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys\n",
      "10000\n",
      "/home/reachanamikasinha/project/runs/toys/size_10000/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/size_10000/checkpoints/toys_model\n",
      "0.903\n",
      "auc 90.32%\n",
      "0.903\n",
      "accuracy 90.30%\n"
     ]
    }
   ],
   "source": [
    "key = 'toys'\n",
    "size =10000\n",
    "batch_size=100\n",
    "print(key)\n",
    "print(size)\n",
    "V = dict_vocab_len[key]\n",
    "\n",
    "\n",
    "size_folder =  \"size_\" + str(size) \n",
    "out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "#out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key))\n",
    "\n",
    "\n",
    "checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "\n",
    "#checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "print(checkpoint_dir)\n",
    "model_name = key\n",
    "\n",
    "graph_meta_file = checkpoint_dir + '/' + 'toys_model.meta'\n",
    "\n",
    "graph=tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "      #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "        new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "        new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "  \n",
    "\n",
    "        \n",
    "    \n",
    "        x_dev = dict_dev_ids[key]\n",
    "        #print(x_dev[0])\n",
    "        #print(dict_dev_ids['vid'][0])\n",
    "        y_dev = dict_dev_ypred[key]\n",
    "        \n",
    "        #print(x_dev)\n",
    "        #print(y_dev)\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "        \n",
    "        pred_proba = graph.get_operation_by_name(\"output/pred_proba\").outputs[0]\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        \n",
    "        y_pred = []\n",
    "        y_pred_proba = []\n",
    "        total_batch_acc = 0\n",
    "        num_batches = 0\n",
    "        y_shuffled = []\n",
    "        for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size), 1):\n",
    "                        \n",
    "                feed_dict = {input_x: x, input_y: y, dropout_keep_prob: 1.0}\n",
    "                batch_pred, batch_pred_proba  = sess.run([ predictions, pred_proba],feed_dict)\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "\n",
    "                num_batches += 1       \n",
    "        #y_pred = np.array(y_pred_list)         \n",
    "              \n",
    "    \n",
    "        new_acc = total_batch_acc/(num_batches)\n",
    "        print(new_acc)\n",
    "        \n",
    "        #Calculate auc\n",
    "#         false_pos_rate, true_pos_rate, _ = roc_curve(y_dev, y_pred_proba[:,1])\n",
    "        false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "        roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "        print(\"auc\",\"{:.02%}\".format(roc_auc))\n",
    "            \n",
    "        #Calculate Accuracy\n",
    "        acc = accuracy_score(y_shuffled, y_pred, normalize=True )\n",
    "        print(np.sum(y_shuffled==y_pred)/y_pred.shape[0])\n",
    "        print(\"accuracy\",\"{:.02%}\".format(acc))\n",
    "        \n",
    "        \n",
    "        #print(y_pred)\n",
    "        #print(y_pred_proba)\n",
    "       \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_accuracy(src_key, size, tar_key):\n",
    "    \n",
    "    batch_size=128\n",
    "    print('target',tar_key)\n",
    "    V = dict_vocab_len[tar_key]\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "\n",
    "    \n",
    "    print(checkpoint_dir)\n",
    "    src_model = src_key\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "    graph_meta_file = checkpoint_dir + '/' + src_model +'01_model.meta'\n",
    "    graph=tf.Graph()\n",
    "\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "    \n",
    "      #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "            new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "            new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "  \n",
    "\n",
    "        \n",
    "    \n",
    "            x_dev = dict_dev_ids[tar_key]\n",
    "        \n",
    "            y_dev = dict_dev_ypred[tar_key]\n",
    "        \n",
    "            #create graph from saved model\n",
    "            input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "            input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "            dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "        \n",
    "            pred_proba = graph.get_operation_by_name(\"output/pred_proba\").outputs[0]\n",
    "            predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "        \n",
    "            y_pred = []\n",
    "            y_pred_proba = []\n",
    "            total_batch_acc = 0\n",
    "            num_batches = 0\n",
    "            y_shuffled = []\n",
    "            for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size), 1):\n",
    "                        \n",
    "                feed_dict = {input_x: x, input_y: y, dropout_keep_prob: 1.0}\n",
    "                batch_pred, batch_pred_proba  = sess.run([ predictions, pred_proba],feed_dict)\n",
    "                batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                total_batch_acc += batch_accuracy\n",
    "                y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                y_shuffled = np.concatenate([y_shuffled, y])\n",
    "\n",
    "                num_batches += 1       \n",
    "        #y_pred = np.array(y_pred_list)         \n",
    "              \n",
    "    \n",
    "            new_acc = total_batch_acc/(num_batches)\n",
    "            print(new_acc)\n",
    "        \n",
    "            #Calculate auc\n",
    "#         false_pos_rate, true_pos_rate, _ = roc_curve(y_dev, y_pred_proba[:,1])\n",
    "            false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "            roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "            print(src_key, tar_key, \"AUC\",\"{:.02%}\".format(roc_auc))\n",
    "            \n",
    "        #Calculate Accuracy\n",
    "            acc = accuracy_score(y_shuffled, y_pred, normalize=True )\n",
    "            #print(np.sum(y_shuffled==y_pred)/y_pred.shape[0])\n",
    "            print(src_key, tar_key, \"accuracy\",\"{:.02%}\".format(acc))\n",
    "            print(\"\")\n",
    "        \n",
    "        \n",
    "        \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target toys\n",
      "/home/reachanamikasinha/project/runs/toys/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/checkpoints/toys01_model\n",
      "0.889914772727\n",
      "toys toys AUC 89.89%\n",
      "toys toys accuracy 88.99%\n",
      "\n",
      "target vid\n",
      "/home/reachanamikasinha/project/runs/toys/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/checkpoints/toys01_model\n",
      "0.814630681818\n",
      "toys vid AUC 81.24%\n",
      "toys vid accuracy 81.46%\n",
      "\n",
      "target aut\n",
      "/home/reachanamikasinha/project/runs/toys/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/checkpoints/toys01_model\n",
      "0.8671875\n",
      "toys aut AUC 77.19%\n",
      "toys aut accuracy 86.72%\n",
      "\n",
      "target hnk\n",
      "/home/reachanamikasinha/project/runs/toys/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/checkpoints/toys01_model\n",
      "0.857954545455\n",
      "toys hnk AUC 85.83%\n",
      "toys hnk accuracy 85.80%\n",
      "\n",
      "target toys\n",
      "/home/reachanamikasinha/project/runs/vid/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/vid/checkpoints/vid01_model\n",
      "0.891335227273\n",
      "vid toys AUC 86.88%\n",
      "vid toys accuracy 89.13%\n",
      "\n",
      "target vid\n",
      "/home/reachanamikasinha/project/runs/vid/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/vid/checkpoints/vid01_model\n",
      "0.852272727273\n",
      "vid vid AUC 86.35%\n",
      "vid vid accuracy 85.23%\n",
      "\n",
      "target aut\n",
      "/home/reachanamikasinha/project/runs/vid/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/vid/checkpoints/vid01_model\n",
      "0.833806818182\n",
      "vid aut AUC 78.17%\n",
      "vid aut accuracy 83.38%\n",
      "\n",
      "target hnk\n",
      "/home/reachanamikasinha/project/runs/vid/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/vid/checkpoints/vid01_model\n",
      "0.853693181818\n",
      "vid hnk AUC 87.40%\n",
      "vid hnk accuracy 85.37%\n",
      "\n",
      "target toys\n",
      "/home/reachanamikasinha/project/runs/aut/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/aut/checkpoints/aut01_model\n",
      "0.884943181818\n",
      "aut toys AUC 83.81%\n",
      "aut toys accuracy 88.49%\n",
      "\n",
      "target vid\n",
      "/home/reachanamikasinha/project/runs/aut/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/aut/checkpoints/aut01_model\n",
      "0.8046875\n",
      "aut vid AUC 79.31%\n",
      "aut vid accuracy 80.47%\n",
      "\n",
      "target aut\n",
      "/home/reachanamikasinha/project/runs/aut/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/aut/checkpoints/aut01_model\n",
      "0.882102272727\n",
      "aut aut AUC 85.17%\n",
      "aut aut accuracy 88.21%\n",
      "\n",
      "target hnk\n",
      "/home/reachanamikasinha/project/runs/aut/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/aut/checkpoints/aut01_model\n",
      "0.853693181818\n",
      "aut hnk AUC 87.89%\n",
      "aut hnk accuracy 85.37%\n",
      "\n",
      "target toys\n",
      "/home/reachanamikasinha/project/runs/hnk/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/hnk/checkpoints/hnk01_model\n",
      "0.894176136364\n",
      "hnk toys AUC 87.96%\n",
      "hnk toys accuracy 89.42%\n",
      "\n",
      "target vid\n",
      "/home/reachanamikasinha/project/runs/hnk/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/hnk/checkpoints/hnk01_model\n",
      "0.813210227273\n",
      "hnk vid AUC 81.16%\n",
      "hnk vid accuracy 81.32%\n",
      "\n",
      "target aut\n",
      "/home/reachanamikasinha/project/runs/hnk/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/hnk/checkpoints/hnk01_model\n",
      "0.8828125\n",
      "hnk aut AUC 83.56%\n",
      "hnk aut accuracy 88.28%\n",
      "\n",
      "target hnk\n",
      "/home/reachanamikasinha/project/runs/hnk/checkpoints\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/hnk/checkpoints/hnk01_model\n",
      "0.872869318182\n",
      "hnk hnk AUC 91.00%\n",
      "hnk hnk accuracy 87.29%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s_key in list_df:\n",
    "    for t_key in list_df:\n",
    "        predict_accuracy(s_key, t_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toys\n",
      "completed cnn creation\n",
      "# batches = 78\n",
      "Train epoch 0, average loss 0.410732, average accuracy 0.84986,\n",
      "\t\tDev epoch 0, average loss 0.384446, average accuracy 0.857337,\n",
      "\t\tDev epoch 0, auc 0.761469, new accuracy 0.857337, right accuracy 0.857337,\n",
      "\t\t\t\t    Time taken for 0 epochs =  85.97571444511414\n",
      "Train epoch 3, average loss 0.278746, average accuracy 0.882512,\n",
      "Train epoch 6, average loss 0.190353, average accuracy 0.92508,\n",
      "\t\tDev epoch 6, average loss 0.267068, average accuracy 0.892323,\n",
      "\t\tDev epoch 6, auc 0.884168, new accuracy 0.892323, right accuracy 0.892323,\n",
      "Train epoch 9, average loss 0.120366, average accuracy 0.963241,\n",
      "Train epoch 12, average loss 0.0750643, average accuracy 0.983874,\n",
      "\t\tDev epoch 12, average loss 0.250142, average accuracy 0.90591,\n",
      "\t\tDev epoch 12, auc 0.898789, new accuracy 0.90591, right accuracy 0.90591,\n",
      "\t\t\t\t    Time taken for 12 epochs =  1057.8321163654327\n",
      "Train epoch 15, average loss 0.0461413, average accuracy 0.995292,\n",
      "Saved model toys /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n"
     ]
    }
   ],
   "source": [
    "#Testing only on toys\n",
    "list_df = ['toys',10000]\n",
    "for key in list_df:\n",
    "    print(key)\n",
    "    train_cnn(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_data(key,size = 5000):\n",
    "    size_train = size #Set size of train set here. This is a hyperparameter.\n",
    "    #key = list_df[0]\n",
    "    #print('Toys reviews\\n')\n",
    "    \n",
    "    dict_train_df[key], dict_dev_df[key], dict_train_y[key], dict_dev_y[key] = set_df_size(size_train,train_toys,dev_toys)\n",
    "    dict_train_ids[key], dict_train_y[key],dict_dev_ids[key], dict_dev_ypred[key], dict_vocab_len[key] = process_inputs(key,vocab_processor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "(2000, 150)\n",
      "y train shape (2000,)\n",
      "Total words: 28336\n",
      "Time taken to vectorize 2000 size dataframe 0.3338496685028076\n"
     ]
    }
   ],
   "source": [
    "get_target_data('vid',2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "\n",
    "#embed_dim = 50 #use when not using pre-trained embeddings\n",
    "embed_dim = hands.shape[1]\n",
    "filter_sizes= [3,4,5]\n",
    "num_filters = 256\n",
    "l2_reg_lambda = 0\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "keep_prob = 0.8\n",
    "evaluate_train = 3 # of epochs at which to print train accuracy\n",
    "evaluate_dev = 6 # of epochs at which to estimate and print dev accuracy\n",
    "time_print = 12 # of epochs at which to print time taken\n",
    "num_classes = 2\n",
    "num_epochs = 24\n",
    "#num_checkpoints = 2\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#size is needed to get the right source domain file to load\n",
    "def continue_train(src_key, size, tar_key):\n",
    "    \n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", src_key))\n",
    "    #out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"testruns\", src_key))\n",
    "    \n",
    "    size_folder =  \"size_\" + str(size) \n",
    "    out_dir  = os.path.abspath(os.path.join(os.path.curdir, \"runs\", key, size_folder))\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "\n",
    "    \n",
    "    print(checkpoint_dir)\n",
    "    src_model = src_key\n",
    "    #graph_meta_file = checkpoint_dir + '/' + 'hnk01_model.meta'\n",
    "#     graph_meta_file = checkpoint_dir + '/' + src_model +'01_model.meta'\n",
    "    graph=tf.Graph()\n",
    "    \n",
    "    x_train = dict_train_ids[tar_key]\n",
    "    y_train = dict_train_y[tar_key]\n",
    "    x_dev = dict_dev_ids[tar_key]\n",
    "    y_dev = dict_dev_ypred[tar_key]\n",
    "    V = dict_vocab_len[tar_key]\n",
    "    \n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            \n",
    "            cnn = TextCNN(sequence_length=x_train.shape[1], num_classes=num_classes, vocab_size=V, learning_rate = learning_rate,\n",
    "                        momentum = momentum, embedding_size=embed_dim, gl_embed = hands.W, filter_sizes= filter_sizes, \n",
    "                      num_filters=num_filters, l2_reg_lambda=l2_reg_lambda)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "          #new_saver = tf.train.import_meta_graph(checkpoint_dir/'hnk_model.meta')\n",
    "#             new_saver = tf.train.import_meta_graph(graph_meta_file)\n",
    "#             new_saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir))\n",
    "            \n",
    "            \n",
    "            #initializing weights from a previous session \n",
    "            initialising_model = src_model+'_model'\n",
    "            print(\" RESTORING SESSION FOR WEIGHTS INITIALIZATION\")\n",
    "            # Exclude output layer weights from variables we will restore\n",
    "            variables_to_restore = [v for v in tf.global_variables()]\n",
    "            # Replace variables scope with that of the current model\n",
    "            loader = tf.train.Saver({v.op.name.replace(src_model, initialising_model): v for v in variables_to_restore})\n",
    "            load_path = checkpoint_dir + '/' + initialising_model \n",
    "            #load_path = checkpoint_dir  \n",
    "            loader.restore(sess, load_path)\n",
    "            print(\" Model loaded from: \" + load_path)\n",
    "  \n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "            print('# batches =', len(x_train)//batch_size)\n",
    "            start = time.time()\n",
    "            for e in range(num_epochs):\n",
    "                    \n",
    "                #sum_scores = np.zeros((batch_size*(len(x_train)//batch_size),1))\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                total_auc = 0\n",
    "                for i, (x, y) in enumerate(batch_generator(x_train, y_train, batch_size), 1):\n",
    "                    feed = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: keep_prob}\n",
    "                   # _, loss, accuracy, auc = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy, cnn.auc],feed_dict = feed)\n",
    "                    _, loss, accuracy = sess.run([cnn.optimizer,cnn.loss, cnn.accuracy],feed_dict = feed)\n",
    "                    total_loss += loss*len(x)\n",
    "                    total_acc += accuracy*len(x)\n",
    "                    \n",
    "                    #total_auc += auc*len(x)\n",
    "                    \n",
    "                if e%evaluate_train==0:\n",
    "                    avg_loss = total_loss/(batch_size*(len(x_train)//batch_size))\n",
    "                    avg_acc = total_acc/(batch_size*(len(x_train)//batch_size))\n",
    "                    #avg_auc = total_auc/(batch_size*(len(x_train)//batch_size))\n",
    "                   # print(\"Train epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                    print(\"Train epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "\n",
    "                if e%evaluate_dev==0:\n",
    "                    \n",
    "                    total_loss = 0\n",
    "                    total_acc = 0\n",
    "                    num_batches = 0\n",
    "                    total_auc = 0\n",
    "                    y_pred = []\n",
    "                    y_pred_proba = []\n",
    "                    y_shuffled = []\n",
    "                    total_batch_acc = 0\n",
    "                    for ii, (x, y) in enumerate(batch_generator(x_dev, y_dev, batch_size), 1):\n",
    "                        feed_dict = {cnn.input_x: x, cnn.input_y: y, cnn.dropout_keep_prob: 1.0}\n",
    "                        #loss, accuracy, auc = sess.run([cnn.loss, cnn.accuracy, cnn.auc],feed_dict)\n",
    "                       # batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        batch_pred,batch_pred_proba,loss, accuracy  = sess.run([cnn.predictions, cnn.pred_proba, cnn.loss, cnn.accuracy],feed_dict)\n",
    "                        total_loss += loss*len(x)\n",
    "                        total_acc += accuracy*len(x)\n",
    "                        \n",
    "                        batch_accuracy= np.sum(y==batch_pred)/y.shape[0]\n",
    "                        total_batch_acc += batch_accuracy\n",
    "                        y_pred= np.concatenate([y_pred, batch_pred])\n",
    "                        y_pred_proba= np.concatenate([y_pred_proba, batch_pred_proba[:,1]])\n",
    "                        y_shuffled = np.concatenate([y_shuffled, y])\n",
    "                        \n",
    "                        num_batches += 1\n",
    "                        \n",
    "                    avg_loss = total_loss/(num_batches*batch_size)\n",
    "                    avg_acc = total_acc/(num_batches*batch_size)\n",
    "                    \n",
    "                    \n",
    "                    right_acc = total_batch_acc/(num_batches)\n",
    "                    #avg_auc = total_auc/(num_batches*batch_size)\n",
    "                    \n",
    "                   #Calculate Accuracy\n",
    "                    new_acc = accuracy_score(y_shuffled, y_pred, normalize=True ) \n",
    "                     \n",
    "                    \n",
    "                    false_pos_rate, true_pos_rate, _ = roc_curve(y_shuffled, y_pred_proba)  \n",
    "                    roc_auc = auc(false_pos_rate, true_pos_rate)\n",
    "                    \n",
    "                #time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},\".format(e, avg_loss, avg_acc))\n",
    "                    print(\"\\t\\tDev epoch {}, auc {:g}, new accuracy {:g}, right accuracy {:g},\".format(e,  roc_auc, new_acc, right_acc))\n",
    "                    #print(\"\\t\\tDev epoch {}, average loss {:g}, average accuracy {:g},average auc {:g}\".format(e, avg_loss, avg_acc, avg_auc))\n",
    "                if e%time_print == 0:\n",
    "                    end = time.time()\n",
    "                    print(\"\\t\\t\\t\\t    Time taken for\",e,\"epochs = \", end-start)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/reachanamikasinha/project/runs/toys/size_10000/checkpoints\n",
      " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
      "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/size_10000/checkpoints/toys_model\n",
      " Model loaded from: /home/reachanamikasinha/project/runs/toys/size_10000/checkpoints/toys_model\n",
      "# batches = 15\n",
      "Train epoch 0, average loss 0.0276176, average accuracy 0.996875,\n",
      "\t\tDev epoch 0, average loss 0.232405, average accuracy 0.910156,\n",
      "\t\tDev epoch 0, auc 0.919749, new accuracy 0.910156, right accuracy 0.910156,\n",
      "\t\t\t\t    Time taken for 0 epochs =  15.348036289215088\n",
      "Train epoch 3, average loss 0.0183418, average accuracy 1,\n",
      "Train epoch 6, average loss 0.0133835, average accuracy 1,\n",
      "\t\tDev epoch 6, average loss 0.238629, average accuracy 0.908203,\n",
      "\t\tDev epoch 6, auc 0.9181, new accuracy 0.908203, right accuracy 0.908203,\n",
      "Train epoch 9, average loss 0.0105111, average accuracy 1,\n",
      "Train epoch 12, average loss 0.00948409, average accuracy 1,\n",
      "\t\tDev epoch 12, average loss 0.229321, average accuracy 0.914062,\n",
      "\t\tDev epoch 12, auc 0.915269, new accuracy 0.914062, right accuracy 0.914062,\n",
      "\t\t\t\t    Time taken for 12 epochs =  200.676424741745\n",
      "Train epoch 15, average loss 0.00868266, average accuracy 1,\n",
      "Train epoch 18, average loss 0.00785425, average accuracy 1,\n",
      "\t\tDev epoch 18, average loss 0.249066, average accuracy 0.908203,\n",
      "\t\tDev epoch 18, auc 0.915878, new accuracy 0.908203, right accuracy 0.908203,\n",
      "Train epoch 21, average loss 0.006944, average accuracy 1,\n"
     ]
    }
   ],
   "source": [
    "# remeber to cal get_target_data('vid',2000) with the target key and size you want to add to source domain\n",
    "#then call continue_train with size of src domain file u want to load\n",
    "continue_train(\"toys\", 10000, \"vid\")\n",
    "\n",
    "#for comparison before train toys vid AUC 81.24%\n",
    "#toys vid accuracy 81.46%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train results \n",
    "\n",
    "toys(10000)\n",
    "completed cnn creation\n",
    "# batches = 78\n",
    "Train epoch 0, average loss 0.410732, average accuracy 0.84986,\n",
    "\t\tDev epoch 0, average loss 0.384446, average accuracy 0.857337,\n",
    "\t\tDev epoch 0, auc 0.761469, new accuracy 0.857337, right accuracy 0.857337,\n",
    "\t\t\t\t    Time taken for 0 epochs =  85.97571444511414\n",
    "Train epoch 3, average loss 0.278746, average accuracy 0.882512,\n",
    "Train epoch 6, average loss 0.190353, average accuracy 0.92508,\n",
    "\t\tDev epoch 6, average loss 0.267068, average accuracy 0.892323,\n",
    "\t\tDev epoch 6, auc 0.884168, new accuracy 0.892323, right accuracy 0.892323,\n",
    "Train epoch 9, average loss 0.120366, average accuracy 0.963241,\n",
    "Train epoch 12, average loss 0.0750643, average accuracy 0.983874,\n",
    "\t\tDev epoch 12, average loss 0.250142, average accuracy 0.90591,\n",
    "\t\tDev epoch 12, auc 0.898789, new accuracy 0.90591, right accuracy 0.90591,\n",
    "\t\t\t\t    Time taken for 12 epochs =  1057.8321163654327\n",
    "Train epoch 15, average loss 0.0461413, average accuracy 0.995292,\n",
    "Saved model toys /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    "\n",
    "Continue train \n",
    "toys(2000)\n",
    "# batches = 15\n",
    "Train epoch 0, average loss 0.0328055, average accuracy 0.998958,\n",
    "\t\tDev epoch 0, average loss 0.253235, average accuracy 0.900391,\n",
    "\t\tDev epoch 0, auc 0.902401, new accuracy 0.900391, right accuracy 0.900391,\n",
    "\t\t\t\t    Time taken for 0 epochs =  15.97208309173584\n",
    "Train epoch 3, average loss 0.0209501, average accuracy 1,\n",
    "Train epoch 6, average loss 0.0157637, average accuracy 1,\n",
    "\t\tDev epoch 6, average loss 0.246794, average accuracy 0.910156,\n",
    "\t\tDev epoch 6, auc 0.904086, new accuracy 0.910156, right accuracy 0.910156,\n",
    "Train epoch 9, average loss 0.0133948, average accuracy 0.999479,\n",
    "Train epoch 12, average loss 0.0110164, average accuracy 1,\n",
    "\t\tDev epoch 12, average loss 0.268596, average accuracy 0.900391,\n",
    "\t\tDev epoch 12, auc 0.903477, new accuracy 0.900391, right accuracy 0.900391,\n",
    "\t\t\t\t    Time taken for 12 epochs =  203.32078433036804\n",
    "Train epoch 15, average loss 0.00984509, average accuracy 1,\n",
    "\n",
    "\n",
    "Continue train \n",
    "toys(4000)\n",
    "/home/reachanamikasinha/project/testruns/toys/checkpoints\n",
    " RESTORING SESSION FOR WEIGHTS INITIALIZATION\n",
    "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    " Model loaded from: /home/reachanamikasinha/project/testruns/toys/checkpoints/toys01_model\n",
    "# batches = 31\n",
    "Train epoch 0, average loss 0.0316445, average accuracy 0.997732,\n",
    "\t\tDev epoch 0, average loss 0.238046, average accuracy 0.911458,\n",
    "\t\tDev epoch 0, auc 0.915253, new accuracy 0.911458, right accuracy 0.911458,\n",
    "\t\t\t\t    Time taken for 0 epochs =  33.795907497406006\n",
    "Train epoch 3, average loss 0.021633, average accuracy 0.999748,\n",
    "Train epoch 6, average loss 0.0158767, average accuracy 0.999496,\n",
    "\t\tDev epoch 6, average loss 0.272812, average accuracy 0.903646,\n",
    "\t\tDev epoch 6, auc 0.915281, new accuracy 0.903646, right accuracy 0.903646,\n",
    "Train epoch 9, average loss 0.0130432, average accuracy 0.999748,\n",
    "Train epoch 12, average loss 0.0109889, average accuracy 1,\n",
    "\t\tDev epoch 12, average loss 0.275246, average accuracy 0.908854,\n",
    "\t\tDev epoch 12, auc 0.91456, new accuracy 0.908854, right accuracy 0.908854,\n",
    "\t\t\t\t    Time taken for 12 epochs =  418.0527718067169\n",
    "Train epoch 15, average loss 0.0100661, average accuracy 0.999748,\n",
    "Train epoch 18, average loss 0.00892393, average accuracy 1,\n",
    "\t\tDev epoch 18, average loss 0.276631, average accuracy 0.907986,\n",
    "\t\tDev epoch 18, auc 0.915397, new accuracy 0.907986, right accuracy 0.907986,\n",
    "Train epoch 21, average loss 0.00748347, average accuracy 1,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of transfer learning testing\n",
    "\n",
    "Comparison\n",
    "target vid\n",
    "/home/reachanamikasinha/project/runs/toys/checkpoints\n",
    "INFO:tensorflow:Restoring parameters from /home/reachanamikasinha/project/runs/toys/checkpoints/toys01_model\n",
    "0.814630681818\n",
    "toys vid AUC 81.24%\n",
    "toys vid accuracy 81.46%\n",
    "\n",
    "Transfer accuracy source toys, target vid(source trained on 10,000 and \n",
    "auc 0.893548, new accuracy 0.898438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving output of model saved run to be able to compare dev accuracy\n",
    "\n",
    "# toys\n",
    "\n",
    "completed cnn creation\n",
    "#batches = 1562\n",
    "Train epoch 0, average loss 0.281697, average accuracy 0.886964,\n",
    "\t\tDev epoch 0, average loss 0.244123, average accuracy 0.908854,\n",
    "\t\t\t\t    Time taken for 0 epochs =  37.60847544670105\n",
    "Train epoch 3, average loss 0.142607, average accuracy 0.944032,\n",
    "Train epoch 6, average loss 0.0771984, average accuracy 0.971111,\n",
    "\t\tDev epoch 6, average loss 0.234148, average accuracy 0.925581,\n",
    "Train epoch 9, average loss 0.0413356, average accuracy 0.985665,\n",
    "Train epoch 12, average loss 0.0262069, average accuracy 0.991207,\n",
    "\t\tDev epoch 12, average loss 0.259911, average accuracy 0.931858,\n",
    "\t\t\t\t    Time taken for 12 epochs =  458.23175573349\n",
    "Train epoch 15, average loss 0.0191853, average accuracy 0.994008,\n",
    "Train epoch 18, average loss 0.0141925, average accuracy 0.995569,\n",
    "\t\tDev epoch 18, average loss 0.297451, average accuracy 0.932759,\n",
    "Train epoch 21, average loss 0.0111091, average accuracy 0.996669,\n",
    "Train epoch 24, average loss 0.00874389, average accuracy 0.997289,\n",
    "\t\tDev epoch 24, average loss 0.37409, average accuracy 0.930889,\n",
    "\t\t\t\t    Time taken for 24 epochs =  879.0003838539124\n",
    "Train epoch 27, average loss 0.00864632, average accuracy 0.997309,\n",
    "Train epoch 30, average loss 0.00784798, average accuracy 0.997459,\n",
    "\t\tDev epoch 30, average loss 0.358449, average accuracy 0.932192,\n",
    "Train epoch 33, average loss 0.00659365, average accuracy 0.997819,\n",
    "Train epoch 36, average loss 0.00578367, average accuracy 0.998349,\n",
    "\t\tDev epoch 36, average loss 0.373084, average accuracy 0.931958,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1299.6440062522888\n",
    "Train epoch 39, average loss 0.0064537, average accuracy 0.998089,\n",
    "Train epoch 42, average loss 0.00580202, average accuracy 0.998259,\n",
    "\t\tDev epoch 42, average loss 0.391404, average accuracy 0.933393,\n",
    "Train epoch 45, average loss 0.00514404, average accuracy 0.99845,\n",
    "Train epoch 48, average loss 0.00371194, average accuracy 0.99892,\n",
    "\t\tDev epoch 48, average loss 0.469131, average accuracy 0.931457,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1720.3404235839844\n",
    "Train epoch 51, average loss 0.0041625, average accuracy 0.99878,\n",
    "Train epoch 54, average loss 0.00460097, average accuracy 0.99856,\n",
    "\t\tDev epoch 54, average loss 0.412365, average accuracy 0.934195,\n",
    "Train epoch 57, average loss 0.00364978, average accuracy 0.99895,\n",
    "Saved model toys /home/ubuntu/project/runs/cnn/checkpoints/toys_model\n",
    "\n",
    "# vid\n",
    "completed cnn creation\n",
    "#batches = 1562\n",
    "Train epoch 0, average loss 0.364666, average accuracy 0.84355,\n",
    "\t\tDev epoch 0, average loss 0.319497, average accuracy 0.86071,\n",
    "\t\t\t\t    Time taken for 0 epochs =  37.799813985824585\n",
    "Train epoch 3, average loss 0.205366, average accuracy 0.917043,\n",
    "Train epoch 6, average loss 0.121604, average accuracy 0.952835,\n",
    "\t\tDev epoch 6, average loss 0.270975, average accuracy 0.899272,\n",
    "Train epoch 9, average loss 0.07385, average accuracy 0.972371,\n",
    "Train epoch 12, average loss 0.0478919, average accuracy 0.982955,\n",
    "\t\tDev epoch 12, average loss 0.35926, average accuracy 0.8959,\n",
    "\t\t\t\t    Time taken for 12 epochs =  459.7377371788025\n",
    "Train epoch 15, average loss 0.0352903, average accuracy 0.987636,\n",
    "Train epoch 18, average loss 0.0294105, average accuracy 0.989997,\n",
    "\t\tDev epoch 18, average loss 0.453399, average accuracy 0.899439,\n",
    "Train epoch 21, average loss 0.0230395, average accuracy 0.992167,\n",
    "Train epoch 24, average loss 0.021058, average accuracy 0.992928,\n",
    "\t\tDev epoch 24, average loss 0.537794, average accuracy 0.899773,\n",
    "\t\t\t\t    Time taken for 24 epochs =  881.6988339424133\n",
    "Train epoch 27, average loss 0.0175359, average accuracy 0.994248,\n",
    "Train epoch 30, average loss 0.0139743, average accuracy 0.995549,\n",
    "\t\tDev epoch 30, average loss 0.571489, average accuracy 0.899673,\n",
    "Train epoch 33, average loss 0.0134842, average accuracy 0.995739,\n",
    "Train epoch 36, average loss 0.0106022, average accuracy 0.996829,\n",
    "\t\tDev epoch 36, average loss 0.609015, average accuracy 0.900407,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1303.694475889206\n",
    "Train epoch 39, average loss 0.0102687, average accuracy 0.996589,\n",
    "Train epoch 42, average loss 0.00967124, average accuracy 0.997099,\n",
    "\t\tDev epoch 42, average loss 0.639956, average accuracy 0.900174,\n",
    "Train epoch 45, average loss 0.00792713, average accuracy 0.997689,\n",
    "Train epoch 48, average loss 0.00804649, average accuracy 0.997559,\n",
    "\t\tDev epoch 48, average loss 0.696189, average accuracy 0.899272,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1725.6383044719696\n",
    "Train epoch 51, average loss 0.00929602, average accuracy 0.997259,\n",
    "Train epoch 54, average loss 0.0067116, average accuracy 0.998039,\n",
    "\t\tDev epoch 54, average loss 0.606182, average accuracy 0.900007,\n",
    "Train epoch 57, average loss 0.00869572, average accuracy 0.997289,\n",
    "Saved model vid /home/ubuntu/project/runs/cnn/checkpoints/vid_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For 100,000 rows\n",
    "## Output of predict on source domain toys\n",
    "Target toys\n",
    "\n",
    "\n",
    "## Output of predict on source domainvid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEEPING TRACK OF RESULTS FROM DIFFERENT RUNS\n",
    "#### Number samples = 10000, Number batches = 156, Without pre-trained embeddings, no dropout\n",
    "Train epoch 0, loss 0.357085, average loss 0.440941, acc 0.84375, average acc 0.845152,\n",
    "\tDev epoch 0, loss 0.494501, average loss 0.394619, acc 0.78125, average acc 0.854959,\n",
    "\t\tTime taken for 0 epochs =  36.18872332572937\n",
    "Train epoch 2, loss 0.25934, average loss 0.335786, acc 0.875, average acc 0.862079,\n",
    "Train epoch 4, loss 0.214457, average loss 0.263726, acc 0.875, average acc 0.891827,\n",
    "Train epoch 6, loss 0.161232, average loss 0.194851, acc 0.890625, average acc 0.92528,\n",
    "Train epoch 8, loss 0.0968393, average loss 0.132971, acc 0.984375, average acc 0.958133,\n",
    "Train epoch 10, loss 0.0598382, average loss 0.0935878, acc 1, average acc 0.978766,\n",
    "\tDev epoch 10, loss 0.318434, average loss 0.279314, acc 0.875, average acc 0.891304,\n",
    "\t\tTime taken for 10 epochs =  366.54717350006104\n",
    "Train epoch 12, loss 0.0432213, average loss 0.089715, acc 1, average acc 0.979768,\n",
    "Train epoch 14, loss 0.0724975, average loss 0.299487, acc 1, average acc 0.957933,\n",
    "Train epoch 16, loss 0.0388074, average loss 0.0520482, acc 1, average acc 0.991987,\n",
    "Train epoch 18, loss 0.0239645, average loss 0.0351604, acc 1, average acc 0.997196,\n",
    "Train epoch 20, loss 0.0157139, average loss 0.0272624, acc 1, average acc 0.996595,\n",
    "\tDev epoch 20, loss 0.304895, average loss 0.283551, acc 0.921875, average acc 0.902853,\n",
    "\t\tTime taken for 20 epochs =  697.680163860321\n",
    "Train epoch 22, loss 0.0131277, average loss 0.017179, acc 1, average acc 0.999299,\n",
    "Train epoch 24, loss 0.0104588, average loss 0.0121853, acc 1, average acc 0.9999,\n",
    "Train epoch 26, loss 0.0072446, average loss 0.00969622, acc 1, average acc 0.9999,\n",
    "Train epoch 28, loss 0.00628954, average loss 0.00805781, acc 1, average acc 0.9999,\n",
    "Train epoch 30, loss 0.00585206, average loss 0.00689346, acc 1, average acc 0.9999,\n",
    "\tDev epoch 30, loss 0.37002, average loss 0.327092, acc 0.875, average acc 0.902514,\n",
    "\t\tTime taken for 30 epochs =  1029.1201057434082\n",
    "Train epoch 32, loss 0.00559655, average loss 0.00594074, acc 1, average acc 0.9999,\n",
    "Train epoch 34, loss 0.0053231, average loss 0.00518374, acc 1, average acc 1,\n",
    "Train epoch 36, loss 0.00504235, average loss 0.00461033, acc 1, average acc 1,\n",
    "Train epoch 38, loss 0.00477842, average loss 0.00416377, acc 1, average acc 1,\n",
    "Train epoch 40, loss 0.00451324, average loss 0.00380536, acc 1, average acc 1,\n",
    "\tDev epoch 40, loss 0.451459, average loss 0.382435, acc 0.859375, average acc 0.899796,\n",
    "\t\tTime taken for 40 epochs =  1360.2103555202484\n",
    "        \n",
    "        \n",
    "#### Number samples = 10000, Number batches = 156, With pre-trained embeddings(Trainable = False), dropout = 0.8\n",
    "Train epoch 0, average loss 0.819034, average acc 0.802784,\n",
    "\tDev epoch 0, average loss 0.415172, average acc 0.853601,\n",
    "\t\tTime taken for 0 epochs =  35.559093713760376\n",
    "Train epoch 2, average loss 0.403757, average acc 0.841046,\n",
    "Train epoch 4, average loss 0.340479, average acc 0.860777,\n",
    "\tDev epoch 5, average loss 0.329067, average acc 0.867188,\n",
    "Train epoch 6, average loss 0.289147, average acc 0.882312,\n",
    "Train epoch 8, average loss 0.237817, average acc 0.904948,\n",
    "Train epoch 10, average loss 0.194272, average acc 0.923978,\n",
    "\tDev epoch 10, average loss 0.330927, average acc 0.876698,\n",
    "\t\tTime taken for 10 epochs =  363.92726016044617\n",
    "Train epoch 12, average loss 0.149883, average acc 0.940405,\n",
    "Train epoch 14, average loss 0.128152, average acc 0.951322,\n",
    "\tDev epoch 15, average loss 0.349508, average acc 0.877717,\n",
    "Train epoch 16, average loss 0.101319, average acc 0.961639,\n",
    "Train epoch 18, average loss 0.079585, average acc 0.970052,\n",
    "Train epoch 20, average loss 0.0705579, average acc 0.97516,\n",
    "\tDev epoch 20, average loss 0.364253, average acc 0.878057,\n",
    "\t\tTime taken for 20 epochs =  692.3398864269257\n",
    "Train epoch 22, average loss 0.0631964, average acc 0.978466,\n",
    "Train epoch 24, average loss 0.0484077, average acc 0.984876,\n",
    "\tDev epoch 25, average loss 0.435054, average acc 0.877717,\n",
    "Train epoch 26, average loss 0.0433892, average acc 0.985377,\n",
    "Train epoch 28, average loss 0.0368327, average acc 0.988381,\n",
    "Train epoch 30, average loss 0.0308169, average acc 0.990385,\n",
    "\tDev epoch 30, average loss 0.570798, average acc 0.875679,\n",
    "\t\tTime taken for 30 epochs =  1052.273297548294\n",
    "Train epoch 32, average loss 0.0291807, average acc 0.991086,\n",
    "Train epoch 34, average loss 0.0271599, average acc 0.991987,\n",
    "\tDev epoch 35, average loss 0.661539, average acc 0.87534,\n",
    "Train epoch 36, average loss 0.029594, average acc 0.991486,\n",
    "Train epoch 38, average loss 0.0236557, average acc 0.99359,\n",
    "Train epoch 40, average loss 0.018746, average acc 0.995292,\n",
    "\tDev epoch 40, average loss 0.506544, average acc 0.878397,\n",
    "\t\tTime taken for 40 epochs =  1466.0729427337646\n",
    "        \n",
    "        \n",
    "#### Changes. Changed convolutional layer weights to xavier initialization. Added random see = 42 to train-test split. Dropped learning rate initial to 0.007\n",
    "\n",
    "Train epoch 0, average loss 0.399899, average accuracy 0.858273,\n",
    "\tDev epoch 0, average loss 0.374428, average accuracy 0.857337,\n",
    "\t\tTime taken for 0 epochs =  34.83389401435852\n",
    "Train epoch 2, average loss 0.326706, average accuracy 0.869391,\n",
    "Train epoch 4, average loss 0.269826, average accuracy 0.891126,\n",
    "\tDev epoch 5, average loss 0.303238, average accuracy 0.88519,\n",
    "Train epoch 6, average loss 0.219368, average accuracy 0.911659,\n",
    "Train epoch 8, average loss 0.171234, average accuracy 0.935296,\n",
    "Train epoch 10, average loss 0.13296, average accuracy 0.953325,\n",
    "\tDev epoch 10, average loss 0.293018, average accuracy 0.887568,\n",
    "\t\tTime taken for 10 epochs =  370.47870922088623\n",
    "Train epoch 12, average loss 0.100562, average accuracy 0.967348,\n",
    "Train epoch 14, average loss 0.0793127, average accuracy 0.977063,\n",
    "\tDev epoch 15, average loss 0.320119, average accuracy 0.886209,\n",
    "Train epoch 16, average loss 0.0582729, average accuracy 0.988482,\n",
    "Train epoch 18, average loss 0.0456755, average accuracy 0.990385,\n",
    "Train epoch 20, average loss 0.0405185, average accuracy 0.992788,\n",
    "\tDev epoch 20, average loss 0.321604, average accuracy 0.886889,\n",
    "\t\tTime taken for 20 epochs =  705.0958936214447\n",
    "Train epoch 22, average loss 0.0351258, average accuracy 0.993089,\n",
    "Train epoch 24, average loss 0.0270392, average accuracy 0.996194,\n",
    "\tDev epoch 25, average loss 0.399808, average accuracy 0.884171,\n",
    "Train epoch 26, average loss 0.0262923, average accuracy 0.995994,\n",
    "Train epoch 28, average loss 0.0242657, average accuracy 0.995994,\n",
    "Train epoch 30, average loss 0.0208821, average accuracy 0.996394,\n",
    "\tDev epoch 30, average loss 0.413923, average accuracy 0.886889,\n",
    "\t\tTime taken for 30 epochs =  1039.4113600254059\n",
    "Train epoch 32, average loss 0.017492, average accuracy 0.997696,\n",
    "Train epoch 34, average loss 0.0146527, average accuracy 0.998097,\n",
    "\tDev epoch 35, average loss 0.386267, average accuracy 0.884851,\n",
    "Train epoch 36, average loss 0.0168233, average accuracy 0.997396,\n",
    "Train epoch 38, average loss 0.0142984, average accuracy 0.997796,\n",
    "Train epoch 40, average loss 0.0110543, average accuracy 0.998998,\n",
    "\tDev epoch 40, average loss 0.478341, average accuracy 0.884171,\n",
    "\t\tTime taken for 40 epochs =  1374.086744070053\n",
    "Train epoch 42, average loss 0.012298, average accuracy 0.998397,\n",
    "Train epoch 44, average loss 0.0116889, average accuracy 0.998197,\n",
    "\tDev epoch 45, average loss 0.448394, average accuracy 0.88587,\n",
    "Train epoch 46, average loss 0.0107089, average accuracy 0.998197,\n",
    "Train epoch 48, average loss 0.00953887, average accuracy 0.998898,\n",
    "Train epoch 50, average loss 0.0097256, average accuracy 0.998898,\n",
    "\tDev epoch 50, average loss 0.424627, average accuracy 0.886209,\n",
    "\t\tTime taken for 50 epochs =  1708.131004333496\n",
    "Train epoch 52, average loss 0.00792942, average accuracy 0.999099,\n",
    "Train epoch 54, average loss 0.00777054, average accuracy 0.999099,\n",
    "\tDev epoch 55, average loss 0.434766, average accuracy 0.887228,\n",
    "Train epoch 56, average loss 0.00812112, average accuracy 0.999099,\n",
    "Train epoch 58, average loss 0.00817043, average accuracy 0.998798,\n",
    "Train epoch 60, average loss 0.00776972, average accuracy 0.998498,\n",
    "\tDev epoch 60, average loss 0.447535, average accuracy 0.886889,\n",
    "\t\tTime taken for 60 epochs =  2042.1303217411041\n",
    "Train epoch 62, average loss 0.00759579, average accuracy 0.998998,\n",
    "Train epoch 64, average loss 0.00697335, average accuracy 0.998798,\n",
    "\tDev epoch 65, average loss 0.514295, average accuracy 0.88519,\n",
    "Train epoch 66, average loss 0.00579109, average accuracy 0.999199,\n",
    "Train epoch 68, average loss 0.00583337, average accuracy 0.999499,\n",
    "\n",
    "#### Changes. set trainable = True in glove embeddings. Changed learning rate back to 0.01 initial.\n",
    "# batches = 156\n",
    "Train epoch 0, average loss 0.399899, average accuracy 0.858273,\n",
    "\tDev epoch 0, average loss 0.374428, average accuracy 0.857337,\n",
    "\t\tTime taken for 0 epochs =  34.83389401435852\n",
    "Train epoch 2, average loss 0.326706, average accuracy 0.869391,\n",
    "Train epoch 4, average loss 0.269826, average accuracy 0.891126,\n",
    "\tDev epoch 5, average loss 0.303238, average accuracy 0.88519,\n",
    "Train epoch 6, average loss 0.219368, average accuracy 0.911659,\n",
    "Train epoch 8, average loss 0.171234, average accuracy 0.935296,\n",
    "Train epoch 10, average loss 0.13296, average accuracy 0.953325,\n",
    "\tDev epoch 10, average loss 0.293018, average accuracy 0.887568,\n",
    "\t\tTime taken for 10 epochs =  370.47870922088623\n",
    "Train epoch 12, average loss 0.100562, average accuracy 0.967348,\n",
    "Train epoch 14, average loss 0.0793127, average accuracy 0.977063,\n",
    "\tDev epoch 15, average loss 0.320119, average accuracy 0.886209,\n",
    "Train epoch 16, average loss 0.0582729, average accuracy 0.988482,\n",
    "Train epoch 18, average loss 0.0456755, average accuracy 0.990385,\n",
    "Train epoch 20, average loss 0.0405185, average accuracy 0.992788,\n",
    "\tDev epoch 20, average loss 0.321604, average accuracy 0.886889,\n",
    "\t\tTime taken for 20 epochs =  705.0958936214447\n",
    "Train epoch 22, average loss 0.0351258, average accuracy 0.993089,\n",
    "Train epoch 24, average loss 0.0270392, average accuracy 0.996194,\n",
    "\tDev epoch 25, average loss 0.399808, average accuracy 0.884171,\n",
    "Train epoch 26, average loss 0.0262923, average accuracy 0.995994,\n",
    "Train epoch 28, average loss 0.0242657, average accuracy 0.995994,\n",
    "Train epoch 30, average loss 0.0208821, average accuracy 0.996394,\n",
    "\tDev epoch 30, average loss 0.413923, average accuracy 0.886889,\n",
    "\t\tTime taken for 30 epochs =  1039.4113600254059\n",
    "Train epoch 32, average loss 0.017492, average accuracy 0.997696,\n",
    "Train epoch 34, average loss 0.0146527, average accuracy 0.998097,\n",
    "\tDev epoch 35, average loss 0.386267, average accuracy 0.884851,\n",
    "Train epoch 36, average loss 0.0168233, average accuracy 0.997396,\n",
    "Train epoch 38, average loss 0.0142984, average accuracy 0.997796,\n",
    "Train epoch 40, average loss 0.0110543, average accuracy 0.998998,\n",
    "\tDev epoch 40, average loss 0.478341, average accuracy 0.884171,\n",
    "\t\tTime taken for 40 epochs =  1374.086744070053\n",
    "Train epoch 42, average loss 0.012298, average accuracy 0.998397,\n",
    "Train epoch 44, average loss 0.0116889, average accuracy 0.998197,\n",
    "\tDev epoch 45, average loss 0.448394, average accuracy 0.88587,\n",
    "Train epoch 46, average loss 0.0107089, average accuracy 0.998197,\n",
    "Train epoch 48, average loss 0.00953887, average accuracy 0.998898,\n",
    "Train epoch 50, average loss 0.0097256, average accuracy 0.998898,\n",
    "\tDev epoch 50, average loss 0.424627, average accuracy 0.886209,\n",
    "\t\tTime taken for 50 epochs =  1708.131004333496\n",
    "Train epoch 52, average loss 0.00792942, average accuracy 0.999099,\n",
    "Train epoch 54, average loss 0.00777054, average accuracy 0.999099,\n",
    "\tDev epoch 55, average loss 0.434766, average accuracy 0.887228,\n",
    "Train epoch 56, average loss 0.00812112, average accuracy 0.999099,\n",
    "Train epoch 58, average loss 0.00817043, average accuracy 0.998798,\n",
    "Train epoch 60, average loss 0.00776972, average accuracy 0.998498,\n",
    "\tDev epoch 60, average loss 0.447535, average accuracy 0.886889,\n",
    "\t\tTime taken for 60 epochs =  2042.1303217411041\n",
    "Train epoch 62, average loss 0.00759579, average accuracy 0.998998,\n",
    "Train epoch 64, average loss 0.00697335, average accuracy 0.998798,\n",
    "\tDev epoch 65, average loss 0.514295, average accuracy 0.88519,\n",
    "Train epoch 66, average loss 0.00579109, average accuracy 0.999199,\n",
    "Train epoch 68, average loss 0.00583337, average accuracy 0.999499,\n",
    "\n",
    "\n",
    "#### Changes : increased sample size to 20000. increased filter number to 256 per filter size. Both together slowed it down 4 times. Ran 150 epochs.\n",
    "\n",
    "Result - get to accuracy of about 90.5% on dev set. First saw it in about 80 epochs.\n",
    "\n",
    "number of batches = 312\n",
    "Train epoch 0, average loss 0.399409, average accuracy 0.851763,\n",
    "\tDev epoch 0, average loss 0.390935, average accuracy 0.849798,\n",
    "\t\tTime taken for 0 epochs =  126.17049622535706\n",
    "Train epoch 2, average loss 0.310082, average accuracy 0.877955,\n",
    "Train epoch 4, average loss 0.240321, average accuracy 0.905298,\n",
    "\tDev epoch 5, average loss 0.309063, average accuracy 0.879872,\n",
    "Train epoch 6, average loss 0.183422, average accuracy 0.929137,\n",
    "Train epoch 8, average loss 0.132305, average accuracy 0.950871,\n",
    "Train epoch 10, average loss 0.091186, average accuracy 0.96855,\n",
    "\tDev epoch 10, average loss 0.359275, average accuracy 0.887769,\n",
    "\t\tTime taken for 10 epochs =  1301.3605210781097\n",
    "Train epoch 12, average loss 0.0655771, average accuracy 0.978966,\n",
    "Train epoch 14, average loss 0.0487824, average accuracy 0.986579,\n",
    "\tDev epoch 15, average loss 0.31907, average accuracy 0.901546,\n",
    "Train epoch 16, average loss 0.0353836, average accuracy 0.991136,\n",
    "Train epoch 18, average loss 0.0272109, average accuracy 0.993389,\n",
    "Train epoch 20, average loss 0.0209386, average accuracy 0.995843,\n",
    "\tDev epoch 20, average loss 0.473887, average accuracy 0.888609,\n",
    "\t\tTime taken for 20 epochs =  2475.8890883922577\n",
    "Train epoch 22, average loss 0.0169513, average accuracy 0.996444,\n",
    "Train epoch 24, average loss 0.0144857, average accuracy 0.997045,\n",
    "\tDev epoch 25, average loss 0.52256, average accuracy 0.886929,\n",
    "Train epoch 26, average loss 0.0115876, average accuracy 0.998147,\n",
    "Train epoch 28, average loss 0.00961356, average accuracy 0.998347,\n",
    "Train epoch 30, average loss 0.00915859, average accuracy 0.998297,\n",
    "\tDev epoch 30, average loss 0.458756, average accuracy 0.895665,\n",
    "\t\tTime taken for 30 epochs =  3649.31303191185\n",
    "Train epoch 32, average loss 0.00898325, average accuracy 0.998498,\n",
    "Train epoch 34, average loss 0.00841926, average accuracy 0.998448,\n",
    "\tDev epoch 35, average loss 0.457308, average accuracy 0.897681,\n",
    "Train epoch 36, average loss 0.00646681, average accuracy 0.999149,\n",
    "Train epoch 38, average loss 0.00662382, average accuracy 0.998598,\n",
    "Train epoch 40, average loss 0.00595506, average accuracy 0.999149,\n",
    "\tDev epoch 40, average loss 0.378182, average accuracy 0.901546,\n",
    "\t\tTime taken for 40 epochs =  4823.336989402771\n",
    "Train epoch 42, average loss 0.00593351, average accuracy 0.999149,\n",
    "Train epoch 44, average loss 0.00464219, average accuracy 0.999249,\n",
    "\tDev epoch 45, average loss 0.431081, average accuracy 0.90121,\n",
    "Train epoch 46, average loss 0.00444085, average accuracy 0.999499,\n",
    "Train epoch 48, average loss 0.00461485, average accuracy 0.999349,\n",
    "Train epoch 50, average loss 0.00466378, average accuracy 0.999199,\n",
    "\tDev epoch 50, average loss 0.380632, average accuracy 0.90289,\n",
    "\t\tTime taken for 50 epochs =  5997.558866024017\n",
    "Train epoch 52, average loss 0.00401276, average accuracy 0.999299,\n",
    "Train epoch 54, average loss 0.00360064, average accuracy 0.999549,\n",
    "\tDev epoch 55, average loss 0.472261, average accuracy 0.900706,\n",
    "Train epoch 56, average loss 0.00390259, average accuracy 0.999449,\n",
    "Train epoch 58, average loss 0.00343323, average accuracy 0.999499,\n",
    "Train epoch 60, average loss 0.00328182, average accuracy 0.999549,\n",
    "\tDev epoch 60, average loss 0.405813, average accuracy 0.901714,\n",
    "\t\tTime taken for 60 epochs =  7171.330280542374\n",
    "Train epoch 62, average loss 0.00357674, average accuracy 0.999499,\n",
    "Train epoch 64, average loss 0.00316356, average accuracy 0.999449,\n",
    "\tDev epoch 65, average loss 0.484432, average accuracy 0.899698,\n",
    "Train epoch 66, average loss 0.00242786, average accuracy 0.99975,\n",
    "Train epoch 68, average loss 0.0029979, average accuracy 0.999399,\n",
    "Train epoch 70, average loss 0.00219736, average accuracy 0.999599,\n",
    "\tDev epoch 70, average loss 0.522188, average accuracy 0.89953,\n",
    "\t\tTime taken for 70 epochs =  8345.40755033493\n",
    "Train epoch 72, average loss 0.00263028, average accuracy 0.999599,\n",
    "Train epoch 74, average loss 0.00262097, average accuracy 0.999599,\n",
    "\tDev epoch 75, average loss 0.501381, average accuracy 0.90037,\n",
    "Train epoch 76, average loss 0.00184087, average accuracy 0.99975,\n",
    "Train epoch 78, average loss 0.00261343, average accuracy 0.999399,\n",
    "Train epoch 80, average loss 0.00210662, average accuracy 0.9997,\n",
    "\tDev epoch 80, average loss 0.437249, average accuracy 0.90457,\n",
    "\t\tTime taken for 80 epochs =  9519.476462364197\n",
    "Train epoch 82, average loss 0.00218873, average accuracy 0.999599,\n",
    "Train epoch 84, average loss 0.00204953, average accuracy 0.9997,\n",
    "\tDev epoch 85, average loss 0.440843, average accuracy 0.90457,\n",
    "Train epoch 86, average loss 0.00178851, average accuracy 0.99975,\n",
    "Train epoch 88, average loss 0.00177724, average accuracy 0.999599,\n",
    "Train epoch 90, average loss 0.0018953, average accuracy 0.999599,\n",
    "\tDev epoch 90, average loss 0.465653, average accuracy 0.905074,\n",
    "\t\tTime taken for 90 epochs =  10693.68774318695\n",
    "Train epoch 92, average loss 0.00145395, average accuracy 0.9998,\n",
    "Train epoch 94, average loss 0.00158429, average accuracy 0.999649,\n",
    "\tDev epoch 95, average loss 0.472749, average accuracy 0.90541,\n",
    "Train epoch 96, average loss 0.00238694, average accuracy 0.999499,\n",
    "Train epoch 98, average loss 0.00175795, average accuracy 0.9997,\n",
    "Train epoch 100, average loss 0.00153946, average accuracy 0.9998,\n",
    "\tDev epoch 100, average loss 0.440989, average accuracy 0.906754,\n",
    "\t\tTime taken for 100 epochs =  11867.341850280762\n",
    "Train epoch 102, average loss 0.00144569, average accuracy 0.9998,\n",
    "Train epoch 104, average loss 0.00135836, average accuracy 0.99975,\n",
    "\tDev epoch 105, average loss 0.450465, average accuracy 0.90457,\n",
    "Train epoch 106, average loss 0.00134604, average accuracy 0.99985,\n",
    "Train epoch 108, average loss 0.00198454, average accuracy 0.999549,\n",
    "Train epoch 110, average loss 0.0016289, average accuracy 0.99975,\n",
    "\tDev epoch 110, average loss 0.43462, average accuracy 0.905242,\n",
    "\t\tTime taken for 110 epochs =  13040.53886771202\n",
    "Train epoch 112, average loss 0.00116808, average accuracy 0.99975,\n",
    "Train epoch 114, average loss 0.00163431, average accuracy 0.999649,\n",
    "\tDev epoch 115, average loss 0.544521, average accuracy 0.901714,\n",
    "Train epoch 116, average loss 0.00107182, average accuracy 0.9999,\n",
    "Train epoch 118, average loss 0.00118616, average accuracy 0.99985,\n",
    "Train epoch 120, average loss 0.00116875, average accuracy 0.9998,\n",
    "\tDev epoch 120, average loss 0.456846, average accuracy 0.906082,\n",
    "\t\tTime taken for 120 epochs =  14214.848599672318\n",
    "Train epoch 122, average loss 0.00144058, average accuracy 0.999649,\n",
    "Train epoch 124, average loss 0.00139588, average accuracy 0.999649,\n",
    "\tDev epoch 125, average loss 0.456731, average accuracy 0.90709,\n",
    "Train epoch 126, average loss 0.00129419, average accuracy 0.99975,\n",
    "Train epoch 128, average loss 0.000993939, average accuracy 0.9998,\n",
    "Train epoch 130, average loss 0.00110859, average accuracy 0.99975,\n",
    "\tDev epoch 130, average loss 0.440627, average accuracy 0.905242,\n",
    "\t\tTime taken for 130 epochs =  15387.949309825897\n",
    "Train epoch 132, average loss 0.000869354, average accuracy 0.9998,\n",
    "Train epoch 134, average loss 0.0010678, average accuracy 0.99975,\n",
    "\tDev epoch 135, average loss 0.662642, average accuracy 0.895497,\n",
    "Train epoch 136, average loss 0.00121623, average accuracy 0.99985,\n",
    "Train epoch 138, average loss 0.00106557, average accuracy 0.9998,\n",
    "Train epoch 140, average loss 0.0012005, average accuracy 0.9997,\n",
    "\tDev epoch 140, average loss 0.48323, average accuracy 0.905578,\n",
    "\t\tTime taken for 140 epochs =  16560.915422201157\n",
    "Train epoch 142, average loss 0.00142349, average accuracy 0.999649,\n",
    "Train epoch 144, average loss 0.00100832, average accuracy 0.9998,\n",
    "\tDev epoch 145, average loss 0.469864, average accuracy 0.906082,\n",
    "Train epoch 146, average loss 0.000867766, average accuracy 0.99985,\n",
    "Train epoch 148, average loss 0.000895252, average accuracy 0.9998,\n",
    "Train epoch 150, average loss 0.00128643, average accuracy 0.99975,\n",
    "\tDev epoch 150, average loss 0.504558, average accuracy 0.904738,\n",
    "\t\tTime taken for 150 epochs =  17732.689709424973\n",
    "Train epoch 152, average loss 0.00106975, average accuracy 0.99975,\n",
    "Train epoch 154, average loss 0.000923771, average accuracy 0.9998,\n",
    "\tDev epoch 155, average loss 0.46343, average accuracy 0.904066,\n",
    "    \n",
    "    \n",
    "#### Home and Kitchen, 100000 reviews. 200 epochs, min-documents = 0, embedding size = 100, embeddings trainable = True.\n",
    "\n",
    "completed cnn creation\n",
    "Number batches = 1562\n",
    "Train epoch 0, average loss 0.304591, average accuracy 0.87457,\n",
    "\tDev epoch 0, average loss 0.237596, average accuracy 0.904013,\n",
    "\t\tTime taken for 0 epochs =  36.92328929901123\n",
    "Train epoch 1, average loss 0.223893, average accuracy 0.910071,\n",
    "Train epoch 2, average loss 0.183924, average accuracy 0.927187,\n",
    "Train epoch 3, average loss 0.150811, average accuracy 0.941301,\n",
    "Train epoch 4, average loss 0.123611, average accuracy 0.952775,\n",
    "Train epoch 5, average loss 0.101681, average accuracy 0.961138,\n",
    "\tDev epoch 5, average loss 0.215108, average accuracy 0.917234,\n",
    "Train epoch 6, average loss 0.0824821, average accuracy 0.9694,\n",
    "Train epoch 7, average loss 0.0682877, average accuracy 0.975272,\n",
    "Train epoch 8, average loss 0.0581098, average accuracy 0.978353,\n",
    "Train epoch 9, average loss 0.0481609, average accuracy 0.982815,\n",
    "Train epoch 10, average loss 0.0426059, average accuracy 0.984625,\n",
    "\tDev epoch 10, average loss 0.258906, average accuracy 0.918336,\n",
    "\t\tTime taken for 10 epochs =  381.81759333610535\n",
    "Train epoch 11, average loss 0.0369901, average accuracy 0.986686,\n",
    "Train epoch 12, average loss 0.0348868, average accuracy 0.986946,\n",
    "Train epoch 13, average loss 0.0291087, average accuracy 0.989937,\n",
    "Train epoch 14, average loss 0.0271577, average accuracy 0.990807,\n",
    "Train epoch 15, average loss 0.0248202, average accuracy 0.991487,\n",
    "\tDev epoch 15, average loss 0.317684, average accuracy 0.925114,\n",
    "Train epoch 16, average loss 0.0232966, average accuracy 0.992037,\n",
    "Train epoch 17, average loss 0.0218823, average accuracy 0.992578,\n",
    "Train epoch 18, average loss 0.0189234, average accuracy 0.993878,\n",
    "Train epoch 19, average loss 0.0175714, average accuracy 0.993978,\n",
    "Train epoch 20, average loss 0.0175559, average accuracy 0.994108,\n",
    "\tDev epoch 20, average loss 0.322537, average accuracy 0.924746,\n",
    "\t\tTime taken for 20 epochs =  727.1127679347992\n",
    "Train epoch 21, average loss 0.0159045, average accuracy 0.994638,\n",
    "Train epoch 22, average loss 0.0141158, average accuracy 0.995409,\n",
    "Train epoch 23, average loss 0.0130852, average accuracy 0.995809,\n",
    "Train epoch 24, average loss 0.0131137, average accuracy 0.995709,\n",
    "Train epoch 25, average loss 0.0123618, average accuracy 0.995899,\n",
    "\tDev epoch 25, average loss 0.403685, average accuracy 0.924646,\n",
    "Train epoch 26, average loss 0.0112836, average accuracy 0.996369,\n",
    "Train epoch 27, average loss 0.0111362, average accuracy 0.996449,\n",
    "Train epoch 28, average loss 0.00865663, average accuracy 0.997299,\n",
    "Train epoch 29, average loss 0.00935199, average accuracy 0.996959,\n",
    "Train epoch 30, average loss 0.00935717, average accuracy 0.996699,\n",
    "\tDev epoch 30, average loss 0.367405, average accuracy 0.924579,\n",
    "\t\tTime taken for 30 epochs =  1072.1994183063507\n",
    "Train epoch 31, average loss 0.00909225, average accuracy 0.997149,\n",
    "Train epoch 32, average loss 0.00754316, average accuracy 0.997819,\n",
    "Train epoch 33, average loss 0.0079119, average accuracy 0.997379,\n",
    "Train epoch 34, average loss 0.00705012, average accuracy 0.997719,\n",
    "Train epoch 35, average loss 0.00730037, average accuracy 0.997829,\n",
    "\tDev epoch 35, average loss 0.370859, average accuracy 0.926749,\n",
    "Train epoch 36, average loss 0.00705196, average accuracy 0.997799,\n",
    "Train epoch 37, average loss 0.00669642, average accuracy 0.998039,\n",
    "Train epoch 38, average loss 0.00627199, average accuracy 0.998009,\n",
    "Train epoch 39, average loss 0.00528379, average accuracy 0.998309,\n",
    "Train epoch 40, average loss 0.00593793, average accuracy 0.998229,\n",
    "\tDev epoch 40, average loss 0.383177, average accuracy 0.927618,\n",
    "\t\tTime taken for 40 epochs =  1416.9269080162048\n",
    "Train epoch 41, average loss 0.00519099, average accuracy 0.998289,\n",
    "Train epoch 42, average loss 0.00574083, average accuracy 0.998259,\n",
    "Train epoch 43, average loss 0.00573397, average accuracy 0.998269,\n",
    "Train epoch 44, average loss 0.00478373, average accuracy 0.99844,\n",
    "Train epoch 45, average loss 0.00476654, average accuracy 0.99864,\n",
    "\tDev epoch 45, average loss 0.401872, average accuracy 0.927651,\n",
    "Train epoch 46, average loss 0.00546603, average accuracy 0.998079,\n",
    "Train epoch 47, average loss 0.00552262, average accuracy 0.998299,\n",
    "Train epoch 48, average loss 0.0044039, average accuracy 0.9986,\n",
    "Train epoch 49, average loss 0.00444202, average accuracy 0.99858,\n",
    "Train epoch 50, average loss 0.00553655, average accuracy 0.998239,\n",
    "\tDev epoch 50, average loss 0.410369, average accuracy 0.926182,\n",
    "\t\tTime taken for 50 epochs =  1761.8077738285065\n",
    "Train epoch 51, average loss 0.00446012, average accuracy 0.99862,\n",
    "Train epoch 52, average loss 0.00399219, average accuracy 0.9988,\n",
    "Train epoch 53, average loss 0.00413133, average accuracy 0.99876,\n",
    "Train epoch 54, average loss 0.00465115, average accuracy 0.99857,\n",
    "Train epoch 55, average loss 0.00390704, average accuracy 0.99883,\n",
    "\tDev epoch 55, average loss 0.523764, average accuracy 0.924212,\n",
    "Train epoch 56, average loss 0.00380121, average accuracy 0.99894,\n",
    "Train epoch 57, average loss 0.0041114, average accuracy 0.99863,\n",
    "Train epoch 58, average loss 0.00400388, average accuracy 0.99874,\n",
    "Train epoch 59, average loss 0.00436616, average accuracy 0.99868,\n",
    "Train epoch 60, average loss 0.00410136, average accuracy 0.99879,\n",
    "\tDev epoch 60, average loss 0.465152, average accuracy 0.926115,\n",
    "\t\tTime taken for 60 epochs =  2106.424416542053\n",
    "Train epoch 61, average loss 0.00331209, average accuracy 0.99906,\n",
    "Train epoch 62, average loss 0.00366131, average accuracy 0.99892,\n",
    "Train epoch 63, average loss 0.00425752, average accuracy 0.99884,\n",
    "Train epoch 64, average loss 0.00341706, average accuracy 0.99892,\n",
    "Train epoch 65, average loss 0.00352085, average accuracy 0.99884,\n",
    "\tDev epoch 65, average loss 0.443976, average accuracy 0.919404,\n",
    "Train epoch 66, average loss 0.00392861, average accuracy 0.99874,\n",
    "Train epoch 67, average loss 0.00390108, average accuracy 0.99879,\n",
    "Train epoch 68, average loss 0.00344137, average accuracy 0.99892,\n",
    "Train epoch 69, average loss 0.00282472, average accuracy 0.99918,\n",
    "Train epoch 70, average loss 0.00305406, average accuracy 0.99906,\n",
    "\tDev epoch 70, average loss 0.497239, average accuracy 0.926783,\n",
    "\t\tTime taken for 70 epochs =  2451.336544752121\n",
    "Train epoch 71, average loss 0.00335245, average accuracy 0.99887,\n",
    "Train epoch 72, average loss 0.00321827, average accuracy 0.99912,\n",
    "Train epoch 73, average loss 0.0030572, average accuracy 0.99906,\n",
    "Train epoch 74, average loss 0.00308252, average accuracy 0.99905,\n",
    "Train epoch 75, average loss 0.00312562, average accuracy 0.99909,\n",
    "\tDev epoch 75, average loss 0.454947, average accuracy 0.927818,\n",
    "Train epoch 76, average loss 0.00308124, average accuracy 0.99912,\n",
    "Train epoch 77, average loss 0.00282977, average accuracy 0.9991,\n",
    "Train epoch 78, average loss 0.00250926, average accuracy 0.99929,\n",
    "Train epoch 79, average loss 0.00278992, average accuracy 0.99917,\n",
    "Train epoch 80, average loss 0.00226384, average accuracy 0.99932,\n",
    "\tDev epoch 80, average loss 0.524143, average accuracy 0.925314,\n",
    "\t\tTime taken for 80 epochs =  2796.155880212784\n",
    "Train epoch 81, average loss 0.00289398, average accuracy 0.99907,\n",
    "Train epoch 82, average loss 0.00267882, average accuracy 0.99915,\n",
    "Train epoch 83, average loss 0.00251234, average accuracy 0.99917,\n",
    "Train epoch 84, average loss 0.00212931, average accuracy 0.99929,\n",
    "Train epoch 85, average loss 0.00204477, average accuracy 0.99936,\n",
    "\tDev epoch 85, average loss 0.533493, average accuracy 0.926382,\n",
    "Train epoch 86, average loss 0.00236821, average accuracy 0.99926,\n",
    "Train epoch 87, average loss 0.00237569, average accuracy 0.99921,\n",
    "Train epoch 88, average loss 0.00245643, average accuracy 0.99923,\n",
    "Train epoch 89, average loss 0.00226378, average accuracy 0.99937,\n",
    "Train epoch 90, average loss 0.00233121, average accuracy 0.99925,\n",
    "\tDev epoch 90, average loss 0.58426, average accuracy 0.924112,\n",
    "\t\tTime taken for 90 epochs =  3140.9629440307617\n",
    "Train epoch 91, average loss 0.00239393, average accuracy 0.99934,\n",
    "Train epoch 92, average loss 0.00168514, average accuracy 0.99947,\n",
    "Train epoch 93, average loss 0.00218618, average accuracy 0.99934,\n",
    "Train epoch 94, average loss 0.00201177, average accuracy 0.99939,\n",
    "Train epoch 95, average loss 0.00265143, average accuracy 0.99903,\n",
    "\n",
    "\tDev epoch 95, average loss 0.522781, average accuracy 0.927284,\n",
    "Train epoch 96, average loss 0.00200737, average accuracy 0.99945,\n",
    "Train epoch 97, average loss 0.00167531, average accuracy 0.99956,\n",
    "Train epoch 98, average loss 0.00197691, average accuracy 0.99943,\n",
    "Train epoch 99, average loss 0.00189404, average accuracy 0.99943,\n",
    "Train epoch 100, average loss 0.00217102, average accuracy 0.99935,\n",
    "\tDev epoch 100, average loss 0.509596, average accuracy 0.927417,\n",
    "\t\tTime taken for 100 epochs =  3485.5287368297577\n",
    "Train epoch 101, average loss 0.00214391, average accuracy 0.99932,\n",
    "Train epoch 102, average loss 0.00166687, average accuracy 0.99949,\n",
    "Train epoch 103, average loss 0.00193214, average accuracy 0.99941,\n",
    "Train epoch 104, average loss 0.00230417, average accuracy 0.99928,\n",
    "Train epoch 105, average loss 0.00262318, average accuracy 0.99917,\n",
    "\tDev epoch 105, average loss 0.580702, average accuracy 0.924379,\n",
    "Train epoch 106, average loss 0.00217951, average accuracy 0.99936,\n",
    "Train epoch 107, average loss 0.00196965, average accuracy 0.9994,\n",
    "Train epoch 108, average loss 0.00169252, average accuracy 0.99945,\n",
    "Train epoch 109, average loss 0.00184972, average accuracy 0.99937,\n",
    "Train epoch 110, average loss 0.00203432, average accuracy 0.99938,\n",
    "\tDev epoch 110, average loss 0.580674, average accuracy 0.925214,\n",
    "\t\tTime taken for 110 epochs =  3830.0551614761353\n",
    "Train epoch 111, average loss 0.0018848, average accuracy 0.99941,\n",
    "Train epoch 112, average loss 0.001956, average accuracy 0.99934,\n",
    "Train epoch 113, average loss 0.00164951, average accuracy 0.99948,\n",
    "Train epoch 114, average loss 0.00189435, average accuracy 0.99941,\n",
    "Train epoch 115, average loss 0.00179166, average accuracy 0.99939,\n",
    "\tDev epoch 115, average loss 0.541616, average accuracy 0.926282,\n",
    "Train epoch 116, average loss 0.0018286, average accuracy 0.99944,\n",
    "Train epoch 117, average loss 0.00176589, average accuracy 0.99943,\n",
    "Train epoch 118, average loss 0.00170349, average accuracy 0.99947,\n",
    "Train epoch 119, average loss 0.00222146, average accuracy 0.99928,\n",
    "Train epoch 120, average loss 0.00133639, average accuracy 0.99959,\n",
    "\tDev epoch 120, average loss 0.587092, average accuracy 0.926482,\n",
    "\t\tTime taken for 120 epochs =  4174.599865913391\n",
    "Train epoch 121, average loss 0.00179675, average accuracy 0.99939,\n",
    "Train epoch 122, average loss 0.00170493, average accuracy 0.99948,\n",
    "Train epoch 123, average loss 0.00196213, average accuracy 0.99939,\n",
    "Train epoch 124, average loss 0.00148926, average accuracy 0.99953,\n",
    "Train epoch 125, average loss 0.00146297, average accuracy 0.99951,\n",
    "\tDev epoch 125, average loss 0.555345, average accuracy 0.926516,\n",
    "Train epoch 126, average loss 0.00200696, average accuracy 0.99936,\n",
    "Train epoch 127, average loss 0.00152734, average accuracy 0.99952,\n",
    "Train epoch 128, average loss 0.00232658, average accuracy 0.9993,\n",
    "Train epoch 129, average loss 0.00197241, average accuracy 0.99939,\n",
    "Train epoch 130, average loss 0.00210397, average accuracy 0.99924,\n",
    "\tDev epoch 130, average loss 0.670168, average accuracy 0.92291,\n",
    "\t\tTime taken for 130 epochs =  4519.105709552765\n",
    "Train epoch 131, average loss 0.00184804, average accuracy 0.99943,\n",
    "Train epoch 132, average loss 0.00178672, average accuracy 0.99941,\n",
    "Train epoch 133, average loss 0.00175419, average accuracy 0.9994,\n",
    "Train epoch 134, average loss 0.0019224, average accuracy 0.99934,\n",
    "Train epoch 135, average loss 0.00222945, average accuracy 0.99931,\n",
    "\tDev epoch 135, average loss 0.657096, average accuracy 0.923377,\n",
    "Train epoch 136, average loss 0.00175209, average accuracy 0.99934,\n",
    "Train epoch 137, average loss 0.00214912, average accuracy 0.99925,\n",
    "Train epoch 138, average loss 0.00162636, average accuracy 0.99941,\n",
    "Train epoch 139, average loss 0.00180691, average accuracy 0.99946,\n",
    "Train epoch 140, average loss 0.00118666, average accuracy 0.99959,\n",
    "\tDev epoch 140, average loss 0.575778, average accuracy 0.926015,\n",
    "\t\tTime taken for 140 epochs =  4863.487067222595\n",
    "        \n",
    "\n",
    "#### Same as above, with 60 epochs.\n",
    "\n",
    "completed cnn creation\n",
    "Number batches = 1562\n",
    "Train epoch 0, average loss 0.30547, average accuracy 0.87393,\n",
    "\tDev epoch 0, average loss 0.237115, average accuracy 0.904915,\n",
    "\t\tTime taken for 0 epochs =  36.875508308410645\n",
    "Train epoch 3, average loss 0.152626, average accuracy 0.940681,\n",
    "Train epoch 6, average loss 0.0826888, average accuracy 0.96937,\n",
    "\tDev epoch 6, average loss 0.237618, average accuracy 0.921307,\n",
    "Train epoch 9, average loss 0.0498599, average accuracy 0.982274,\n",
    "Train epoch 12, average loss 0.0340667, average accuracy 0.987906,\n",
    "\tDev epoch 12, average loss 0.290549, average accuracy 0.922075,\n",
    "\t\tTime taken for 12 epochs =  449.51883339881897\n",
    "Train epoch 15, average loss 0.0254497, average accuracy 0.991217,\n",
    "Train epoch 18, average loss 0.0187158, average accuracy 0.993648,\n",
    "\tDev epoch 18, average loss 0.306756, average accuracy 0.923945,\n",
    "Train epoch 21, average loss 0.0152883, average accuracy 0.995038,\n",
    "Train epoch 24, average loss 0.0124834, average accuracy 0.995999,\n",
    "\tDev epoch 24, average loss 0.329978, average accuracy 0.926716,\n",
    "\t\tTime taken for 24 epochs =  862.4385898113251\n",
    "Train epoch 27, average loss 0.0112456, average accuracy 0.996409,\n",
    "Train epoch 30, average loss 0.00972575, average accuracy 0.996939,\n",
    "\tDev epoch 30, average loss 0.361886, average accuracy 0.92695,\n",
    "Train epoch 33, average loss 0.00892223, average accuracy 0.997189,\n",
    "Train epoch 36, average loss 0.00712946, average accuracy 0.997789,\n",
    "\tDev epoch 36, average loss 0.381359, average accuracy 0.92685,\n",
    "\t\tTime taken for 36 epochs =  1275.2902917861938\n",
    "Train epoch 39, average loss 0.00628213, average accuracy 0.998139,\n",
    "Train epoch 42, average loss 0.00596454, average accuracy 0.998159,\n",
    "\tDev epoch 42, average loss 0.47203, average accuracy 0.924446,\n",
    "Train epoch 45, average loss 0.00500223, average accuracy 0.9985,\n",
    "Train epoch 48, average loss 0.00469857, average accuracy 0.99864,\n",
    "\tDev epoch 48, average loss 0.401148, average accuracy 0.926749,\n",
    "\t\tTime taken for 48 epochs =  1688.264419078827\n",
    "Train epoch 51, average loss 0.00402232, average accuracy 0.99886,\n",
    "Train epoch 54, average loss 0.00421826, average accuracy 0.99865,\n",
    "\tDev epoch 54, average loss 0.416046, average accuracy 0.928385,\n",
    "Train epoch 57, average loss 0.00368313, average accuracy 0.99893,\n",
    "Train epoch 60, average loss 0.00335348, average accuracy 0.99901,\n",
    "\tDev epoch 60, average loss 0.438822, average accuracy 0.927918,\n",
    "\t\tTime taken for 60 epochs =  2101.384346008301\n",
    "\n",
    "#### With the embeddings set to trainable = false.\n",
    "completed cnn creation\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.304837, average accuracy 0.87436,\n",
    "\t\tDev epoch 0, average loss 0.239171, average accuracy 0.905649,\n",
    "\t\t\t\t    Time taken for 0 epochs =  33.351370334625244\n",
    "Train epoch 3, average loss 0.159725, average accuracy 0.93797,\n",
    "Train epoch 6, average loss 0.0951332, average accuracy 0.963478,\n",
    "\t\tDev epoch 6, average loss 0.222475, average accuracy 0.916299,\n",
    "Train epoch 9, average loss 0.0597436, average accuracy 0.978053,\n",
    "Train epoch 12, average loss 0.0414887, average accuracy 0.985045,\n",
    "\t\tDev epoch 12, average loss 0.258621, average accuracy 0.921708,\n",
    "\t\t\t\t    Time taken for 12 epochs =  405.9028522968292\n",
    "Train epoch 15, average loss 0.03174, average accuracy 0.988536,\n",
    "Train epoch 18, average loss 0.0268379, average accuracy 0.990747,\n",
    "\t\tDev epoch 18, average loss 0.308509, average accuracy 0.922242,\n",
    "Train epoch 21, average loss 0.0237158, average accuracy 0.991737,\n",
    "Train epoch 24, average loss 0.0182942, average accuracy 0.993528,\n",
    "\t\tDev epoch 24, average loss 0.330288, average accuracy 0.922643,\n",
    "\t\t\t\t    Time taken for 24 epochs =  778.5845618247986\n",
    "Train epoch 27, average loss 0.0170312, average accuracy 0.994198,\n",
    "Train epoch 30, average loss 0.0140965, average accuracy 0.994978,\n",
    "\t\tDev epoch 30, average loss 0.389294, average accuracy 0.922476,\n",
    "Train epoch 33, average loss 0.0119405, average accuracy 0.996029,\n",
    "Train epoch 36, average loss 0.0102918, average accuracy 0.996929,\n",
    "\t\tDev epoch 36, average loss 0.390921, average accuracy 0.922943,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1150.900290966034\n",
    "Train epoch 39, average loss 0.00885309, average accuracy 0.997279,\n",
    "Train epoch 42, average loss 0.0098332, average accuracy 0.996869,\n",
    "\t\tDev epoch 42, average loss 0.408098, average accuracy 0.922476,\n",
    "Train epoch 45, average loss 0.00801163, average accuracy 0.997559,\n",
    "Train epoch 48, average loss 0.0075703, average accuracy 0.997559,\n",
    "\t\tDev epoch 48, average loss 0.405061, average accuracy 0.920339,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1523.076869726181\n",
    "Train epoch 51, average loss 0.00815841, average accuracy 0.997429,\n",
    "Train epoch 54, average loss 0.00750207, average accuracy 0.997439,\n",
    "\t\tDev epoch 54, average loss 0.411605, average accuracy 0.922109,\n",
    "Train epoch 57, average loss 0.00687906, average accuracy 0.997739,\n",
    "Train epoch 60, average loss 0.00724441, average accuracy 0.997749,\n",
    "\t\tDev epoch 60, average loss 0.513384, average accuracy 0.920039,\n",
    "\t\t\t\t    Time taken for 60 epochs =  1895.2125108242035\n",
    "                    \n",
    " #### embedding dim = 50. trainable = tur\n",
    " Writing to /home/ubuntu/W266Project/final_project/runs/cnn\n",
    "\n",
    "completed cnn creation\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.344391, average accuracy 0.857604,\n",
    "\t\tDev epoch 0, average loss 0.289542, average accuracy 0.875501,\n",
    "\t\t\t\t    Time taken for 0 epochs =  32.877453088760376\n",
    "Train epoch 3, average loss 0.205036, average accuracy 0.915813,\n",
    "Train epoch 6, average loss 0.143437, average accuracy 0.943792,\n",
    "\t\tDev epoch 6, average loss 0.245589, average accuracy 0.913261,\n",
    "Train epoch 9, average loss 0.10241, average accuracy 0.960037,\n",
    "Train epoch 12, average loss 0.0766644, average accuracy 0.970851,\n",
    "\t\tDev epoch 12, average loss 0.256715, average accuracy 0.918636,\n",
    "\t\t\t\t    Time taken for 12 epochs =  357.13569045066833\n",
    "Train epoch 15, average loss 0.0596088, average accuracy 0.978213,\n",
    "Train epoch 18, average loss 0.0488154, average accuracy 0.982314,\n",
    "\t\tDev epoch 18, average loss 0.298308, average accuracy 0.917835,\n",
    "Train epoch 21, average loss 0.0426194, average accuracy 0.984715,\n",
    "Train epoch 24, average loss 0.0340587, average accuracy 0.988276,\n",
    "\t\tDev epoch 24, average loss 0.324277, average accuracy 0.920339,\n",
    "\t\t\t\t    Time taken for 24 epochs =  681.0641686916351\n",
    "Train epoch 27, average loss 0.0312987, average accuracy 0.989157,\n",
    "Train epoch 30, average loss 0.0283847, average accuracy 0.990167,\n",
    "\t\tDev epoch 30, average loss 0.374785, average accuracy 0.92261,\n",
    "Train epoch 33, average loss 0.0240973, average accuracy 0.991917,\n",
    "Train epoch 36, average loss 0.0231575, average accuracy 0.992087,\n",
    "\t\tDev epoch 36, average loss 0.372793, average accuracy 0.922175,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1004.7895133495331\n",
    "Train epoch 39, average loss 0.0181793, average accuracy 0.993778,\n",
    "Train epoch 42, average loss 0.0158069, average accuracy 0.994878,\n",
    "\t\tDev epoch 42, average loss 0.394058, average accuracy 0.917535,\n",
    "Train epoch 45, average loss 0.0152652, average accuracy 0.994888,\n",
    "Train epoch 48, average loss 0.0122295, average accuracy 0.995979,\n",
    "\t\tDev epoch 48, average loss 0.415253, average accuracy 0.917601,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1328.6486732959747\n",
    "Train epoch 51, average loss 0.0120298, average accuracy 0.996009,\n",
    "Train epoch 54, average loss 0.0118266, average accuracy 0.996119,\n",
    "\t\tDev epoch 54, average loss 0.413061, average accuracy 0.922443,\n",
    "        \n",
    "#### increased embedding size to 200\n",
    "\n",
    "Train epoch 0, average loss 0.286025, average accuracy 0.882512,\n",
    "\t\tDev epoch 0, average loss 0.238953, average accuracy 0.901142,\n",
    "\t\t\t\t    Time taken for 0 epochs =  110.49707412719727\n",
    "Train epoch 3, average loss 0.115594, average accuracy 0.956656,\n",
    "Train epoch 6, average loss 0.0497285, average accuracy 0.982724,\n",
    "\t\tDev epoch 6, average loss 0.225743, average accuracy 0.929988,\n",
    "Train epoch 9, average loss 0.0273755, average accuracy 0.990967,\n",
    "Train epoch 12, average loss 0.0186573, average accuracy 0.993748,\n",
    "\t\tDev epoch 12, average loss 0.281195, average accuracy 0.931424,\n",
    "\t\t\t\t    Time taken for 12 epochs =  803.1010024547577\n",
    "Train epoch 15, average loss 0.0134854, average accuracy 0.995819,\n",
    "Train epoch 18, average loss 0.0100261, average accuracy 0.997069,\n",
    "\t\tDev epoch 18, average loss 0.315512, average accuracy 0.930188,\n",
    "Train epoch 21, average loss 0.00782281, average accuracy 0.997709,\n",
    "Train epoch 24, average loss 0.00664231, average accuracy 0.998049,\n",
    "\t\tDev epoch 24, average loss 0.325848, average accuracy 0.930288,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1495.02197265625\n",
    "Train epoch 27, average loss 0.0047185, average accuracy 0.9988,\n",
    "Train epoch 30, average loss 0.00426745, average accuracy 0.99881,\n",
    "\t\tDev epoch 30, average loss 0.402758, average accuracy 0.92892,\n",
    "Train epoch 33, average loss 0.00427015, average accuracy 0.99867,\n",
    "Train epoch 36, average loss 0.00313919, average accuracy 0.99917,\n",
    "\t\tDev epoch 36, average loss 0.364247, average accuracy 0.931858,\n",
    "\t\t\t\t    Time taken for 36 epochs =  2186.8154296875\n",
    "Train epoch 39, average loss 0.00310154, average accuracy 0.99915,\n",
    "Train epoch 42, average loss 0.00245393, average accuracy 0.99941,\n",
    "\t\tDev epoch 42, average loss 0.430721, average accuracy 0.930255,\n",
    "        \n",
    "#### decreased embedding size to 100, and reduced number of filters to 128 per filter size.\n",
    "\n",
    "# batches = 1562\n",
    "Train epoch 0, average loss 0.324282, average accuracy 0.864767,\n",
    "\t\tDev epoch 0, average loss 0.247642, average accuracy 0.896368,\n",
    "\t\t\t\t    Time taken for 0 epochs =  31.22071099281311\n",
    "Train epoch 3, average loss 0.170937, average accuracy 0.932518,\n",
    "Train epoch 6, average loss 0.111375, average accuracy 0.956436,\n",
    "\t\tDev epoch 6, average loss 0.215396, average accuracy 0.916533,\n",
    "Train epoch 9, average loss 0.0755983, average accuracy 0.971181,\n",
    "Train epoch 12, average loss 0.0536806, average accuracy 0.980444,\n",
    "\t\tDev epoch 12, average loss 0.272522, average accuracy 0.921541,\n",
    "\t\t\t\t    Time taken for 12 epochs =  300.37999510765076\n",
    "Train epoch 15, average loss 0.0438895, average accuracy 0.983705,\n",
    "Train epoch 18, average loss 0.0348257, average accuracy 0.987456,\n",
    "\t\tDev epoch 18, average loss 0.311513, average accuracy 0.922676,\n",
    "Train epoch 21, average loss 0.0279689, average accuracy 0.990087,\n",
    "Train epoch 24, average loss 0.025247, average accuracy 0.990957,\n",
    "\t\tDev epoch 24, average loss 0.341999, average accuracy 0.923811,\n",
    "\t\t\t\t    Time taken for 24 epochs =  569.6881365776062\n",
    "Train epoch 27, average loss 0.0203997, average accuracy 0.992968,\n",
    "Train epoch 30, average loss 0.0187122, average accuracy 0.993508,\n",
    "\t\tDev epoch 30, average loss 0.357159, average accuracy 0.923811,\n",
    "Train epoch 33, average loss 0.0158869, average accuracy 0.994488,\n",
    "Train epoch 36, average loss 0.0158628, average accuracy 0.994568,\n",
    "\t\tDev epoch 36, average loss 0.371735, average accuracy 0.92311,\n",
    "\t\t\t\t    Time taken for 36 epochs =  838.69158244133\n",
    "Train epoch 39, average loss 0.0133922, average accuracy 0.995659,\n",
    "Train epoch 42, average loss 0.0127672, average accuracy 0.995709,\n",
    "\t\tDev epoch 42, average loss 0.402635, average accuracy 0.92498,\n",
    "Train epoch 45, average loss 0.0119068, average accuracy 0.996049,\n",
    "Train epoch 48, average loss 0.0107565, average accuracy 0.996499,\n",
    "\t\tDev epoch 48, average loss 0.419966, average accuracy 0.92478,\n",
    "\t\t\t\t    Time taken for 48 epochs =  1107.9437527656555\n",
    "Train epoch 51, average loss 0.0100834, average accuracy 0.996689,\n",
    "Train epoch 54, average loss 0.00994073, average accuracy 0.996889,\n",
    "\t\tDev epoch 54, average loss 0.41175, average accuracy 0.924245,\n",
    "Train epoch 57, average loss 0.00979135, average accuracy 0.996649,\n",
    "Train epoch 60, average loss 0.00821928, average accuracy 0.997469,\n",
    "\t\tDev epoch 60, average loss 0.469267, average accuracy 0.926115,\n",
    "\t\t\t\t    Time taken for 60 epochs =  1377.0541887283325\n",
    "                    \n",
    " #### increased number of filters to 512 per filter size.\n",
    " \n",
    " Train epoch 0, average loss 0.311692, average accuracy 0.870288,\n",
    "\t\tDev epoch 0, average loss 0.242969, average accuracy 0.896701,\n",
    "\t\t\t\t    Time taken for 0 epochs =  84.6077299118042\n",
    "Train epoch 3, average loss 0.142796, average accuracy 0.944482,\n",
    "Train epoch 6, average loss 0.0668666, average accuracy 0.975692,\n",
    "\t\tDev epoch 6, average loss 0.210706, average accuracy 0.924913,\n",
    "Train epoch 9, average loss 0.0342145, average accuracy 0.988336,\n",
    "Train epoch 12, average loss 0.0224968, average accuracy 0.992708,\n",
    "\t\tDev epoch 12, average loss 0.274663, average accuracy 0.926916,\n",
    "\t\t\t\t    Time taken for 12 epochs =  809.0257966518402\n",
    "Train epoch 15, average loss 0.0171242, average accuracy 0.994098,\n",
    "Train epoch 18, average loss 0.0118646, average accuracy 0.996329,\n",
    "\t\tDev epoch 18, average loss 0.302414, average accuracy 0.928218,\n",
    "Train epoch 21, average loss 0.0098251, average accuracy 0.996879,\n",
    "Train epoch 24, average loss 0.0067338, average accuracy 0.998029,\n",
    "\t\tDev epoch 24, average loss 0.331184, average accuracy 0.927985,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1532.959394454956\n",
    "Train epoch 27, average loss 0.00506558, average accuracy 0.99858,\n",
    "Train epoch 30, average loss 0.00396653, average accuracy 0.99893,\n",
    "\t\tDev epoch 30, average loss 0.343347, average accuracy 0.928352,\n",
    "Train epoch 33, average loss 0.00384556, average accuracy 0.99898,\n",
    "Train epoch 36, average loss 0.0030804, average accuracy 0.99932,\n",
    "\t\tDev epoch 36, average loss 0.373929, average accuracy 0.928552,\n",
    "\t\t\t\t    Time taken for 36 epochs =  2256.9254744052887\n",
    "Train epoch 39, average loss 0.00246744, average accuracy 0.99933,\n",
    "Train epoch 42, average loss 0.00211266, average accuracy 0.9996,\n",
    "\t\tDev epoch 42, average loss 0.361598, average accuracy 0.929053,\n",
    "Train epoch 45, average loss 0.00217611, average accuracy 0.99941,\n",
    "Train epoch 48, average loss 0.00197936, average accuracy 0.99955,\n",
    "\t\tDev epoch 48, average loss 0.391054, average accuracy 0.929754,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2980.909019947052\n",
    "Train epoch 51, average loss 0.00187027, average accuracy 0.99957,\n",
    "Train epoch 54, average loss 0.00191055, average accuracy 0.99958,\n",
    "\t\tDev epoch 54, average loss 0.378678, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00153142, average accuracy 0.99968,\n",
    "Train epoch 60, average loss 0.00134926, average accuracy 0.99974,\n",
    "\t\tDev epoch 60, average loss 0.405928, average accuracy 0.929854,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3704.6175475120544\n",
    "                    \n",
    "completed cnn creation\n",
    "#### added a filter for length 6. so filter sizes became (3,4,5,6). \n",
    "Train epoch 0, average loss 0.315134, average accuracy 0.868868,\n",
    "\t\tDev epoch 0, average loss 0.248855, average accuracy 0.89353,\n",
    "\t\t\t\t    Time taken for 0 epochs =  69.76286554336548\n",
    "Train epoch 3, average loss 0.149292, average accuracy 0.942482,\n",
    "Train epoch 6, average loss 0.072978, average accuracy 0.973211,\n",
    "\t\tDev epoch 6, average loss 0.220149, average accuracy 0.921908,\n",
    "Train epoch 9, average loss 0.0399216, average accuracy 0.985855,\n",
    "Train epoch 12, average loss 0.0274265, average accuracy 0.990367,\n",
    "\t\tDev epoch 12, average loss 0.321294, average accuracy 0.922342,\n",
    "\t\t\t\t    Time taken for 12 epochs =  666.9912831783295\n",
    "Train epoch 15, average loss 0.0199664, average accuracy 0.993398,\n",
    "Train epoch 18, average loss 0.0145588, average accuracy 0.995429,\n",
    "\t\tDev epoch 18, average loss 0.30958, average accuracy 0.921341,\n",
    "Train epoch 21, average loss 0.011635, average accuracy 0.996449,\n",
    "Train epoch 24, average loss 0.00927037, average accuracy 0.997119,\n",
    "\t\tDev epoch 24, average loss 0.352559, average accuracy 0.926449,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1264.2059795856476\n",
    "Train epoch 27, average loss 0.00758558, average accuracy 0.997789,\n",
    "Train epoch 30, average loss 0.0056179, average accuracy 0.998409,\n",
    "\t\tDev epoch 30, average loss 0.385125, average accuracy 0.926749,\n",
    "Train epoch 33, average loss 0.0049369, average accuracy 0.99856,\n",
    "Train epoch 36, average loss 0.00391196, average accuracy 0.99882,\n",
    "\t\tDev epoch 36, average loss 0.369655, average accuracy 0.926482,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1861.2735829353333\n",
    "Train epoch 39, average loss 0.00425983, average accuracy 0.99884,\n",
    "Train epoch 42, average loss 0.00378442, average accuracy 0.99898,\n",
    "\t\tDev epoch 42, average loss 0.386586, average accuracy 0.926883,\n",
    "Train epoch 45, average loss 0.00346757, average accuracy 0.99902,\n",
    "Train epoch 48, average loss 0.00338043, average accuracy 0.99902,\n",
    "\t\tDev epoch 48, average loss 0.395295, average accuracy 0.927784,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2458.481989145279\n",
    "Train epoch 51, average loss 0.00368297, average accuracy 0.99901,\n",
    "Train epoch 54, average loss 0.00253086, average accuracy 0.99935,\n",
    "\t\tDev epoch 54, average loss 0.410479, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00307247, average accuracy 0.99915,\n",
    "Train epoch 60, average loss 0.00267205, average accuracy 0.99934,\n",
    "\t\tDev epoch 60, average loss 0.449313, average accuracy 0.926215,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3055.6355838775635 \n",
    "                    \n",
    "                    \n",
    "#### With random shuffle added for batches.\n",
    "Train epoch 0, average loss 0.315134, average accuracy 0.868868,\n",
    "\t\tDev epoch 0, average loss 0.248855, average accuracy 0.89353,\n",
    "\t\t\t\t    Time taken for 0 epochs =  69.76286554336548\n",
    "Train epoch 3, average loss 0.149292, average accuracy 0.942482,\n",
    "Train epoch 6, average loss 0.072978, average accuracy 0.973211,\n",
    "\t\tDev epoch 6, average loss 0.220149, average accuracy 0.921908,\n",
    "Train epoch 9, average loss 0.0399216, average accuracy 0.985855,\n",
    "Train epoch 12, average loss 0.0274265, average accuracy 0.990367,\n",
    "\t\tDev epoch 12, average loss 0.321294, average accuracy 0.922342,\n",
    "\t\t\t\t    Time taken for 12 epochs =  666.9912831783295\n",
    "Train epoch 15, average loss 0.0199664, average accuracy 0.993398,\n",
    "Train epoch 18, average loss 0.0145588, average accuracy 0.995429,\n",
    "\t\tDev epoch 18, average loss 0.30958, average accuracy 0.921341,\n",
    "Train epoch 21, average loss 0.011635, average accuracy 0.996449,\n",
    "Train epoch 24, average loss 0.00927037, average accuracy 0.997119,\n",
    "\t\tDev epoch 24, average loss 0.352559, average accuracy 0.926449,\n",
    "\t\t\t\t    Time taken for 24 epochs =  1264.2059795856476\n",
    "Train epoch 27, average loss 0.00758558, average accuracy 0.997789,\n",
    "Train epoch 30, average loss 0.0056179, average accuracy 0.998409,\n",
    "\t\tDev epoch 30, average loss 0.385125, average accuracy 0.926749,\n",
    "Train epoch 33, average loss 0.0049369, average accuracy 0.99856,\n",
    "Train epoch 36, average loss 0.00391196, average accuracy 0.99882,\n",
    "\t\tDev epoch 36, average loss 0.369655, average accuracy 0.926482,\n",
    "\t\t\t\t    Time taken for 36 epochs =  1861.2735829353333\n",
    "Train epoch 39, average loss 0.00425983, average accuracy 0.99884,\n",
    "Train epoch 42, average loss 0.00378442, average accuracy 0.99898,\n",
    "\t\tDev epoch 42, average loss 0.386586, average accuracy 0.926883,\n",
    "Train epoch 45, average loss 0.00346757, average accuracy 0.99902,\n",
    "Train epoch 48, average loss 0.00338043, average accuracy 0.99902,\n",
    "\t\tDev epoch 48, average loss 0.395295, average accuracy 0.927784,\n",
    "\t\t\t\t    Time taken for 48 epochs =  2458.481989145279\n",
    "Train epoch 51, average loss 0.00368297, average accuracy 0.99901,\n",
    "Train epoch 54, average loss 0.00253086, average accuracy 0.99935,\n",
    "\t\tDev epoch 54, average loss 0.410479, average accuracy 0.92685,\n",
    "Train epoch 57, average loss 0.00307247, average accuracy 0.99915,\n",
    "Train epoch 60, average loss 0.00267205, average accuracy 0.99934,\n",
    "\t\tDev epoch 60, average loss 0.449313, average accuracy 0.926215,\n",
    "\t\t\t\t    Time taken for 60 epochs =  3055.6355838775635"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
