{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Optimizing Transfer Learning in Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "As humans we continually apply our knowledge in one area to learn things faster in another area. For example,  it is reasonably easier for a soccer player to learn how to play basketball compared to someone who has not played any sport. Can we apply this knowledge transferrability to machine learning. In machine learning, a good model is one that generalizes well to unseen data based on what it learns from the training data. Creating labelled data is expensive and is not always possible.  So effective utilization of existing labeled datasets can have many practical applications. In situations where there is a dearth of training data, transfer learning augments this generalization step by starting from models that have been trained on different data with similar or different tasks. We will examine the application of transfer learning to the field  of sentiment analysis specifically in the task of predicting ratings from reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "As a primary source of inspiration, we have used the PhD Thesis by Robert Remus titled Genre and Domain dependencies in Sentiment Analysis. Remus in the thesis examines the effect of genre and domain in various tasks related to sentiment analysis, one of which is sentiment polarity detection. He uses the concept of domain similarity. While various metrics for similarity are mentioned, the one we have used is JS Divergence to determine the similarity / divergence of word distributions of two domains. He also leverages JS Divergence to select a subset of instances from the source domain for creating a model for domain adaptation to the target domain. In his thesis, he uses instance selection(IS) to pick a subset from the source domain that is similar to the target domain. He shows that IS from source, and from source + target (9:1 ratio) is more effective than source only.\n",
    "\n",
    "A paper by Hal from 2007 looks at domain adaptation in the realm of supervised learning. It proposes a simple idea of augmenting the feature space of both the source as well as the target domain and use it as the input to a classifier which aims at learning a function to minimize the loss in the target domain.The idea was to take each feature in the source domain and make two version of it namely source-specific and generic.The same is done to the target domain with target-specific and generic.By augmenting the feature space, the algorithm learns to adapt on its own.This approach has been used well for tasks like parts of speech tagging, parsing in the paper but not sentiment analysis. We found this approach very intriguing and would like to apply this to sentiment analysis task as a backup if there are issues with our primary approach.\n",
    "\n",
    "We also looked at the transferrability of neural networks in a paper by Mou et al from 2016 titled: How Transferable are Neural Networks in NLP Applications? This paper looks at tranferrability in different tasks like sentence classification and also sentiment analysis. For sentiment analysis they looked at LSTM-RNN while CNNs were used for sentence classification. The paper also looks at how transferable  different layers of NLP neural models are. The paer finds that transferrability between tasks is dependent on how semantically equivalent the two tasks are. With specific relevance to our project, the paper reports  that for sentiment analysis, tranferring and finetuning the embedding layer as well as the hidden layer helps in improving performance. The output layer is more specific to the dataset and performs best when left initialized randomly. An interesting aspect of this paper is the question about when the parameters are ready to trasfer. The paper reported a sharp increase in accuracy from epochs 1-5 which plateaus later. We intend to leverage this in our work.\n",
    "\n",
    "A more recent paper that looks at the problem of domain adaptation for sentiment learning is Domain Adaptation for Large Scale Sentiment Classification : A Deep Learning Approach, but Glorot, Bordes, Bengio. The paper uses an unsupervised deep learning approach (specifically de-ionized auto-encoders) to extract a meaningful feature representation of each review in an unsupervised fashion. They then train a sentiment classifier (SVM) on the features extracted with this auto-encoder. They find that sentiment classifiers trained with this approach outperform state-of-the-art methods in domain adaptation for sentiment classification on a benchmark of Amazon reviews composed of 4 types of Amazon products (the benchmark was created by Blitzer in 2007). While their work shows a substantial reduction in loss in transfer from source to target domain, there is still a loss. \n",
    "\n",
    "Yet another interesting work which may lend itself to transfer learning in sentiment analysis has been that of deep contextualized word representation in a paper published this year by Clark el al. This paper explores word vectors each token is assigned a representation that is a function of the entire input sentence. The vectors are derived from a bidirectional LSTM that is trained with a coupled language model(LM) on a large text corpus. The word vectors are learned functions of the internal states of a deep bidirectional language model (biLM). These representations are called ELMo(Embeddings from a language model). This paper investigates the performance of ELMo on several tasks including sentiment analysis where it reports an improvement of 1% in accuracy over state of the art. Adding ELMo to a model increases the sample efficiency considerably, both in terms of number of parameter updates to reach state-of-the-art performance and the overall training set size. Although, this paper does not allude to transfer learning, the efficiency gained here can be helpful when we want to minimize training time on source domain in order to be able to transfer sooner.\n",
    "\n",
    "We would like to focus on ways to get to the same accuracy as on the target domain model only, by enhancing the source domain with a few examples from the target domain. We want to a) detemine how many examples are needed from the target domain when they are selected randomly b) whether we can reduce the number of examples needed by applying heuristics to pick the examples from the target domain that are least represented in the source domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods \n",
    "We want to divide the project into 3 parts, the 3rd is a stretch goal. depending on time and challenge of the tasks.\n",
    " \n",
    "Part 1 : How well do different machine learning models transfer across domains ? We will evaluate 2-3 machine learning techniques (Naïve Bayes, SVM, and Neural Networks) and determine which transfers the best. We will start with pretrained GLOVE embeddings and explore the performance with or without training it further. (Our objective  is not to find the best model on the source / target domain by itself, but rather the model that lends best to transferability from  source to target).\n",
    "\n",
    "Measure: Comparative Accuracy of predicting rating from reviews when using one domain versus another. This will define the baseline for transfer learning for part 2.\n",
    "\n",
    "Part 2: Main part of the project. We will examine and develop techniques to improve transfer learning from one domain to another. \n",
    "\n",
    "Examples include:\n",
    "\n",
    "a. Using domain similarity (source, target domain) to pick the source domain for transfer learning.     (Ref: 2) We will use  JS Divergence as the metric for domain similarity / difference.\n",
    "\n",
    "b. Evaluate whether adding a small number of labeled target instances can help us get to desired accuracy in target domain.\n",
    "\n",
    "1st Metric:  Nts/Nt where\n",
    "\n",
    "Nts = # labeled instances from target domain needed when starting from a model built on a source domain, to get to similar accuracy as a model built entirely on the target domain.\n",
    "\n",
    "Nt = ~ minimum number of labeled instances from target domain to reach maximum possible accuracy when building a model directly on the target domain only.\n",
    "              \t\n",
    "2nd Metric: - Tts/Tt where \n",
    "\n",
    "Tts = Epochs needed to get to similar accuracy when starting with a model trained on source             domain\n",
    "\n",
    " Tt = Minimum Epochs needed to reach maximum accuracy with a model trained on the target domain         only.\n",
    " \n",
    "c. Try to determine which examples from target domain would be most helpful in improving accuracy of the model (eg those most different from the source domain)\n",
    "Sample methods to pick examples from target domain : \n",
    "- Instances in the target domain with the most new vocabulary words vs the source domain.\n",
    "- Target domain instances with the highest JS Divergence vs the source domain.\n",
    "Metric:  Same as in 2b, and vs the result in 2b.\n",
    "\n",
    "d.  Stretch goal: If time permits, and we develop sufficient expertise to write the algorithms, we will attempt using Attention based neural networks (Ref: 11), or Auto-Encoders (Ref : 8) to see if they help improve accuracy of transfer learning, and how much fewer training instances are needed with those models.\n",
    "    \n",
    "### Details on JS Divergence :\n",
    "JS Divergence is similar to KL Divergence, but better suited to comparing domains with different vocabularies.\n",
    "\n",
    "KL Divergence = $$ D_{KL}(P\\ ||\\ Q) = \\sum_{x} P(x) \\log_2 {P(x)/{Q(x)}} $$\n",
    "\n",
    "JS Divergence = $$ D_{JS}(P\\ ||\\ Q) = 1/2 * (D_{KL}(P\\ ||\\ M) + D_{KL}(Q\\ ||\\ M)) $$\n",
    "\n",
    "where M = 1/2(Q+R)\n",
    "\n",
    "JS Divergence has the benefit that it is defined even when Q(x) is 0 for a given word, whereaas KL Divergence is not.\n",
    "\n",
    "### Details on RNN  :\n",
    "Motivated by the paper on Transferability of Neural networks, which uses an LSTM based RNN for sentiment analysis task, we will use  a recurrent neural network with long short term memory(LSTM)units. \n",
    "\n",
    "Model trained on Source domain = $M_S$\n",
    "\n",
    "Model trained on Source domain + Target domain = $M_{S+T}$\n",
    "\n",
    "We will start with pretrained GLOVE embeddings with 100 dimensions and finetune it in training of $M_S$. Then we intend to transfer it to train  $M_{S+T}$  allowing it to fine tune further.\n",
    "\n",
    "We will also initialize the hidden layer of $M_{S+T}$ from the  $M_S$ and let it train further.\n",
    "\n",
    "A softmax layer is added to the last word’s hidden state for classification. This layer will be randomly initialized.\n",
    "\n",
    "\n",
    "\n",
    "Part 3: Stretch goal\n",
    "If time permits, we would like to also examine the effectiveness of transfer learning with different tasks within sentiment analysis. Specifically, we would like to see how transfer learning works in Aspect Based Sentiment Analysis. For this, we would be using the data from SemEval 2016, and a CNN / RNN to predict the aspects and associated sentiments.\n",
    "\n",
    "Data: Labeled data for fine grained sentiment analysis in two categories in English : restaurants, and laptops. Paper describing the data(Ref 3). Sample analysis paper (Ref 6)    \n",
    "\n",
    "\n",
    "\n",
    "# Results and discussion \n",
    "(for your baseline model, though feel free to include material for anything else you’ve done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps section for work you plan to do before submitting the final version\n",
    "(you’ll remove this section and replace it with your conclusions, final results and analysis in your final report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Book : Sentiment Analysis and Opinion Mining, Bing Liu.\n",
    "\n",
    "2.  Genre and Domain Dependencies in Sentiment Analysis (PhD Thesis)\n",
    "http://www.qucosa.de/fileadmin/data/qucosa/documents/16543/dissertation_rremus_angenommen_20150423.pdf\n",
    "\n",
    "3.  SemEval 2016, Task 5, paper describing the data: https://www.researchgate.net/publication/305334494_SemEval-2016_Task_5_Aspect_Based_Sentiment_Analysis)\n",
    "\n",
    "4. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering\n",
    "R. He, J. McAuley\n",
    "WWW, 2016\n",
    "Pdf\n",
    "\n",
    "5. Image-based recommendations on styles and substitutes\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "SIGIR, 2015\n",
    "pdf\n",
    "6.  INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual Aspect-based Sentiment Analysis. https://arxiv.org/abs/1609.02748v2\n",
    "7.  How Transferable are Neural Networks in NLP Applications? https://arxiv.org/pdf/1603.06111.pdf\n",
    "8. Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach http://www.icml-2011.org/papers/342_icmlpaper.pdf\n",
    "9. Frustratingly Easy Domain Adaptation: Hal Daum´e III. www.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf\n",
    "10. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification  http://john.blitzer.com/papers/sentiment_domain.pdf\n",
    "11. ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs http://www.aclweb.org/anthology/Q16-1019\n",
    "12. Learning Attitudes and Attributes from Multi-Aspect Reviews http://i.stanford.edu/~julian/pdfs/icdm2012.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
